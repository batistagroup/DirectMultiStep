encoder:
  vocab_dim: 53
  hid_dim: 256
  n_layers: 6
  n_heads: 8
  ff_mult: 3
  ff_activation: gelu
  dropout: 0.1
  attn_bias: false
  context_window: 280
  start_idx: 0
  mask_idx: 51
  pad_idx: 52
  initiate_steps: true
  include_steps: true
  model_type: EncoderAConfig
decoder:
  vocab_dim: 53
  hid_dim: 256
  n_layers: 6
  n_heads: 8
  ff_mult: 3
  ff_activation: gelu
  dropout: 0.1
  attn_bias: false
  context_window: 1075
  start_idx: 0
  mask_idx: 51
  pad_idx: 52
  model_type: TransformerConfig
