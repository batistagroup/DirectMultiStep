{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DirectMultiStep: Direct Route Generation for Multi-Step Retrosynthesis","text":"<p>DirectMultiStep is a novel multi-step first approach for generating retrosynthesis routes in chemistry. The project provides multiple models for different retrosynthesis generation approaches.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#online-demo","title":"Online Demo","text":"<p>Try out our deployed models without any installation at models.batistalab.com.</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install the package directly from PyPI:</p> <pre><code>pip install directmultistep\n</code></pre>"},{"location":"#development","title":"Development","text":"<p>We welcome any contributions, feel free to clone the repo and create a PR. We recommend using uv:</p> <pre><code>uv venv --python 3.11\nsource .venv/bin/activate\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"#usage-example","title":"Usage Example","text":"<p>Here's a quick example to generate a retrosynthesis route:</p> <pre><code>from directmultistep.generate import generate_routes\nfrom pathlib import Path\n\n# Generate a route for a target molecule\ntarget = \"CNCc1cc(-c2ccccc2F)n(S(=O)(=O)c2cccnc2)c1\"\nstarting_material = \"CN\"\n\n# Using flash model with starting material\npaths = generate_routes(\n    target, \n    n_steps=2,\n    starting_material=starting_material, \n    model=\"flash\", \n    beam_size=5,\n    config_path=\"path/to/config.yaml\", \n    ckpt_dir=\"path/to/checkpoints\"\n)\n\n# Or use explorer model to automatically determine steps\npaths = generate_routes(\n    target,\n    starting_material=starting_material,\n    model=\"explorer\",\n    beam_size=5,\n    config_path=\"path/to/config.yaml\", \n    ckpt_dir=\"path/to/checkpoints\"\n)\n</code></pre>"},{"location":"#license","title":"License","text":"<ul> <li>Code: MIT License</li> <li>Paper content (arXiv preprint): CC-BY 4.0</li> </ul>"},{"location":"DirectMultiStep/evaluation/","title":"Subset Evaluation","text":"<p>This documentation covers how to evaluate model performance on specific subsets of data using beam search.</p>"},{"location":"DirectMultiStep/evaluation/#example-use","title":"Example Use","text":"<p>Evaluating a model on a subset involves several steps:</p> <ol> <li>Configure the evaluation parameters using <code>EvalConfig</code></li> <li>Load the model using <code>ModelFactory</code></li> <li>Initialize <code>ModelEvaluator</code> and run evaluation</li> </ol> <p>See <code>use-examples/eval-subset.py</code> for a full example.</p>"},{"location":"DirectMultiStep/evaluation/#source-code","title":"Source Code","text":""},{"location":"DirectMultiStep/evaluation/#directmultistep.generation.eval","title":"<code>directmultistep.generation.eval</code>","text":""},{"location":"DirectMultiStep/evaluation/#directmultistep.generation.eval.EvalConfig","title":"<code>EvalConfig</code>  <code>dataclass</code>","text":"Source code in <code>src/directmultistep/generation/eval.py</code> <pre><code>@dataclass\nclass EvalConfig:\n    epoch: int\n    data_path: Path\n    run_name: str\n    eval_dataset: str\n    beam_width: int\n\n    use_sm: bool\n    use_steps: bool\n\n    enc_active_experts: int | None = None\n    dec_active_experts: int | None = None\n\n    batch_size: int = 1\n    num_workers: int = 8\n    persistent_workers: bool = True\n\n    # post_init values\n    _checkpoint_path: Path | None = None\n    eval_name: str = \"\"\n\n    def __post_init__(self) -&gt; None:\n        # files = list((self.data_path / \"training\" / self.run_name).glob(f\"epoch={self.epoch}*\"))\n        # assert len(files) == 1, f\"Expected 1 checkpoint file, but found {len(files)}: {files}\"\n        # self._checkpoint_path: Path = files[0]\n\n        allowed_ds = [\"n1_50\", \"n1_500\", \"n5_50\", \"n5_500\", \"pharma\"]\n        assert (\n            self.eval_dataset in allowed_ds\n        ), f\"Eval dataset {self.eval_dataset} not in allowed datasets: {allowed_ds}\"\n\n        b_str = f\"b{self.beam_width}\"\n        sm_str = \"sm\" if self.use_sm else \"nosm\"\n        steps_str = \"st\" if self.use_steps else \"nost\"\n        suffix = \"\"\n        if self.enc_active_experts is not None:\n            suffix += f\"_ea={self.enc_active_experts}\"\n        if self.dec_active_experts is not None:\n            suffix += f\"_da={self.dec_active_experts}\"\n\n        self.eval_name = f\"{self.eval_dataset}_{b_str}_{sm_str}_{steps_str}\" + suffix\n\n    @property\n    def checkpoint_path(self) -&gt; Path:\n        assert self._checkpoint_path is not None, \"Checkpoint path is not set\"\n        return self._checkpoint_path\n\n    def save(self, path: Path) -&gt; None:\n        \"\"\"Save config to YAML file.\n\n        Args:\n            path: Path to save config file\n        \"\"\"\n        config_dict = asdict(self)\n        config_dict[\"data_path\"] = str(config_dict[\"data_path\"])\n        config_dict[\"_checkpoint_path\"] = None\n\n        with open(path, \"w\") as f:\n            yaml.safe_dump(config_dict, f, default_flow_style=False, sort_keys=False)\n\n    @classmethod\n    def load(cls, path: Path) -&gt; \"EvalConfig\":\n        \"\"\"Load config from YAML file.\n\n        Args:\n            path: Path to config file\n\n        Returns:\n            Loaded config object\n        \"\"\"\n        with open(path) as f:\n            config_dict = yaml.safe_load(f)\n\n        config_dict[\"data_path\"] = Path(config_dict[\"data_path\"])\n        return cls(**config_dict)\n</code></pre>"},{"location":"DirectMultiStep/evaluation/#directmultistep.generation.eval.EvalConfig.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load config from YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to config file</p> required <p>Returns:</p> Type Description <code>EvalConfig</code> <p>Loaded config object</p> Source code in <code>src/directmultistep/generation/eval.py</code> <pre><code>@classmethod\ndef load(cls, path: Path) -&gt; \"EvalConfig\":\n    \"\"\"Load config from YAML file.\n\n    Args:\n        path: Path to config file\n\n    Returns:\n        Loaded config object\n    \"\"\"\n    with open(path) as f:\n        config_dict = yaml.safe_load(f)\n\n    config_dict[\"data_path\"] = Path(config_dict[\"data_path\"])\n    return cls(**config_dict)\n</code></pre>"},{"location":"DirectMultiStep/evaluation/#directmultistep.generation.eval.EvalConfig.save","title":"<code>save(path)</code>","text":"<p>Save config to YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to save config file</p> required Source code in <code>src/directmultistep/generation/eval.py</code> <pre><code>def save(self, path: Path) -&gt; None:\n    \"\"\"Save config to YAML file.\n\n    Args:\n        path: Path to save config file\n    \"\"\"\n    config_dict = asdict(self)\n    config_dict[\"data_path\"] = str(config_dict[\"data_path\"])\n    config_dict[\"_checkpoint_path\"] = None\n\n    with open(path, \"w\") as f:\n        yaml.safe_dump(config_dict, f, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"DirectMultiStep/evaluation/#directmultistep.generation.eval.ModelEvaluator","title":"<code>ModelEvaluator</code>","text":"Source code in <code>src/directmultistep/generation/eval.py</code> <pre><code>class ModelEvaluator:\n    def __init__(self, model: nn.Module, ec: EvalConfig, tc: TrainingConfig, device: torch_device):\n        self.model = model\n        self.model.eval()\n        self.device = device\n        self.ec = ec\n        self.tc = tc\n\n        self.save_path = (\n            self.ec.data_path / \"evaluation\" / self.tc.run_name / f\"epoch={self.ec.epoch}\" / self.ec.eval_name\n        )\n        self.save_path.mkdir(parents=True, exist_ok=True)\n\n    @staticmethod\n    def _beam_pickle_exists(save_path: Path) -&gt; bool:\n        return (save_path / \"all_beam_results_NS2.pkl\").exists()\n\n    def load_eval_dataset(self) -&gt; None:\n        if self.ec.eval_dataset == \"pharma\":\n            data = load_pharma_compounds(self.ec.data_path / \"pharma_compounds.json\", load_sm=self.ec.use_sm)\n            name_idx = data[\"nameToIdx\"]\n        else:\n            prcsd = self.ec.data_path / \"processed\"\n            name_idx = None\n            if self.ec.use_sm:\n                data = load_dataset_sm(prcsd / ds_name_to_fname[self.ec.eval_dataset])\n            else:\n                data = load_dataset_nosm(prcsd / ds_name_to_fname[self.ec.eval_dataset])\n        self.ds = RoutesDataset(\n            metadata_path=self.ec.data_path / \"configs\" / self.tc.metadata_fname,\n            products=data[\"products\"],\n            path_strings=data[\"path_strings\"],\n            n_steps_list=data[\"n_steps_list\"],\n            starting_materials=data[\"starting_materials\"],\n            mode=\"generation\",\n            name_idx=name_idx,\n        )\n        self.dl = DataLoader(\n            self.ds,\n            batch_size=self.ec.batch_size,\n            shuffle=False,\n            num_workers=self.ec.num_workers,\n            persistent_workers=self.ec.persistent_workers,\n        )\n\n    def prepare_beam_search(self) -&gt; None:\n        self.beam = BeamSearch(\n            model=self.model,\n            beam_size=self.ec.beam_width,\n            start_idx=self.ds.token_to_idx[\"&lt;SOS&gt;\"],\n            pad_idx=self.ds.token_to_idx[\" \"],\n            end_idx=self.ds.token_to_idx[\"?\"],\n            max_length=self.ds.seq_out_max_length,\n            idx_to_token=self.ds.idx_to_token,\n            device=self.device,\n        )\n\n    def run_beam_search(self, save_pickle: bool = True, force_rerun: bool = False) -&gt; BeamResultType:\n        if self._beam_pickle_exists(self.save_path) and not force_rerun:\n            raise FileExistsError(f\"Beam search results already exist at {self.save_path / 'all_beam_results_NS2.pkl'}\")\n        all_beam_results_NS2: list[list[tuple[str, float]]] = []\n        for batch_idx, (prod_sm, steps, path) in tqdm(enumerate(self.dl), total=len(self.dl)):\n            beam_result_BS2 = self.beam.decode(\n                src_BC=prod_sm.to(self.device),\n                steps_B1=steps.to(self.device),\n                path_start_BL=path.to(self.device),\n            )\n            all_beam_results_NS2.extend(beam_result_BS2)\n        if save_pickle:\n            with open(self.save_path / \"all_beam_results_NS2.pkl\", \"wb\") as f:\n                pickle.dump(all_beam_results_NS2, f)\n        return all_beam_results_NS2\n\n    def calculate_top_k_accuracy(\n        self,\n        k_vals: list[int] | None = None,\n        save_pickle: bool = True,\n        check_true_reacs: bool = True,\n        check_stock: bool = True,\n    ) -&gt; dict[str, int | dict[str, str]]:\n        return self.recalculate_top_k_accuracy(\n            data_path=self.ec.data_path,\n            save_path=self.save_path,\n            products=self.ds.products,\n            path_strings=self.ds.path_strings,\n            starting_materials=self.ds.sms,\n            eval_ds=self.ec.eval_dataset,\n            k_vals=k_vals,\n            save_pickle=save_pickle,\n            check_true_reacs=self.ec.use_sm,\n            check_stock=check_stock,\n        )\n\n    @staticmethod\n    def calculate_valid_paths_accuracy(\n        save_path: Path,\n        path_strings: list[str],\n        products: list[str],\n        k_vals: list[int] | None = None,\n        save_pickle: bool = True,\n    ) -&gt; dict[str, int | dict[str, str]]:\n        if k_vals is None:\n            k_vals = [1, 2, 3, 4, 5, 10, 20, 50]\n        solvability = {}\n        if (save_path / \"valid_paths_NS2n.pkl\").exists():\n            with open(save_path / \"valid_paths_NS2n.pkl\", \"rb\") as f:\n                valid_paths_NS2n = pickle.load(f)\n        else:\n            if not ModelEvaluator._beam_pickle_exists(save_path):\n                raise FileNotFoundError(f\"Beam search results not found at {save_path / 'all_beam_results_NS2.pkl'}\")\n            with open(save_path / \"all_beam_results_NS2.pkl\", \"rb\") as f:\n                all_beam_results_NS2 = pickle.load(f)\n            valid_paths_NS2n = find_valid_paths(all_beam_results_NS2)\n            solvability[\"solved (all)\"] = len(products) - count_unsolved_targets(all_beam_results_NS2)\n            if save_pickle:\n                with open(save_path / \"valid_paths_NS2n.pkl\", \"wb\") as f:\n                    pickle.dump(valid_paths_NS2n, f)\n\n        matches_N, perm_matches_N = find_matching_paths(valid_paths_NS2n, path_strings)\n        top_ks = {\n            \"accuracy (valid, no perms)\": find_top_n_accuracy(matches_N, k_vals),\n            \"accuracy (valid, with perms)\": find_top_n_accuracy(perm_matches_N, k_vals),\n        }\n        solvability[\"solved (valid)\"] = len(products) - count_unsolved_targets(valid_paths_NS2n)\n\n        with open(save_path / \"top_k_accuracy_valid.yaml\", \"w\") as f:\n            yaml.dump({**solvability, **top_ks}, f, sort_keys=False)\n\n        return {**solvability, **top_ks}\n\n    @staticmethod\n    def calculate_processed_paths_accuracy(\n        data_path: Path,\n        save_path: Path,\n        products: list[str],\n        path_strings: list[str],\n        starting_materials: list[str] | None,\n        eval_ds: str,\n        k_vals: list[int] | None = None,\n        save_pickle: bool = True,\n        check_true_reacs: bool = True,\n        check_stock: bool = True,\n        force_rerun: bool = False,\n    ) -&gt; dict[str, int | dict[str, str]]:\n        if k_vals is None:\n            k_vals = [1, 2, 3, 4, 5, 10, 20, 50]\n\n        if not (save_path / \"valid_paths_NS2n.pkl\").exists():\n            raise FileNotFoundError(f\"Valid paths not found at {save_path / 'valid_paths_NS2n.pkl'}\")\n\n        # Load valid paths\n        with open(save_path / \"valid_paths_NS2n.pkl\", \"rb\") as f:\n            valid_paths_NS2n = pickle.load(f)\n\n        # Step 1: Remove repetitions\n        solvability = {}\n        unique_paths_fname = \"unique_paths_NS2n.pkl\"\n        if not force_rerun and (save_path / unique_paths_fname).exists():\n            with open(save_path / unique_paths_fname, \"rb\") as f:\n                unique_paths_NS2n = pickle.load(f)\n        else:\n            canon_paths_NS2n = canonicalize_paths(valid_paths_NS2n)\n            unique_paths_NS2n = remove_repetitions_within_beam_result(canon_paths_NS2n)\n            if save_pickle:\n                with open(save_path / unique_paths_fname, \"wb\") as f:\n                    pickle.dump(unique_paths_NS2n, f)\n            solvability[\"solved (canonicalized)\"] = len(products) - count_unsolved_targets(canon_paths_NS2n)\n        solvability[\"solved (unique)\"] = len(products) - count_unsolved_targets(unique_paths_NS2n)\n\n        # Step 2: Filter by commercial stock if needed\n        if check_stock:\n            if eval_ds in [\"n1\", \"n5\", \"n1_50\", \"n1_500\", \"n5_50\", \"n5_500\"]:\n                eval_ds = eval_ds.split(\"_\")[0]\n                stock = load_commercial_stock(data_path / \"paroutes\" / f\"{eval_ds}-stock.txt\")\n            else:\n                stock = None\n            available_paths_NS2n = (\n                find_paths_with_commercial_sm(unique_paths_NS2n, stock) if stock else unique_paths_NS2n\n            )\n        else:\n            available_paths_NS2n = unique_paths_NS2n\n\n        # Step 3: Find paths with correct products and reactants\n        correct_paths_NS2n = find_paths_with_correct_product_and_reactants(\n            available_paths_NS2n,\n            true_products=products,\n            true_reacs=starting_materials if check_true_reacs else None,\n        )\n\n        solvability = {\n            \"solved (available)\": len(products) - count_unsolved_targets(available_paths_NS2n),\n            \"solved (correct)\": len(products) - count_unsolved_targets(correct_paths_NS2n),\n        }\n\n        matches_N, perm_matches_N = find_matching_paths(correct_paths_NS2n, path_strings)\n        top_ks = {\n            \"accuracy (processed, no perms)\": find_top_n_accuracy(matches_N, k_vals),\n            \"accuracy (processed, with perms)\": find_top_n_accuracy(perm_matches_N, k_vals),\n        }\n\n        suffix = f\"true_reacs={check_true_reacs}_stock={check_stock}\"\n        if save_pickle:\n            with open(save_path / f\"processed_paths_NS2n_{suffix}.pkl\", \"wb\") as f:\n                pickle.dump(correct_paths_NS2n, f)\n\n        with open(save_path / f\"top_k_accuracy_{suffix}.yaml\", \"w\") as f:\n            yaml.dump({**solvability, **top_ks}, f, sort_keys=False)\n\n        return {**solvability, **top_ks}\n\n    @staticmethod\n    def recalculate_top_k_accuracy(\n        data_path: Path,\n        save_path: Path,\n        products: list[str],\n        path_strings: list[str],\n        starting_materials: list[str] | None,\n        eval_ds: str,\n        k_vals: list[int] | None = None,\n        save_pickle: bool = True,\n        check_true_reacs: bool = True,\n        check_stock: bool = True,\n    ) -&gt; dict[str, int | dict[str, str]]:\n        \"\"\"Legacy function that combines both valid and processed paths accuracy calculations.\"\"\"\n        valid_results = ModelEvaluator.calculate_valid_paths_accuracy(\n            save_path=save_path,\n            path_strings=path_strings,\n            products=products,\n            k_vals=k_vals,\n            save_pickle=save_pickle,\n        )\n        processed_results = ModelEvaluator.calculate_processed_paths_accuracy(\n            data_path=data_path,\n            save_path=save_path,\n            products=products,\n            path_strings=path_strings,\n            starting_materials=starting_materials,\n            eval_ds=eval_ds,\n            k_vals=k_vals,\n            save_pickle=save_pickle,\n            check_true_reacs=check_true_reacs,\n            check_stock=check_stock,\n        )\n        return {**processed_results, **valid_results}\n\n    def prepare_name_to_rank(self) -&gt; dict[str, list[int | None]]:\n        fname = f\"{self.ec.eval_dataset}_processed_paths_true_reacs={self.ec.use_sm}_stock=False_NS2n.pkl\"\n        if not (self.save_path / fname).exists():\n            raise FileNotFoundError(f\"Correct paths not found at {self.save_path / fname}\")\n        with open(self.save_path / fname, \"rb\") as f:\n            correct_paths_NS2n = pickle.load(f)\n        _, perm_matches_N = find_matching_paths(correct_paths_NS2n, self.ds.path_strings)\n        assert self.ds.name_idx is not None, \"Name index is None\"\n        name_to_rank = {name: [perm_matches_N[i] for i in idxs] for name, idxs in self.ds.name_idx.items()}\n        with open(self.save_path / \"name_to_rank.yaml\", \"w\") as f:\n            yaml.dump(name_to_rank, f, sort_keys=False)\n        return name_to_rank\n</code></pre>"},{"location":"DirectMultiStep/evaluation/#directmultistep.generation.eval.ModelEvaluator.recalculate_top_k_accuracy","title":"<code>recalculate_top_k_accuracy(data_path, save_path, products, path_strings, starting_materials, eval_ds, k_vals=None, save_pickle=True, check_true_reacs=True, check_stock=True)</code>  <code>staticmethod</code>","text":"<p>Legacy function that combines both valid and processed paths accuracy calculations.</p> Source code in <code>src/directmultistep/generation/eval.py</code> <pre><code>@staticmethod\ndef recalculate_top_k_accuracy(\n    data_path: Path,\n    save_path: Path,\n    products: list[str],\n    path_strings: list[str],\n    starting_materials: list[str] | None,\n    eval_ds: str,\n    k_vals: list[int] | None = None,\n    save_pickle: bool = True,\n    check_true_reacs: bool = True,\n    check_stock: bool = True,\n) -&gt; dict[str, int | dict[str, str]]:\n    \"\"\"Legacy function that combines both valid and processed paths accuracy calculations.\"\"\"\n    valid_results = ModelEvaluator.calculate_valid_paths_accuracy(\n        save_path=save_path,\n        path_strings=path_strings,\n        products=products,\n        k_vals=k_vals,\n        save_pickle=save_pickle,\n    )\n    processed_results = ModelEvaluator.calculate_processed_paths_accuracy(\n        data_path=data_path,\n        save_path=save_path,\n        products=products,\n        path_strings=path_strings,\n        starting_materials=starting_materials,\n        eval_ds=eval_ds,\n        k_vals=k_vals,\n        save_pickle=save_pickle,\n        check_true_reacs=check_true_reacs,\n        check_stock=check_stock,\n    )\n    return {**processed_results, **valid_results}\n</code></pre>"},{"location":"DirectMultiStep/model-init/","title":"Creating a model instance","text":"<p>There are several ways to create a DMS model instance, ranging from using preset configurations to custom configurations.</p>"},{"location":"DirectMultiStep/model-init/#using-preset-configurations","title":"Using Preset Configurations","text":"<p>The simplest way to create a model is using one of the preset configurations:</p> <pre><code>from directmultistep.model import ModelFactory\n\nfactory = ModelFactory.from_preset(\"flash_10M\", compile_model=True)\nmodel = factory.create_model()\n</code></pre> <p>Available presets include: <code>deep_40M</code>, <code>explorer_xl_50M</code>, <code>flash_10M</code>, <code>flash_20M</code>, <code>flex_20M</code>, and <code>wide_40M</code>.</p>"},{"location":"DirectMultiStep/model-init/#custom-configuration","title":"Custom Configuration","text":"<p>For more control, you can create a custom configuration:</p> <pre><code>from directmultistep.model.config import Seq2SeqConfig, EncoderAConfig, MoEDecoderConfig\n\nconfig = Seq2SeqConfig(\n    encoder=EncoderAConfig(\n        vocab_dim=53,\n        hid_dim=256,\n        n_layers=6,\n        n_heads=8,\n        ff_mult=3,\n        ff_activation=\"gelu\",\n        dropout=0.1,\n        attn_bias=False,\n        context_window=280,\n        start_idx=0,\n        mask_idx=51,\n        pad_idx=52,\n        initiate_steps=True,\n        include_steps=True\n    ),\n    decoder=MoEDecoderConfig(\n        vocab_dim=53,\n        hid_dim=256,\n        n_layers=6,\n        n_heads=8,\n        ff_mult=3,\n        ff_activation=\"gelu\",\n        dropout=0.1,\n        attn_bias=False,\n        context_window=1075,\n        start_idx=0,\n        mask_idx=51,\n        pad_idx=52,\n        n_experts=3,\n        top_k=2,\n        capacity_factor=1.0,\n    ),\n)\n\nfactory = ModelFactory(config, device=None, compile_model=True)\nmodel = factory.create_model()\n</code></pre>"},{"location":"DirectMultiStep/model-init/#configuration-types","title":"Configuration Types","text":"<p>The model supports different types of encoders and decoders:</p> <ul> <li>Encoders:</li> <li><code>EncoderAConfig</code>: EncoderA Type (the one we've been using so far)</li> <li> <p><code>MoEEncoderConfig</code>: Mixture of Experts encoder</p> </li> <li> <p>Decoders:</p> </li> <li><code>TransformerConfig</code>: Standard transformer decoder</li> <li><code>MoEDecoderConfig</code>: Mixture of Experts decoder</li> </ul>"},{"location":"DirectMultiStep/model-init/#saving-and-loading-configurations","title":"Saving and Loading Configurations","text":"<p>Configurations can be saved to and loaded from YAML files:</p> <pre><code># Save configuration\nconfig.save(\"model_config.yaml\")\n\n# Load configuration and create model\nfactory = ModelFactory.from_config_file(\"model_config.yaml\")\nmodel = factory.create_model()\n</code></pre>"},{"location":"DirectMultiStep/model-init/#source-code","title":"Source Code","text":""},{"location":"DirectMultiStep/model-init/#directmultistep.model.config","title":"<code>directmultistep.model.config</code>","text":""},{"location":"DirectMultiStep/model-init/#directmultistep.model.config.TransformerConfig","title":"<code>TransformerConfig</code>  <code>dataclass</code>","text":"<p>Configuration for transformer components.</p> <p>Attributes:</p> Name Type Description <code>vocab_dim</code> <code>int</code> <p>Vocabulary dimension.</p> <code>hid_dim</code> <code>int</code> <p>Hidden dimension.</p> <code>n_layers</code> <code>int</code> <p>Number of layers.</p> <code>n_heads</code> <code>int</code> <p>Number of attention heads.</p> <code>ff_mult</code> <code>int</code> <p>Feedforward multiplier.</p> <code>ff_activation</code> <code>Literal['gelu', 'relu']</code> <p>Feedforward activation function ('gelu' or 'relu').</p> <code>dropout</code> <code>float</code> <p>Dropout probability.</p> <code>attn_bias</code> <code>bool</code> <p>Whether to use attention bias.</p> <code>context_window</code> <code>int</code> <p>Context window size.</p> <code>start_idx</code> <code>int</code> <p>Start token index.</p> <code>mask_idx</code> <code>int</code> <p>Mask token index.</p> <code>pad_idx</code> <code>int</code> <p>Padding token index.</p> Source code in <code>src/directmultistep/model/config.py</code> <pre><code>@dataclass\nclass TransformerConfig:\n    \"\"\"Configuration for transformer components.\n\n    Attributes:\n        vocab_dim: Vocabulary dimension.\n        hid_dim: Hidden dimension.\n        n_layers: Number of layers.\n        n_heads: Number of attention heads.\n        ff_mult: Feedforward multiplier.\n        ff_activation: Feedforward activation function ('gelu' or 'relu').\n        dropout: Dropout probability.\n        attn_bias: Whether to use attention bias.\n        context_window: Context window size.\n        start_idx: Start token index.\n        mask_idx: Mask token index.\n        pad_idx: Padding token index.\n    \"\"\"\n\n    vocab_dim: int\n    hid_dim: int\n    n_layers: int\n    n_heads: int\n    ff_mult: int\n    ff_activation: Literal[\"gelu\", \"relu\"]\n    dropout: float\n    attn_bias: bool\n    context_window: int\n    start_idx: int\n    mask_idx: int\n    pad_idx: int\n\n    def __post_init__(self) -&gt; None:\n        if self.hid_dim % self.n_heads != 0:\n            raise ValueError(f\"{self.hid_dim=} must be divisible by {self.n_heads=}\")\n        if self.ff_activation not in [\"gelu\", \"relu\"]:\n            raise ValueError(f\"{self.ff_activation=} must be either 'gelu' or 'relu'\")\n\n    def save(self, path: Path) -&gt; None:\n        \"\"\"Save config to yaml file.\n\n        Args:\n            path: Path to save the config to.\n        \"\"\"\n        data = asdict(self)\n        data[\"model_type\"] = self.__class__.__name__\n        with open(path, \"w\") as f:\n            yaml.dump(data, f, sort_keys=False, default_flow_style=False)\n\n    @classmethod\n    def load(cls: Type[T], path: Path) -&gt; T:\n        \"\"\"Load config from yaml file.\n\n        Args:\n            path: Path to load the config from.\n\n        Returns:\n            Loaded config.\n        \"\"\"\n        with open(path) as f:\n            data = yaml.safe_load(f)\n        return cls(**data)\n</code></pre>"},{"location":"DirectMultiStep/model-init/#directmultistep.model.config.TransformerConfig.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load config from yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to load the config from.</p> required <p>Returns:</p> Type Description <code>T</code> <p>Loaded config.</p> Source code in <code>src/directmultistep/model/config.py</code> <pre><code>@classmethod\ndef load(cls: Type[T], path: Path) -&gt; T:\n    \"\"\"Load config from yaml file.\n\n    Args:\n        path: Path to load the config from.\n\n    Returns:\n        Loaded config.\n    \"\"\"\n    with open(path) as f:\n        data = yaml.safe_load(f)\n    return cls(**data)\n</code></pre>"},{"location":"DirectMultiStep/model-init/#directmultistep.model.config.TransformerConfig.save","title":"<code>save(path)</code>","text":"<p>Save config to yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to save the config to.</p> required Source code in <code>src/directmultistep/model/config.py</code> <pre><code>def save(self, path: Path) -&gt; None:\n    \"\"\"Save config to yaml file.\n\n    Args:\n        path: Path to save the config to.\n    \"\"\"\n    data = asdict(self)\n    data[\"model_type\"] = self.__class__.__name__\n    with open(path, \"w\") as f:\n        yaml.dump(data, f, sort_keys=False, default_flow_style=False)\n</code></pre>"},{"location":"DirectMultiStep/model-init/#directmultistep.model.config.MoEDecoderConfig","title":"<code>MoEDecoderConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TransformerConfig</code></p> <p>Configuration for Mixture of Experts decoder components.</p> <p>Attributes:</p> Name Type Description <code>n_experts</code> <code>int</code> <p>Number of experts.</p> <code>top_k</code> <code>int</code> <p>Number of experts to use in forward pass.</p> <code>capacity_factor</code> <code>float</code> <p>Capacity factor for experts.</p> Source code in <code>src/directmultistep/model/config.py</code> <pre><code>@dataclass\nclass MoEDecoderConfig(TransformerConfig):\n    \"\"\"Configuration for Mixture of Experts decoder components.\n\n    Attributes:\n        n_experts: Number of experts.\n        top_k: Number of experts to use in forward pass.\n        capacity_factor: Capacity factor for experts.\n    \"\"\"\n\n    n_experts: int\n    top_k: int\n    capacity_factor: float\n</code></pre>"},{"location":"DirectMultiStep/model-init/#directmultistep.model.config.EncoderAConfig","title":"<code>EncoderAConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TransformerConfig</code></p> <p>Configuration for EncoderA components.</p> <p>Attributes:</p> Name Type Description <code>initiate_steps</code> <code>bool</code> <p>Whether to initiate steps.</p> <code>include_steps</code> <code>bool</code> <p>Whether to include steps.</p> Source code in <code>src/directmultistep/model/config.py</code> <pre><code>@dataclass\nclass EncoderAConfig(TransformerConfig):\n    \"\"\"Configuration for EncoderA components.\n\n    Attributes:\n        initiate_steps: Whether to initiate steps.\n        include_steps: Whether to include steps.\n    \"\"\"\n\n    initiate_steps: bool\n    include_steps: bool\n</code></pre>"},{"location":"DirectMultiStep/model-init/#directmultistep.model.config.MoEEncoderConfig","title":"<code>MoEEncoderConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>EncoderAConfig</code></p> <p>Configuration for Mixture of Experts encoder components.</p> <p>Attributes:</p> Name Type Description <code>n_experts</code> <code>int</code> <p>Number of experts.</p> <code>top_k</code> <code>int</code> <p>Number of experts to use in forward pass.</p> <code>capacity_factor</code> <code>float</code> <p>Capacity factor for experts.</p> Source code in <code>src/directmultistep/model/config.py</code> <pre><code>@dataclass\nclass MoEEncoderConfig(EncoderAConfig):\n    \"\"\"Configuration for Mixture of Experts encoder components.\n\n    Attributes:\n        n_experts: Number of experts.\n        top_k: Number of experts to use in forward pass.\n        capacity_factor: Capacity factor for experts.\n    \"\"\"\n\n    n_experts: int\n    top_k: int\n    capacity_factor: float\n</code></pre>"},{"location":"DirectMultiStep/model-init/#directmultistep.model.config.Seq2SeqConfig","title":"<code>Seq2SeqConfig</code>  <code>dataclass</code>","text":"<p>Complete model configuration.</p> <p>Attributes:</p> Name Type Description <code>encoder</code> <code>TransformerConfig</code> <p>Encoder configuration.</p> <code>decoder</code> <code>TransformerConfig</code> <p>Decoder configuration.</p> Source code in <code>src/directmultistep/model/config.py</code> <pre><code>@dataclass\nclass Seq2SeqConfig:\n    \"\"\"Complete model configuration.\n\n    Attributes:\n        encoder: Encoder configuration.\n        decoder: Decoder configuration.\n    \"\"\"\n\n    encoder: TransformerConfig\n    decoder: TransformerConfig\n\n    def save(self, path: Path) -&gt; None:\n        \"\"\"Save config to yaml file.\n\n        Args:\n            path: Path to save the config to.\n        \"\"\"\n        config_dict = {\n            \"encoder\": asdict(self.encoder) | {\"model_type\": self.encoder.__class__.__name__},\n            \"decoder\": asdict(self.decoder) | {\"model_type\": self.decoder.__class__.__name__},\n        }\n        with open(path, \"w\") as f:\n            yaml.dump(config_dict, f, sort_keys=False)\n\n    @classmethod\n    def load(cls, path: Path) -&gt; \"Seq2SeqConfig\":\n        \"\"\"Load config from yaml file.\n\n        Args:\n            path: Path to load the config from.\n\n        Returns:\n            Loaded Seq2SeqConfig.\n        \"\"\"\n        with open(path) as f:\n            data = yaml.safe_load(f)\n\n        # Determine correct encoder/decoder types based on model_type\n        encoder_data = data.pop(\"encoder\")\n        decoder_data = data.pop(\"decoder\")\n\n        model_type_to_config = {\n            \"TransformerConfig\": TransformerConfig,\n            \"MoEDecoderConfig\": MoEDecoderConfig,\n            \"EncoderAConfig\": EncoderAConfig,\n            \"MoEEncoderConfig\": MoEEncoderConfig,\n        }\n\n        encoder_model_type = encoder_data.pop(\"model_type\")\n        decoder_model_type = decoder_data.pop(\"model_type\")\n\n        encoder_type = model_type_to_config[encoder_model_type]\n        decoder_type = model_type_to_config[decoder_model_type]\n\n        encoder = encoder_type(**encoder_data)\n        decoder = decoder_type(**decoder_data)\n\n        return cls(encoder=encoder, decoder=decoder, **data)\n</code></pre>"},{"location":"DirectMultiStep/model-init/#directmultistep.model.config.Seq2SeqConfig.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load config from yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to load the config from.</p> required <p>Returns:</p> Type Description <code>Seq2SeqConfig</code> <p>Loaded Seq2SeqConfig.</p> Source code in <code>src/directmultistep/model/config.py</code> <pre><code>@classmethod\ndef load(cls, path: Path) -&gt; \"Seq2SeqConfig\":\n    \"\"\"Load config from yaml file.\n\n    Args:\n        path: Path to load the config from.\n\n    Returns:\n        Loaded Seq2SeqConfig.\n    \"\"\"\n    with open(path) as f:\n        data = yaml.safe_load(f)\n\n    # Determine correct encoder/decoder types based on model_type\n    encoder_data = data.pop(\"encoder\")\n    decoder_data = data.pop(\"decoder\")\n\n    model_type_to_config = {\n        \"TransformerConfig\": TransformerConfig,\n        \"MoEDecoderConfig\": MoEDecoderConfig,\n        \"EncoderAConfig\": EncoderAConfig,\n        \"MoEEncoderConfig\": MoEEncoderConfig,\n    }\n\n    encoder_model_type = encoder_data.pop(\"model_type\")\n    decoder_model_type = decoder_data.pop(\"model_type\")\n\n    encoder_type = model_type_to_config[encoder_model_type]\n    decoder_type = model_type_to_config[decoder_model_type]\n\n    encoder = encoder_type(**encoder_data)\n    decoder = decoder_type(**decoder_data)\n\n    return cls(encoder=encoder, decoder=decoder, **data)\n</code></pre>"},{"location":"DirectMultiStep/model-init/#directmultistep.model.config.Seq2SeqConfig.save","title":"<code>save(path)</code>","text":"<p>Save config to yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to save the config to.</p> required Source code in <code>src/directmultistep/model/config.py</code> <pre><code>def save(self, path: Path) -&gt; None:\n    \"\"\"Save config to yaml file.\n\n    Args:\n        path: Path to save the config to.\n    \"\"\"\n    config_dict = {\n        \"encoder\": asdict(self.encoder) | {\"model_type\": self.encoder.__class__.__name__},\n        \"decoder\": asdict(self.decoder) | {\"model_type\": self.decoder.__class__.__name__},\n    }\n    with open(path, \"w\") as f:\n        yaml.dump(config_dict, f, sort_keys=False)\n</code></pre>"},{"location":"DirectMultiStep/model-init/#directmultistep.model.factory","title":"<code>directmultistep.model.factory</code>","text":""},{"location":"DirectMultiStep/model-init/#directmultistep.model.factory.ModelFactory","title":"<code>ModelFactory</code>","text":"<p>Factory class for creating and configuring models.</p> Source code in <code>src/directmultistep/model/factory.py</code> <pre><code>class ModelFactory:\n    \"\"\"Factory class for creating and configuring models.\"\"\"\n\n    def __init__(\n        self,\n        config: Seq2SeqConfig,\n        device: str | None = None,\n        compile_model: bool = True,\n        allow_mps: bool = False,\n    ) -&gt; None:\n        \"\"\"Initializes the ModelFactory.\n\n        Args:\n            config: The complete model configuration.\n            device: Optional device specification. If None, the best available device is used.\n            compile_model: Whether to compile the model using torch.compile.\n            allow_mps: Whether to allow MPS device usage.\n        \"\"\"\n        self.config = config\n        self.device = self.determine_device(device, allow_mps)\n        self.compile_model = compile_model\n\n    def check_for_eval_config_updates(self, ec: EvalConfig) -&gt; None:\n        if isinstance(self.config.encoder, MoEEncoderConfig):\n            if ec.enc_active_experts is None:\n                raise ValueError(\"Encoder active experts must be set in eval config\")\n            self.config.encoder.top_k = ec.enc_active_experts\n        if isinstance(self.config.decoder, MoEDecoderConfig):\n            if ec.dec_active_experts is None:\n                raise ValueError(\"Decoder active experts must be set in eval config\")\n            self.config.decoder.top_k = ec.dec_active_experts\n\n    @staticmethod\n    def determine_device(device: str | None = None, allow_mps: bool = False) -&gt; torch_device:\n        \"\"\"Determines the appropriate device for model placement.\n\n        Args:\n            device: Optional device specification.\n\n        Returns:\n            The determined torch.device.\n        \"\"\"\n        if device is None:\n            if torch.cuda.is_available():\n                device = \"cuda\"\n            elif allow_mps and torch.backends.mps.is_available():\n                device = \"mps\"\n            else:\n                device = \"cpu\"\n        return torch.device(device)\n\n    @staticmethod\n    def _count_parameters(model: nn.Module) -&gt; int:\n        \"\"\"Counts the trainable parameters in a model.\n\n        Args:\n            model: The PyTorch model.\n\n        Returns:\n            The number of trainable parameters.\n        \"\"\"\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    def create_model(self) -&gt; Seq2Seq:\n        \"\"\"Creates and configures a Seq2Seq model based on the provided configuration.\n\n        Returns:\n            The configured Seq2Seq model.\n        \"\"\"\n        # Create encoder based on configuration type\n        if not isinstance(self.config.encoder, (EncoderAConfig, MoEEncoderConfig)):\n            raise TypeError(\"Encoder config must be either EncoderAConfig or MoEEncoderConfig\")\n        if not isinstance(self.config.decoder, (TransformerConfig, MoEDecoderConfig)):\n            raise TypeError(\"Decoder config must be either TransformerConfig or MoEDecoderConfig\")\n\n        encoder: Encoder | MoEEncoder\n        if isinstance(self.config.encoder, MoEEncoderConfig):\n            encoder = MoEEncoder(\n                vocab_dim=self.config.encoder.vocab_dim,\n                hid_dim=self.config.encoder.hid_dim,\n                context_window=self.config.encoder.context_window,\n                n_layers=self.config.encoder.n_layers,\n                n_heads=self.config.encoder.n_heads,\n                ff_mult=self.config.encoder.ff_mult,\n                ff_activation=self.config.encoder.ff_activation,\n                dropout=self.config.encoder.dropout,\n                attn_bias=self.config.encoder.attn_bias,\n                initiate_steps=self.config.encoder.initiate_steps,\n                include_steps=self.config.encoder.include_steps,\n                n_experts=self.config.encoder.n_experts,\n                top_k=self.config.encoder.top_k,\n                capacity_factor=self.config.encoder.capacity_factor,\n            )\n        else:\n            encoder = Encoder(\n                vocab_dim=self.config.encoder.vocab_dim,\n                hid_dim=self.config.encoder.hid_dim,\n                context_window=self.config.encoder.context_window,\n                n_layers=self.config.encoder.n_layers,\n                n_heads=self.config.encoder.n_heads,\n                ff_mult=self.config.encoder.ff_mult,\n                ff_activation=self.config.encoder.ff_activation,\n                dropout=self.config.encoder.dropout,\n                attn_bias=self.config.encoder.attn_bias,\n                initiate_steps=self.config.encoder.initiate_steps,\n                include_steps=self.config.encoder.include_steps,\n            )\n\n        decoder: Decoder | MoEDecoder\n        if isinstance(self.config.decoder, MoEDecoderConfig):\n            decoder = MoEDecoder(\n                vocab_dim=self.config.decoder.vocab_dim,\n                hid_dim=self.config.decoder.hid_dim,\n                context_window=self.config.decoder.context_window,\n                n_layers=self.config.decoder.n_layers,\n                n_heads=self.config.decoder.n_heads,\n                dropout=self.config.decoder.dropout,\n                attn_bias=self.config.decoder.attn_bias,\n                ff_mult=self.config.decoder.ff_mult,\n                ff_activation=self.config.decoder.ff_activation,\n                n_experts=self.config.decoder.n_experts,\n                top_k=self.config.decoder.top_k,\n                capacity_factor=self.config.decoder.capacity_factor,\n            )\n        else:\n            decoder = Decoder(\n                vocab_dim=self.config.decoder.vocab_dim,\n                hid_dim=self.config.decoder.hid_dim,\n                context_window=self.config.decoder.context_window,\n                n_layers=self.config.decoder.n_layers,\n                n_heads=self.config.decoder.n_heads,\n                dropout=self.config.decoder.dropout,\n                attn_bias=self.config.decoder.attn_bias,\n                ff_mult=self.config.decoder.ff_mult,\n                ff_activation=self.config.decoder.ff_activation,\n            )\n\n        model = Seq2Seq(\n            encoder=encoder,\n            decoder=decoder,\n            src_pad_idx=self.config.encoder.pad_idx,\n            trg_pad_idx=self.config.decoder.pad_idx,\n        )\n\n        model.to(self.device)\n\n        if self.compile_model:\n            model = torch.compile(model)  # type: ignore\n\n        print(f\"The model has {self._count_parameters(model):,} trainable parameters\")\n        return model\n\n    @classmethod\n    def from_config_file(\n        cls,\n        config_path: str | Path,\n        device: str | None = None,\n        compile_model: bool = True,\n    ) -&gt; \"ModelFactory\":\n        \"\"\"Creates a ModelFactory instance from a configuration file.\n\n        Args:\n            config_path: Path to the configuration file.\n            device: Optional device specification.\n            compile_model: Whether to compile the model.\n\n        Returns:\n            The configured ModelFactory instance.\n        \"\"\"\n        config = Seq2SeqConfig.load(Path(config_path))\n        return cls(config=config, device=device, compile_model=compile_model)\n\n    @classmethod\n    def from_preset(cls, preset_name: str, device: str | None = None, compile_model: bool = True) -&gt; \"ModelFactory\":\n        \"\"\"Loads a preset configuration by name.\n\n        Args:\n            preset_name: The name of the preset configuration.\n            device: Optional device specification.\n            compile_model: Whether to compile the model.\n\n        Returns:\n            The configured ModelFactory instance.\n\n        Raises:\n            ValueError: If the preset is not found.\n        \"\"\"\n        try:\n            with resources.path(\"directmultistep.model.default_configs\", f\"{preset_name}.yaml\") as config_path:\n                return cls.from_config_file(config_path, device, compile_model)\n        except FileNotFoundError:\n            raise ValueError(\n                f\"Preset '{preset_name}' not found. Available presets: deep_40M, explorer_xl_50M, flash_10M, flash_20M, flex_20M, wide_40M\"\n            )\n\n    @staticmethod\n    def load_checkpoint(model: Seq2Seq, ckpt_path: Path, device: torch.device) -&gt; Seq2Seq:\n        ckpt_torch = torch.load(ckpt_path, map_location=device)\n        model.load_state_dict(ckpt_torch)\n        model.to(device)\n        model.eval()\n        return model\n\n    @staticmethod\n    def load_lightning_checkpoint(model: Seq2Seq, ckpt_path: Path, device: torch.device) -&gt; Seq2Seq:\n        ckpt_lightning = torch.load(ckpt_path, map_location=device)\n        ckpt_torch = {k.replace(\"model.\", \"\"): v for k, v in ckpt_lightning[\"state_dict\"].items()}\n        model.load_state_dict(ckpt_torch)\n        model.to(device)\n        model.eval()\n        return model\n</code></pre>"},{"location":"DirectMultiStep/model-init/#directmultistep.model.factory.ModelFactory.__init__","title":"<code>__init__(config, device=None, compile_model=True, allow_mps=False)</code>","text":"<p>Initializes the ModelFactory.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Seq2SeqConfig</code> <p>The complete model configuration.</p> required <code>device</code> <code>str | None</code> <p>Optional device specification. If None, the best available device is used.</p> <code>None</code> <code>compile_model</code> <code>bool</code> <p>Whether to compile the model using torch.compile.</p> <code>True</code> <code>allow_mps</code> <code>bool</code> <p>Whether to allow MPS device usage.</p> <code>False</code> Source code in <code>src/directmultistep/model/factory.py</code> <pre><code>def __init__(\n    self,\n    config: Seq2SeqConfig,\n    device: str | None = None,\n    compile_model: bool = True,\n    allow_mps: bool = False,\n) -&gt; None:\n    \"\"\"Initializes the ModelFactory.\n\n    Args:\n        config: The complete model configuration.\n        device: Optional device specification. If None, the best available device is used.\n        compile_model: Whether to compile the model using torch.compile.\n        allow_mps: Whether to allow MPS device usage.\n    \"\"\"\n    self.config = config\n    self.device = self.determine_device(device, allow_mps)\n    self.compile_model = compile_model\n</code></pre>"},{"location":"DirectMultiStep/model-init/#directmultistep.model.factory.ModelFactory.create_model","title":"<code>create_model()</code>","text":"<p>Creates and configures a Seq2Seq model based on the provided configuration.</p> <p>Returns:</p> Type Description <code>Seq2Seq</code> <p>The configured Seq2Seq model.</p> Source code in <code>src/directmultistep/model/factory.py</code> <pre><code>def create_model(self) -&gt; Seq2Seq:\n    \"\"\"Creates and configures a Seq2Seq model based on the provided configuration.\n\n    Returns:\n        The configured Seq2Seq model.\n    \"\"\"\n    # Create encoder based on configuration type\n    if not isinstance(self.config.encoder, (EncoderAConfig, MoEEncoderConfig)):\n        raise TypeError(\"Encoder config must be either EncoderAConfig or MoEEncoderConfig\")\n    if not isinstance(self.config.decoder, (TransformerConfig, MoEDecoderConfig)):\n        raise TypeError(\"Decoder config must be either TransformerConfig or MoEDecoderConfig\")\n\n    encoder: Encoder | MoEEncoder\n    if isinstance(self.config.encoder, MoEEncoderConfig):\n        encoder = MoEEncoder(\n            vocab_dim=self.config.encoder.vocab_dim,\n            hid_dim=self.config.encoder.hid_dim,\n            context_window=self.config.encoder.context_window,\n            n_layers=self.config.encoder.n_layers,\n            n_heads=self.config.encoder.n_heads,\n            ff_mult=self.config.encoder.ff_mult,\n            ff_activation=self.config.encoder.ff_activation,\n            dropout=self.config.encoder.dropout,\n            attn_bias=self.config.encoder.attn_bias,\n            initiate_steps=self.config.encoder.initiate_steps,\n            include_steps=self.config.encoder.include_steps,\n            n_experts=self.config.encoder.n_experts,\n            top_k=self.config.encoder.top_k,\n            capacity_factor=self.config.encoder.capacity_factor,\n        )\n    else:\n        encoder = Encoder(\n            vocab_dim=self.config.encoder.vocab_dim,\n            hid_dim=self.config.encoder.hid_dim,\n            context_window=self.config.encoder.context_window,\n            n_layers=self.config.encoder.n_layers,\n            n_heads=self.config.encoder.n_heads,\n            ff_mult=self.config.encoder.ff_mult,\n            ff_activation=self.config.encoder.ff_activation,\n            dropout=self.config.encoder.dropout,\n            attn_bias=self.config.encoder.attn_bias,\n            initiate_steps=self.config.encoder.initiate_steps,\n            include_steps=self.config.encoder.include_steps,\n        )\n\n    decoder: Decoder | MoEDecoder\n    if isinstance(self.config.decoder, MoEDecoderConfig):\n        decoder = MoEDecoder(\n            vocab_dim=self.config.decoder.vocab_dim,\n            hid_dim=self.config.decoder.hid_dim,\n            context_window=self.config.decoder.context_window,\n            n_layers=self.config.decoder.n_layers,\n            n_heads=self.config.decoder.n_heads,\n            dropout=self.config.decoder.dropout,\n            attn_bias=self.config.decoder.attn_bias,\n            ff_mult=self.config.decoder.ff_mult,\n            ff_activation=self.config.decoder.ff_activation,\n            n_experts=self.config.decoder.n_experts,\n            top_k=self.config.decoder.top_k,\n            capacity_factor=self.config.decoder.capacity_factor,\n        )\n    else:\n        decoder = Decoder(\n            vocab_dim=self.config.decoder.vocab_dim,\n            hid_dim=self.config.decoder.hid_dim,\n            context_window=self.config.decoder.context_window,\n            n_layers=self.config.decoder.n_layers,\n            n_heads=self.config.decoder.n_heads,\n            dropout=self.config.decoder.dropout,\n            attn_bias=self.config.decoder.attn_bias,\n            ff_mult=self.config.decoder.ff_mult,\n            ff_activation=self.config.decoder.ff_activation,\n        )\n\n    model = Seq2Seq(\n        encoder=encoder,\n        decoder=decoder,\n        src_pad_idx=self.config.encoder.pad_idx,\n        trg_pad_idx=self.config.decoder.pad_idx,\n    )\n\n    model.to(self.device)\n\n    if self.compile_model:\n        model = torch.compile(model)  # type: ignore\n\n    print(f\"The model has {self._count_parameters(model):,} trainable parameters\")\n    return model\n</code></pre>"},{"location":"DirectMultiStep/model-init/#directmultistep.model.factory.ModelFactory.determine_device","title":"<code>determine_device(device=None, allow_mps=False)</code>  <code>staticmethod</code>","text":"<p>Determines the appropriate device for model placement.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str | None</code> <p>Optional device specification.</p> <code>None</code> <p>Returns:</p> Type Description <code>device</code> <p>The determined torch.device.</p> Source code in <code>src/directmultistep/model/factory.py</code> <pre><code>@staticmethod\ndef determine_device(device: str | None = None, allow_mps: bool = False) -&gt; torch_device:\n    \"\"\"Determines the appropriate device for model placement.\n\n    Args:\n        device: Optional device specification.\n\n    Returns:\n        The determined torch.device.\n    \"\"\"\n    if device is None:\n        if torch.cuda.is_available():\n            device = \"cuda\"\n        elif allow_mps and torch.backends.mps.is_available():\n            device = \"mps\"\n        else:\n            device = \"cpu\"\n    return torch.device(device)\n</code></pre>"},{"location":"DirectMultiStep/model-init/#directmultistep.model.factory.ModelFactory.from_config_file","title":"<code>from_config_file(config_path, device=None, compile_model=True)</code>  <code>classmethod</code>","text":"<p>Creates a ModelFactory instance from a configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str | Path</code> <p>Path to the configuration file.</p> required <code>device</code> <code>str | None</code> <p>Optional device specification.</p> <code>None</code> <code>compile_model</code> <code>bool</code> <p>Whether to compile the model.</p> <code>True</code> <p>Returns:</p> Type Description <code>ModelFactory</code> <p>The configured ModelFactory instance.</p> Source code in <code>src/directmultistep/model/factory.py</code> <pre><code>@classmethod\ndef from_config_file(\n    cls,\n    config_path: str | Path,\n    device: str | None = None,\n    compile_model: bool = True,\n) -&gt; \"ModelFactory\":\n    \"\"\"Creates a ModelFactory instance from a configuration file.\n\n    Args:\n        config_path: Path to the configuration file.\n        device: Optional device specification.\n        compile_model: Whether to compile the model.\n\n    Returns:\n        The configured ModelFactory instance.\n    \"\"\"\n    config = Seq2SeqConfig.load(Path(config_path))\n    return cls(config=config, device=device, compile_model=compile_model)\n</code></pre>"},{"location":"DirectMultiStep/model-init/#directmultistep.model.factory.ModelFactory.from_preset","title":"<code>from_preset(preset_name, device=None, compile_model=True)</code>  <code>classmethod</code>","text":"<p>Loads a preset configuration by name.</p> <p>Parameters:</p> Name Type Description Default <code>preset_name</code> <code>str</code> <p>The name of the preset configuration.</p> required <code>device</code> <code>str | None</code> <p>Optional device specification.</p> <code>None</code> <code>compile_model</code> <code>bool</code> <p>Whether to compile the model.</p> <code>True</code> <p>Returns:</p> Type Description <code>ModelFactory</code> <p>The configured ModelFactory instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the preset is not found.</p> Source code in <code>src/directmultistep/model/factory.py</code> <pre><code>@classmethod\ndef from_preset(cls, preset_name: str, device: str | None = None, compile_model: bool = True) -&gt; \"ModelFactory\":\n    \"\"\"Loads a preset configuration by name.\n\n    Args:\n        preset_name: The name of the preset configuration.\n        device: Optional device specification.\n        compile_model: Whether to compile the model.\n\n    Returns:\n        The configured ModelFactory instance.\n\n    Raises:\n        ValueError: If the preset is not found.\n    \"\"\"\n    try:\n        with resources.path(\"directmultistep.model.default_configs\", f\"{preset_name}.yaml\") as config_path:\n            return cls.from_config_file(config_path, device, compile_model)\n    except FileNotFoundError:\n        raise ValueError(\n            f\"Preset '{preset_name}' not found. Available presets: deep_40M, explorer_xl_50M, flash_10M, flash_20M, flex_20M, wide_40M\"\n        )\n</code></pre>"},{"location":"DirectMultiStep/training/","title":"Training","text":""},{"location":"DirectMultiStep/training/#example-use","title":"Example Use","text":"<p>Training a model involves three main steps:</p> <ol> <li>Create a model configuration and instance using <code>ModelFactory</code></li> <li>Configure the training parameters using <code>TrainingConfig</code></li> <li>Initialize the <code>ModelTrainer</code> and start training</li> </ol> <p>See <code>use-examples/train_model.py</code> for a full example.</p>"},{"location":"DirectMultiStep/training/#source-code","title":"Source Code","text":""},{"location":"DirectMultiStep/training/#directmultistep.training.config","title":"<code>directmultistep.training.config</code>","text":""},{"location":"DirectMultiStep/training/#directmultistep.training.config.TrainingConfig","title":"<code>TrainingConfig</code>  <code>dataclass</code>","text":"Source code in <code>src/directmultistep/training/config.py</code> <pre><code>@dataclass\nclass TrainingConfig:\n    # Data configs\n    data_path: Path\n\n    # Training setup\n    run_name: str\n    train_fname: str\n    val_fname: str\n    metadata_fname: str\n\n    # Training hyperparameters\n    batch_size: int\n    learning_rate: float\n    max_epochs: int\n\n    # Scheduler configs\n    warmup_steps: int\n    decay_steps: int\n    decay_factor: float\n\n    pad_idx: int\n    mask_idx: int\n\n    # Checkpointing\n    save_top_k: int = -1\n    checkpoint_every_n_epochs: int = 2\n\n    num_workers: int = 1\n    n_devices: int = 1\n    seed: int = 42\n\n    accelerator: str = \"auto\"\n    matmul_precision: str = \"high\"\n    summary_depth: int = 2\n    dist_strategy: str = \"ddp_find_unused_parameters_true\"\n\n    gradient_clip_val: float = 1.0\n    gradient_clip_algorithm: str = \"value\"\n\n    def __post_init__(self) -&gt; None:\n        self.data_path.mkdir(parents=True, exist_ok=True)\n        self.run_name = f\"{self.run_name}_seed={self.seed}\"\n\n        if self.matmul_precision not in [\"high\", \"medium\", \"low\"]:\n            raise ValueError(f\"{self.matmul_precision=} must be one of 'high', 'medium', or 'low'\")\n\n        if self.dist_strategy not in [\"auto\", \"fsdp\", \"ddp\", \"ddp_spawn\", \"ddp_find_unused_parameters_true\"]:\n            raise ValueError(\n                f\"{self.dist_strategy=} must be one of 'fsdp', 'ddp', 'ddp_spawn', or 'ddp_find_unused_parameters_true'\"\n            )\n\n        if self.gradient_clip_algorithm not in [\"norm\", \"value\"]:\n            raise ValueError(f\"{self.gradient_clip_algorithm=} must be one of 'norm' or 'value'\")\n\n    def save(self, path: Path) -&gt; None:\n        \"\"\"Save config to YAML file.\n\n        Args:\n            path: Path to save config file\n        \"\"\"\n        config_dict = asdict(self)\n        config_dict[\"data_path\"] = str(config_dict[\"data_path\"])\n\n        with open(path, \"w\") as f:\n            yaml.safe_dump(config_dict, f, default_flow_style=False, sort_keys=False)\n\n    @classmethod\n    def load(cls, path: Path) -&gt; \"TrainingConfig\":\n        \"\"\"Load config from YAML file.\n\n        Args:\n            path: Path to config file\n\n        Returns:\n            Loaded config object\n        \"\"\"\n        with open(path) as f:\n            config_dict = yaml.safe_load(f)\n\n        config_dict[\"data_path\"] = Path(config_dict[\"data_path\"])\n        instance = cls.__new__(cls)\n        for key, value in config_dict.items():\n            setattr(instance, key, value)\n        return instance\n</code></pre>"},{"location":"DirectMultiStep/training/#directmultistep.training.config.TrainingConfig.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load config from YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to config file</p> required <p>Returns:</p> Type Description <code>TrainingConfig</code> <p>Loaded config object</p> Source code in <code>src/directmultistep/training/config.py</code> <pre><code>@classmethod\ndef load(cls, path: Path) -&gt; \"TrainingConfig\":\n    \"\"\"Load config from YAML file.\n\n    Args:\n        path: Path to config file\n\n    Returns:\n        Loaded config object\n    \"\"\"\n    with open(path) as f:\n        config_dict = yaml.safe_load(f)\n\n    config_dict[\"data_path\"] = Path(config_dict[\"data_path\"])\n    instance = cls.__new__(cls)\n    for key, value in config_dict.items():\n        setattr(instance, key, value)\n    return instance\n</code></pre>"},{"location":"DirectMultiStep/training/#directmultistep.training.config.TrainingConfig.save","title":"<code>save(path)</code>","text":"<p>Save config to YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to save config file</p> required Source code in <code>src/directmultistep/training/config.py</code> <pre><code>def save(self, path: Path) -&gt; None:\n    \"\"\"Save config to YAML file.\n\n    Args:\n        path: Path to save config file\n    \"\"\"\n    config_dict = asdict(self)\n    config_dict[\"data_path\"] = str(config_dict[\"data_path\"])\n\n    with open(path, \"w\") as f:\n        yaml.safe_dump(config_dict, f, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"DirectMultiStep/training/#directmultistep.training.trainer","title":"<code>directmultistep.training.trainer</code>","text":""},{"location":"DirectMultiStep/training/#directmultistep.training.trainer.ModelTrainer","title":"<code>ModelTrainer</code>","text":"<p>High-level trainer class that orchestrates the training process.</p> Source code in <code>src/directmultistep/training/trainer.py</code> <pre><code>class ModelTrainer:\n    \"\"\"High-level trainer class that orchestrates the training process.\"\"\"\n\n    def __init__(self, config: TrainingConfig):\n        \"\"\"Initialize trainer with configuration.\n\n        Args:\n            config: Training configuration\n        \"\"\"\n        self.config = config\n        self._setup_environment()\n\n    def _setup_environment(self) -&gt; None:\n        \"\"\"Configure training environment.\"\"\"\n        L.seed_everything(self.config.seed)\n        torch.set_float32_matmul_precision(self.config.matmul_precision)\n\n    def _create_lightning_module(self, model: torch.nn.Module) -&gt; LTraining:\n        \"\"\"Create the Lightning training module.\n\n        Args:\n            model: The model to train\n\n        Returns:\n            Configured PLTraining module\n        \"\"\"\n        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.config.pad_idx, reduction=\"mean\")\n\n        return LTraining(\n            model=model,\n            pad_idx=self.config.pad_idx,\n            mask_idx=self.config.mask_idx,\n            criterion=criterion,\n            lr=self.config.learning_rate,\n            batch_size=self.config.batch_size,\n            warmup_steps=self.config.warmup_steps,\n            decay_steps=self.config.decay_steps,\n            decay_factor=self.config.decay_factor,\n        )\n\n    def _setup_callbacks(self) -&gt; list[Any]:\n        \"\"\"Configure training callbacks.\n\n        Returns:\n            List of Lightning callbacks\n        \"\"\"\n        checkpoint_callback = ModelCheckpoint(\n            monitor=\"val_loss\",\n            dirpath=self.config.data_path / \"training\" / self.config.run_name,\n            save_last=True,\n            save_top_k=self.config.save_top_k,\n            every_n_epochs=self.config.checkpoint_every_n_epochs,\n        )\n\n        return [checkpoint_callback, RichModelSummary(max_depth=self.config.summary_depth)]\n\n    def _create_trainer(self) -&gt; L.Trainer:\n        \"\"\"Create Lightning trainer.\n\n        Returns:\n            Configured Lightning trainer\n        \"\"\"\n        return L.Trainer(\n            default_root_dir=self.config.data_path / \"training\" / self.config.run_name,\n            max_epochs=self.config.max_epochs,\n            accelerator=self.config.accelerator,\n            devices=self.config.n_devices,\n            num_nodes=1,\n            strategy=self.config.dist_strategy,\n            callbacks=self._setup_callbacks(),\n            gradient_clip_val=self.config.gradient_clip_val,\n            gradient_clip_algorithm=self.config.gradient_clip_algorithm,\n        )\n\n    def _create_dataloaders(\n        self,\n        train_dataset: RoutesDataset,\n        val_dataset: RoutesDataset,\n    ) -&gt; tuple[DataLoader[tuple[Tensor, ...]], DataLoader[tuple[Tensor, ...]]]:\n        \"\"\"Create training and validation dataloaders.\n\n        Args:\n            train_dataset: Training dataset\n            val_dataset: Validation dataset\n\n        Returns:\n            Tuple of (train_dataloader, val_dataloader)\n        \"\"\"\n        train_loader = torch.utils.data.DataLoader(\n            dataset=train_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=True,\n            num_workers=self.config.num_workers,\n            persistent_workers=True,\n            pin_memory=True,\n        )\n\n        val_loader = torch.utils.data.DataLoader(\n            dataset=val_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=False,\n            num_workers=self.config.num_workers,\n            persistent_workers=True,\n            pin_memory=True,\n        )\n\n        return train_loader, val_loader\n\n    def train(\n        self,\n        model: torch.nn.Module,\n        train_dataset: RoutesDataset,\n        val_dataset: RoutesDataset,\n    ) -&gt; None:\n        \"\"\"Train the model.\n\n        Args:\n            model: Model to train\n            train_dataset: Training dataset\n            val_dataset: Validation dataset\n        \"\"\"\n        lightning_model = self._create_lightning_module(model)\n        trainer = self._create_trainer()\n        dl_train, dl_val = self._create_dataloaders(train_dataset, val_dataset)\n        latest_ckpt = helpers.find_checkpoint(self.config.data_path / \"training\", self.config.run_name)\n\n        if latest_ckpt is not None:\n            print(f\"Loading model from {latest_ckpt}\")\n            trainer.fit(lightning_model, dl_train, dl_val, ckpt_path=latest_ckpt)\n        else:\n            trainer.fit(lightning_model, dl_train, dl_val)\n</code></pre>"},{"location":"DirectMultiStep/training/#directmultistep.training.trainer.ModelTrainer.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize trainer with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TrainingConfig</code> <p>Training configuration</p> required Source code in <code>src/directmultistep/training/trainer.py</code> <pre><code>def __init__(self, config: TrainingConfig):\n    \"\"\"Initialize trainer with configuration.\n\n    Args:\n        config: Training configuration\n    \"\"\"\n    self.config = config\n    self._setup_environment()\n</code></pre>"},{"location":"DirectMultiStep/training/#directmultistep.training.trainer.ModelTrainer.train","title":"<code>train(model, train_dataset, val_dataset)</code>","text":"<p>Train the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to train</p> required <code>train_dataset</code> <code>RoutesDataset</code> <p>Training dataset</p> required <code>val_dataset</code> <code>RoutesDataset</code> <p>Validation dataset</p> required Source code in <code>src/directmultistep/training/trainer.py</code> <pre><code>def train(\n    self,\n    model: torch.nn.Module,\n    train_dataset: RoutesDataset,\n    val_dataset: RoutesDataset,\n) -&gt; None:\n    \"\"\"Train the model.\n\n    Args:\n        model: Model to train\n        train_dataset: Training dataset\n        val_dataset: Validation dataset\n    \"\"\"\n    lightning_model = self._create_lightning_module(model)\n    trainer = self._create_trainer()\n    dl_train, dl_val = self._create_dataloaders(train_dataset, val_dataset)\n    latest_ckpt = helpers.find_checkpoint(self.config.data_path / \"training\", self.config.run_name)\n\n    if latest_ckpt is not None:\n        print(f\"Loading model from {latest_ckpt}\")\n        trainer.fit(lightning_model, dl_train, dl_val, ckpt_path=latest_ckpt)\n    else:\n        trainer.fit(lightning_model, dl_train, dl_val)\n</code></pre>"},{"location":"DirectMultiStep/training/#directmultistep.training.lightning","title":"<code>directmultistep.training.lightning</code>","text":""},{"location":"DirectMultiStep/training/#directmultistep.training.lightning.LTraining","title":"<code>LTraining</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A PyTorch Lightning module for training sequence-to-sequence models.</p> Source code in <code>src/directmultistep/training/lightning.py</code> <pre><code>class LTraining(pl.LightningModule):\n    \"\"\"A PyTorch Lightning module for training sequence-to-sequence models.\"\"\"\n\n    def __init__(\n        self,\n        pad_idx: int,\n        mask_idx: int,\n        lr: float,\n        batch_size: int,\n        warmup_steps: int = 4000,\n        decay_steps: int = 24000,\n        decay_factor: float = 0.1,\n        model: nn.Module | None = None,\n        criterion: nn.Module | None = None,\n        processed_tokens: int = 0,\n        start_idx: int = 0,\n    ):\n        \"\"\"Initializes the PLTraining module.\n\n        Args:\n            pad_idx: The index of the padding token.\n            mask_idx: The index of the mask token.\n            lr: The initial learning rate.\n            batch_size: The batch size.\n            warmup_steps: The number of warmup steps for the learning rate scheduler.\n            decay_steps: The number of decay steps for the learning rate scheduler.\n            decay_factor: The decay factor for the learning rate scheduler.\n            model: The sequence-to-sequence model.\n            criterion: The loss function.\n            processed_tokens: The number of tokens processed so far.\n            start_idx: The index of the start token.\n        \"\"\"\n        super().__init__()\n        if model is not None:\n            self.model = model\n        if criterion is not None:\n            self.criterion = criterion\n        self.start_idx = start_idx\n        self.pad_idx = pad_idx\n        self.mask_idx = mask_idx\n        self.learning_rate = lr\n        self.batch_size = batch_size\n        self.warmup_steps = warmup_steps\n        self.decay_steps = decay_steps\n        self.decay_factor = decay_factor\n        self.processed_tokens = processed_tokens\n        self.save_hyperparameters(ignore=[\"criterion\", \"model\"])\n        self.compute_loss = self.compute_loss_full\n\n    def mask_src(self, src_BC: Tensor, masking_prob: float) -&gt; Tensor:\n        \"\"\"Masks the source sequence with a given probability.\n\n        Args:\n            src_BC: The source sequence tensor of shape [B, C].\n            masking_prob: The probability of masking a token.\n\n        Returns:\n            The masked source sequence tensor of shape [B, C].\n        \"\"\"\n        mask_idx_BC = torch.rand(src_BC.shape).to(src_BC.device) &lt; masking_prob\n        not_pad_BC = src_BC != self.pad_idx\n        final_mask_BC = mask_idx_BC &amp; not_pad_BC\n        masked_src_BC = src_BC.clone()\n        masked_src_BC[final_mask_BC] = self.mask_idx\n        return masked_src_BC\n\n    def compute_loss_full(self, batch: Tensor, batch_idx: int) -&gt; Tensor:\n        \"\"\"Computes the loss for the full sequence training.\n\n        This method calculates the loss for all tokens in the sequence.\n\n        Args:\n            batch: The input batch tensor.\n            batch_idx: The index of the batch.\n\n        Returns:\n            The computed loss tensor.\n        \"\"\"\n        src_item_BC = batch[0]\n        tgt_item_BL = batch[1].long()\n        steps_B1 = batch[2].view(-1, 1)\n        masked_src_BC = self.mask_src(src_item_BC, masking_prob=0.05)\n        # the output actually is [B, L-1, V] given slicing of tgt_item_BL\n        output_BLV = self.model(masked_src_BC, tgt_item_BL[:, :-1], steps_B1)\n        output_blV = output_BLV.view(-1, output_BLV.shape[-1])  # [B*(L-1), V]\n        tgt_bl = tgt_item_BL[:, 1:].reshape(-1)  # [B*(L-1)]\n        loss = self.criterion(output_blV, tgt_bl)\n        self.processed_tokens += tgt_item_BL.shape[0] * tgt_item_BL.shape[1]\n        return cast(Tensor, loss)\n\n    def log_step_info(self, loss: Tensor, mode: str, prog_bar: bool) -&gt; None:\n        \"\"\"Logs the loss and other training information.\n\n        Args:\n            loss: The loss tensor.\n            mode: The mode of training ('train' or 'val').\n            prog_bar: Whether to display the loss in the progress bar.\n        \"\"\"\n        self.log(\n            f\"{mode}_loss\",\n            loss,\n            batch_size=self.batch_size,\n            prog_bar=prog_bar,\n            sync_dist=True,\n        )\n        self.log(\"processed_tokens\", self.processed_tokens, sync_dist=True)\n        if mode == \"train\":\n            current_lr = self.trainer.optimizers[0].param_groups[0][\"lr\"]\n            self.log(f\"{mode}_lr\", current_lr, batch_size=self.batch_size, sync_dist=True)\n\n    def training_step(self, batch: Tensor, batch_idx: int) -&gt; Tensor:\n        \"\"\"Performs a single training step.\n\n        Args:\n            batch: The input batch tensor.\n            batch_idx: The index of the batch.\n\n        Returns:\n            The computed loss tensor.\n        \"\"\"\n        loss = self.compute_loss(batch, batch_idx)\n        self.log_step_info(loss, \"train\", prog_bar=True)\n        return loss\n\n    def validation_step(self, batch: Tensor, batch_idx: int) -&gt; Tensor:\n        \"\"\"Performs a single validation step.\n\n        Args:\n            batch: The input batch tensor.\n            batch_idx: The index of the batch.\n\n        Returns:\n            The computed loss tensor.\n        \"\"\"\n        loss = self.compute_loss(batch, batch_idx)\n        self.log_step_info(loss, \"val\", prog_bar=True)\n        return loss\n\n    def configure_optimizers(\n        self,\n    ) -&gt; tuple[list[torch.optim.Optimizer], list[dict[str, Any]]]:\n        \"\"\"Configures the optimizer and learning rate scheduler.\n\n        Returns:\n            A tuple containing the list of optimizers and the list of\n            learning rate schedulers.\n        \"\"\"\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n        # return optimizer\n        scheduler = torch.optim.lr_scheduler.LambdaLR(\n            optimizer,\n            lr_lambda=warmup_and_cosine_decay(\n                warmup_steps=self.warmup_steps,\n                decay_steps=self.decay_steps,\n                decay_factor=self.decay_factor,\n            ),\n            verbose=False,\n        )\n        lr_scheduler = {\n            \"scheduler\": scheduler,  # The LR scheduler instance (required)\n            \"interval\": \"step\",  # The unit of the scheduler's step size\n            \"frequency\": 1,  # The frequency of the scheduler\n        }\n        return [optimizer], [lr_scheduler]\n\n    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Adds the processed tokens to the checkpoint.\n\n        Args:\n            checkpoint: The checkpoint dictionary.\n        \"\"\"\n        # Add processed_tokens to the checkpoint dictionary\n        checkpoint[\"processed_tokens\"] = self.processed_tokens\n\n    def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Loads the processed tokens from the checkpoint.\n\n        Args:\n            checkpoint: The checkpoint dictionary.\n        \"\"\"\n        # Load processed_tokens from the checkpoint dictionary\n        self.processed_tokens = checkpoint.get(\"processed_tokens\", 0)\n</code></pre>"},{"location":"DirectMultiStep/training/#directmultistep.training.lightning.LTraining.__init__","title":"<code>__init__(pad_idx, mask_idx, lr, batch_size, warmup_steps=4000, decay_steps=24000, decay_factor=0.1, model=None, criterion=None, processed_tokens=0, start_idx=0)</code>","text":"<p>Initializes the PLTraining module.</p> <p>Parameters:</p> Name Type Description Default <code>pad_idx</code> <code>int</code> <p>The index of the padding token.</p> required <code>mask_idx</code> <code>int</code> <p>The index of the mask token.</p> required <code>lr</code> <code>float</code> <p>The initial learning rate.</p> required <code>batch_size</code> <code>int</code> <p>The batch size.</p> required <code>warmup_steps</code> <code>int</code> <p>The number of warmup steps for the learning rate scheduler.</p> <code>4000</code> <code>decay_steps</code> <code>int</code> <p>The number of decay steps for the learning rate scheduler.</p> <code>24000</code> <code>decay_factor</code> <code>float</code> <p>The decay factor for the learning rate scheduler.</p> <code>0.1</code> <code>model</code> <code>Module | None</code> <p>The sequence-to-sequence model.</p> <code>None</code> <code>criterion</code> <code>Module | None</code> <p>The loss function.</p> <code>None</code> <code>processed_tokens</code> <code>int</code> <p>The number of tokens processed so far.</p> <code>0</code> <code>start_idx</code> <code>int</code> <p>The index of the start token.</p> <code>0</code> Source code in <code>src/directmultistep/training/lightning.py</code> <pre><code>def __init__(\n    self,\n    pad_idx: int,\n    mask_idx: int,\n    lr: float,\n    batch_size: int,\n    warmup_steps: int = 4000,\n    decay_steps: int = 24000,\n    decay_factor: float = 0.1,\n    model: nn.Module | None = None,\n    criterion: nn.Module | None = None,\n    processed_tokens: int = 0,\n    start_idx: int = 0,\n):\n    \"\"\"Initializes the PLTraining module.\n\n    Args:\n        pad_idx: The index of the padding token.\n        mask_idx: The index of the mask token.\n        lr: The initial learning rate.\n        batch_size: The batch size.\n        warmup_steps: The number of warmup steps for the learning rate scheduler.\n        decay_steps: The number of decay steps for the learning rate scheduler.\n        decay_factor: The decay factor for the learning rate scheduler.\n        model: The sequence-to-sequence model.\n        criterion: The loss function.\n        processed_tokens: The number of tokens processed so far.\n        start_idx: The index of the start token.\n    \"\"\"\n    super().__init__()\n    if model is not None:\n        self.model = model\n    if criterion is not None:\n        self.criterion = criterion\n    self.start_idx = start_idx\n    self.pad_idx = pad_idx\n    self.mask_idx = mask_idx\n    self.learning_rate = lr\n    self.batch_size = batch_size\n    self.warmup_steps = warmup_steps\n    self.decay_steps = decay_steps\n    self.decay_factor = decay_factor\n    self.processed_tokens = processed_tokens\n    self.save_hyperparameters(ignore=[\"criterion\", \"model\"])\n    self.compute_loss = self.compute_loss_full\n</code></pre>"},{"location":"DirectMultiStep/training/#directmultistep.training.lightning.LTraining.compute_loss_full","title":"<code>compute_loss_full(batch, batch_idx)</code>","text":"<p>Computes the loss for the full sequence training.</p> <p>This method calculates the loss for all tokens in the sequence.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>The input batch tensor.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed loss tensor.</p> Source code in <code>src/directmultistep/training/lightning.py</code> <pre><code>def compute_loss_full(self, batch: Tensor, batch_idx: int) -&gt; Tensor:\n    \"\"\"Computes the loss for the full sequence training.\n\n    This method calculates the loss for all tokens in the sequence.\n\n    Args:\n        batch: The input batch tensor.\n        batch_idx: The index of the batch.\n\n    Returns:\n        The computed loss tensor.\n    \"\"\"\n    src_item_BC = batch[0]\n    tgt_item_BL = batch[1].long()\n    steps_B1 = batch[2].view(-1, 1)\n    masked_src_BC = self.mask_src(src_item_BC, masking_prob=0.05)\n    # the output actually is [B, L-1, V] given slicing of tgt_item_BL\n    output_BLV = self.model(masked_src_BC, tgt_item_BL[:, :-1], steps_B1)\n    output_blV = output_BLV.view(-1, output_BLV.shape[-1])  # [B*(L-1), V]\n    tgt_bl = tgt_item_BL[:, 1:].reshape(-1)  # [B*(L-1)]\n    loss = self.criterion(output_blV, tgt_bl)\n    self.processed_tokens += tgt_item_BL.shape[0] * tgt_item_BL.shape[1]\n    return cast(Tensor, loss)\n</code></pre>"},{"location":"DirectMultiStep/training/#directmultistep.training.lightning.LTraining.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer and learning rate scheduler.</p> <p>Returns:</p> Type Description <code>list[Optimizer]</code> <p>A tuple containing the list of optimizers and the list of</p> <code>list[dict[str, Any]]</code> <p>learning rate schedulers.</p> Source code in <code>src/directmultistep/training/lightning.py</code> <pre><code>def configure_optimizers(\n    self,\n) -&gt; tuple[list[torch.optim.Optimizer], list[dict[str, Any]]]:\n    \"\"\"Configures the optimizer and learning rate scheduler.\n\n    Returns:\n        A tuple containing the list of optimizers and the list of\n        learning rate schedulers.\n    \"\"\"\n    optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n    # return optimizer\n    scheduler = torch.optim.lr_scheduler.LambdaLR(\n        optimizer,\n        lr_lambda=warmup_and_cosine_decay(\n            warmup_steps=self.warmup_steps,\n            decay_steps=self.decay_steps,\n            decay_factor=self.decay_factor,\n        ),\n        verbose=False,\n    )\n    lr_scheduler = {\n        \"scheduler\": scheduler,  # The LR scheduler instance (required)\n        \"interval\": \"step\",  # The unit of the scheduler's step size\n        \"frequency\": 1,  # The frequency of the scheduler\n    }\n    return [optimizer], [lr_scheduler]\n</code></pre>"},{"location":"DirectMultiStep/training/#directmultistep.training.lightning.LTraining.log_step_info","title":"<code>log_step_info(loss, mode, prog_bar)</code>","text":"<p>Logs the loss and other training information.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Tensor</code> <p>The loss tensor.</p> required <code>mode</code> <code>str</code> <p>The mode of training ('train' or 'val').</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the loss in the progress bar.</p> required Source code in <code>src/directmultistep/training/lightning.py</code> <pre><code>def log_step_info(self, loss: Tensor, mode: str, prog_bar: bool) -&gt; None:\n    \"\"\"Logs the loss and other training information.\n\n    Args:\n        loss: The loss tensor.\n        mode: The mode of training ('train' or 'val').\n        prog_bar: Whether to display the loss in the progress bar.\n    \"\"\"\n    self.log(\n        f\"{mode}_loss\",\n        loss,\n        batch_size=self.batch_size,\n        prog_bar=prog_bar,\n        sync_dist=True,\n    )\n    self.log(\"processed_tokens\", self.processed_tokens, sync_dist=True)\n    if mode == \"train\":\n        current_lr = self.trainer.optimizers[0].param_groups[0][\"lr\"]\n        self.log(f\"{mode}_lr\", current_lr, batch_size=self.batch_size, sync_dist=True)\n</code></pre>"},{"location":"DirectMultiStep/training/#directmultistep.training.lightning.LTraining.mask_src","title":"<code>mask_src(src_BC, masking_prob)</code>","text":"<p>Masks the source sequence with a given probability.</p> <p>Parameters:</p> Name Type Description Default <code>src_BC</code> <code>Tensor</code> <p>The source sequence tensor of shape [B, C].</p> required <code>masking_prob</code> <code>float</code> <p>The probability of masking a token.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The masked source sequence tensor of shape [B, C].</p> Source code in <code>src/directmultistep/training/lightning.py</code> <pre><code>def mask_src(self, src_BC: Tensor, masking_prob: float) -&gt; Tensor:\n    \"\"\"Masks the source sequence with a given probability.\n\n    Args:\n        src_BC: The source sequence tensor of shape [B, C].\n        masking_prob: The probability of masking a token.\n\n    Returns:\n        The masked source sequence tensor of shape [B, C].\n    \"\"\"\n    mask_idx_BC = torch.rand(src_BC.shape).to(src_BC.device) &lt; masking_prob\n    not_pad_BC = src_BC != self.pad_idx\n    final_mask_BC = mask_idx_BC &amp; not_pad_BC\n    masked_src_BC = src_BC.clone()\n    masked_src_BC[final_mask_BC] = self.mask_idx\n    return masked_src_BC\n</code></pre>"},{"location":"DirectMultiStep/training/#directmultistep.training.lightning.LTraining.on_load_checkpoint","title":"<code>on_load_checkpoint(checkpoint)</code>","text":"<p>Loads the processed tokens from the checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>dict[str, Any]</code> <p>The checkpoint dictionary.</p> required Source code in <code>src/directmultistep/training/lightning.py</code> <pre><code>def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Loads the processed tokens from the checkpoint.\n\n    Args:\n        checkpoint: The checkpoint dictionary.\n    \"\"\"\n    # Load processed_tokens from the checkpoint dictionary\n    self.processed_tokens = checkpoint.get(\"processed_tokens\", 0)\n</code></pre>"},{"location":"DirectMultiStep/training/#directmultistep.training.lightning.LTraining.on_save_checkpoint","title":"<code>on_save_checkpoint(checkpoint)</code>","text":"<p>Adds the processed tokens to the checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>dict[str, Any]</code> <p>The checkpoint dictionary.</p> required Source code in <code>src/directmultistep/training/lightning.py</code> <pre><code>def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Adds the processed tokens to the checkpoint.\n\n    Args:\n        checkpoint: The checkpoint dictionary.\n    \"\"\"\n    # Add processed_tokens to the checkpoint dictionary\n    checkpoint[\"processed_tokens\"] = self.processed_tokens\n</code></pre>"},{"location":"DirectMultiStep/training/#directmultistep.training.lightning.LTraining.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>The input batch tensor.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed loss tensor.</p> Source code in <code>src/directmultistep/training/lightning.py</code> <pre><code>def training_step(self, batch: Tensor, batch_idx: int) -&gt; Tensor:\n    \"\"\"Performs a single training step.\n\n    Args:\n        batch: The input batch tensor.\n        batch_idx: The index of the batch.\n\n    Returns:\n        The computed loss tensor.\n    \"\"\"\n    loss = self.compute_loss(batch, batch_idx)\n    self.log_step_info(loss, \"train\", prog_bar=True)\n    return loss\n</code></pre>"},{"location":"DirectMultiStep/training/#directmultistep.training.lightning.LTraining.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>The input batch tensor.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed loss tensor.</p> Source code in <code>src/directmultistep/training/lightning.py</code> <pre><code>def validation_step(self, batch: Tensor, batch_idx: int) -&gt; Tensor:\n    \"\"\"Performs a single validation step.\n\n    Args:\n        batch: The input batch tensor.\n        batch_idx: The index of the batch.\n\n    Returns:\n        The computed loss tensor.\n    \"\"\"\n    loss = self.compute_loss(batch, batch_idx)\n    self.log_step_info(loss, \"val\", prog_bar=True)\n    return loss\n</code></pre>"},{"location":"DirectMultiStep/training/#directmultistep.training.lightning.warmup_and_cosine_decay","title":"<code>warmup_and_cosine_decay(warmup_steps, decay_steps, decay_factor)</code>","text":"<p>Creates a learning rate schedule with warmup and cosine decay.</p> <p>The learning rate increases linearly during the warmup phase, then decreases following a cosine function during the decay phase, and finally remains constant at the decay factor.</p> <p>Parameters:</p> Name Type Description Default <code>warmup_steps</code> <code>int</code> <p>The number of steps for the warmup phase.</p> required <code>decay_steps</code> <code>int</code> <p>The number of steps for the decay phase.</p> required <code>decay_factor</code> <code>float</code> <p>The final learning rate factor after decay.</p> required <p>Returns:</p> Type Description <code>Callable[[int], float]</code> <p>A function that takes the current step as input and returns the</p> <code>Callable[[int], float]</code> <p>corresponding learning rate factor.</p> Source code in <code>src/directmultistep/training/lightning.py</code> <pre><code>def warmup_and_cosine_decay(warmup_steps: int, decay_steps: int, decay_factor: float) -&gt; Callable[[int], float]:\n    \"\"\"Creates a learning rate schedule with warmup and cosine decay.\n\n    The learning rate increases linearly during the warmup phase, then\n    decreases following a cosine function during the decay phase, and\n    finally remains constant at the decay factor.\n\n    Args:\n        warmup_steps: The number of steps for the warmup phase.\n        decay_steps: The number of steps for the decay phase.\n        decay_factor: The final learning rate factor after decay.\n\n    Returns:\n        A function that takes the current step as input and returns the\n        corresponding learning rate factor.\n    \"\"\"\n\n    def _get_new_lr(step: int) -&gt; float:\n        if step &lt; warmup_steps:\n            return step / warmup_steps\n        elif step &gt;= warmup_steps and step &lt; warmup_steps + decay_steps:\n            factor = 0.5 * (1 + np.cos(np.pi * (step - warmup_steps) / decay_steps))\n            return cast(float, max(factor, decay_factor))\n        else:\n            return decay_factor\n\n    return _get_new_lr\n</code></pre>"},{"location":"DirectMultiStep/visualizations/","title":"Visualizing Routes","text":""},{"location":"DirectMultiStep/visualizations/#example-use","title":"Example use","text":"<p>To visualize a path string, you can use the following snippet:</p> <pre><code>from directmultistep.utils.web_visualize import draw_tree_from_path_string\n\npath = \"{'smiles':'O=C(c1ccc(NS(=O)(=O)c2cccc3cccnc23)cc1)N1CCN(CC2CC2)CC1','children':[{'smiles':'O=C(O)c1ccc(NS(=O)(=O)c2cccc3cccnc23)cc1','children':[{'smiles':'CCOC(=O)c1ccc(NS(=O)(=O)c2cccc3cccnc23)cc1','children':[{'smiles':'CCOC(=O)c1ccc(N)cc1'},{'smiles':'O=S(=O)(Cl)c1cccc2cccnc12'}]}]},{'smiles':'C1CN(CC2CC2)CCN1'}]}\"\n\nsvg_str = draw_tree_from_path_string(\n    path_string=path,\n    save_path=Path(\"data/figures/desired_file_name\"),\n    width=400,\n    height=400,\n    x_margin=50,\n    y_margin=100,\n    theme=\"light\",\n)\n</code></pre>"},{"location":"DirectMultiStep/visualizations/#source-code","title":"Source Code","text":""},{"location":"DirectMultiStep/visualizations/#directmultistep.utils.web_visualize","title":"<code>directmultistep.utils.web_visualize</code>","text":""},{"location":"DirectMultiStep/visualizations/#directmultistep.utils.web_visualize.ThemeType","title":"<code>ThemeType = Literal['light', 'dark']</code>  <code>module-attribute</code>","text":""},{"location":"DirectMultiStep/visualizations/#directmultistep.utils.web_visualize.FilteredDict","title":"<code>FilteredDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>A dictionary format for multistep routes, used in DirectMultiStep models.</p> <p>This dictionary is designed to represent a node in a synthetic route tree. It contains the SMILES string of a molecule and a list of its child nodes. To get its string format, use <code>stringify_dict</code>.</p> <p>Attributes:</p> Name Type Description <code>smiles</code> <code>str</code> <p>SMILES string of the molecule.</p> <code>children</code> <code>list[FilteredDict]</code> <p>List of child nodes, each a FilteredDict.</p> Source code in <code>src/directmultistep/utils/pre_process.py</code> <pre><code>class FilteredDict(TypedDict, total=False):\n    \"\"\"A dictionary format for multistep routes, used in DirectMultiStep models.\n\n    This dictionary is designed to represent a node in a synthetic route tree.\n    It contains the SMILES string of a molecule and a list of its child nodes.\n    To get its string format, use `stringify_dict`.\n\n    Attributes:\n        smiles: SMILES string of the molecule.\n        children: List of child nodes, each a FilteredDict.\n    \"\"\"\n\n    smiles: str\n    children: list[\"FilteredDict\"]\n</code></pre>"},{"location":"DirectMultiStep/visualizations/#directmultistep.utils.web_visualize.ColorPalette","title":"<code>ColorPalette</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Defines a color palette for drawing molecules.</p> <p>Attributes:</p> Name Type Description <code>atom_colors</code> <code>dict[int, tuple[float, float, float]]</code> <p>A dictionary mapping atomic numbers to RGB color tuples.</p> <code>annotation</code> <code>tuple[float, float, float, float]</code> <p>An RGBA color tuple for annotations.</p> <code>border</code> <code>tuple[float, float, float]</code> <p>An RGB color tuple for borders.</p> <code>text</code> <code>tuple[float, float, float]</code> <p>An RGB color tuple for text.</p> <code>background</code> <code>tuple[float, float, float, float]</code> <p>An RGBA color tuple for background.</p> Source code in <code>src/directmultistep/utils/web_visualize.py</code> <pre><code>class ColorPalette(NamedTuple):\n    \"\"\"Defines a color palette for drawing molecules.\n\n    Attributes:\n        atom_colors: A dictionary mapping atomic numbers to RGB color tuples.\n        annotation: An RGBA color tuple for annotations.\n        border: An RGB color tuple for borders.\n        text: An RGB color tuple for text.\n        background: An RGBA color tuple for background.\n    \"\"\"\n\n    atom_colors: dict[int, tuple[float, float, float]]\n    annotation: tuple[float, float, float, float]\n    border: tuple[float, float, float]\n    text: tuple[float, float, float]\n    background: tuple[float, float, float, float]\n</code></pre>"},{"location":"DirectMultiStep/visualizations/#directmultistep.utils.web_visualize.RetroSynthesisTree","title":"<code>RetroSynthesisTree</code>","text":"<p>Basic tree structure for retrosynthesis visualization.</p> Source code in <code>src/directmultistep/utils/web_visualize.py</code> <pre><code>class RetroSynthesisTree:\n    \"\"\"Basic tree structure for retrosynthesis visualization.\"\"\"\n\n    def __init__(self, idx: int = 0) -&gt; None:\n        \"\"\"\n        Initializes a new node in the retrosynthesis tree.\n\n        Args:\n            idx: The unique identifier for the node.\n        \"\"\"\n        self.node_id = idx\n        self.smiles = \"\"\n        self.children: list[RetroSynthesisTree] = []\n\n    def build_tree(self, path_dict: FilteredDict) -&gt; int:\n        \"\"\"Recursively builds the retrosynthesis tree from a dictionary.\n\n        Args:\n            path_dict: A dictionary representing the tree structure.\n\n        Returns:\n            The next available node ID.\n        \"\"\"\n        self.smiles = path_dict[\"smiles\"]\n        cur_id = self.node_id + 1\n\n        if \"children\" in path_dict:\n            for child in path_dict[\"children\"]:\n                node = RetroSynthesisTree(idx=cur_id)\n                cur_id = node.build_tree(path_dict=child)\n                self.children.append(node)\n        return cur_id\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns a string representation of the tree node and its children.\"\"\"\n        child_ids = [str(child.node_id) for child in self.children]\n        return f\"Node ID: {self.node_id}, Children: {child_ids}, SMILES: {self.smiles}\\n\" + \"\".join(\n            str(child) for child in self.children\n        )\n</code></pre>"},{"location":"DirectMultiStep/visualizations/#directmultistep.utils.web_visualize.RetroSynthesisTree.__init__","title":"<code>__init__(idx=0)</code>","text":"<p>Initializes a new node in the retrosynthesis tree.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The unique identifier for the node.</p> <code>0</code> Source code in <code>src/directmultistep/utils/web_visualize.py</code> <pre><code>def __init__(self, idx: int = 0) -&gt; None:\n    \"\"\"\n    Initializes a new node in the retrosynthesis tree.\n\n    Args:\n        idx: The unique identifier for the node.\n    \"\"\"\n    self.node_id = idx\n    self.smiles = \"\"\n    self.children: list[RetroSynthesisTree] = []\n</code></pre>"},{"location":"DirectMultiStep/visualizations/#directmultistep.utils.web_visualize.RetroSynthesisTree.__str__","title":"<code>__str__()</code>","text":"<p>Returns a string representation of the tree node and its children.</p> Source code in <code>src/directmultistep/utils/web_visualize.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns a string representation of the tree node and its children.\"\"\"\n    child_ids = [str(child.node_id) for child in self.children]\n    return f\"Node ID: {self.node_id}, Children: {child_ids}, SMILES: {self.smiles}\\n\" + \"\".join(\n        str(child) for child in self.children\n    )\n</code></pre>"},{"location":"DirectMultiStep/visualizations/#directmultistep.utils.web_visualize.RetroSynthesisTree.build_tree","title":"<code>build_tree(path_dict)</code>","text":"<p>Recursively builds the retrosynthesis tree from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>path_dict</code> <code>FilteredDict</code> <p>A dictionary representing the tree structure.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The next available node ID.</p> Source code in <code>src/directmultistep/utils/web_visualize.py</code> <pre><code>def build_tree(self, path_dict: FilteredDict) -&gt; int:\n    \"\"\"Recursively builds the retrosynthesis tree from a dictionary.\n\n    Args:\n        path_dict: A dictionary representing the tree structure.\n\n    Returns:\n        The next available node ID.\n    \"\"\"\n    self.smiles = path_dict[\"smiles\"]\n    cur_id = self.node_id + 1\n\n    if \"children\" in path_dict:\n        for child in path_dict[\"children\"]:\n            node = RetroSynthesisTree(idx=cur_id)\n            cur_id = node.build_tree(path_dict=child)\n            self.children.append(node)\n    return cur_id\n</code></pre>"},{"location":"DirectMultiStep/visualizations/#directmultistep.utils.web_visualize.TreeDimensions","title":"<code>TreeDimensions</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Represents the dimensions of a tree or subtree.</p> Source code in <code>src/directmultistep/utils/web_visualize.py</code> <pre><code>class TreeDimensions(NamedTuple):\n    \"\"\"Represents the dimensions of a tree or subtree.\"\"\"\n\n    width: int\n    height: int\n</code></pre>"},{"location":"DirectMultiStep/visualizations/#directmultistep.utils.web_visualize.compute_subtree_dimensions","title":"<code>compute_subtree_dimensions(tree, img_width, img_height, y_offset)</code>","text":"<p>Compute dimensions of a subtree for layout.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>RetroSynthesisTree</code> <p>The subtree to compute dimensions for.</p> required <code>img_width</code> <code>int</code> <p>The width of the molecule image.</p> required <code>img_height</code> <code>int</code> <p>The height of the molecule image.</p> required <code>y_offset</code> <code>int</code> <p>The vertical offset between nodes.</p> required <p>Returns:</p> Type Description <code>TreeDimensions</code> <p>The dimensions of the subtree.</p> Source code in <code>src/directmultistep/utils/web_visualize.py</code> <pre><code>def compute_subtree_dimensions(\n    tree: RetroSynthesisTree, img_width: int, img_height: int, y_offset: int\n) -&gt; TreeDimensions:\n    \"\"\"Compute dimensions of a subtree for layout.\n\n    Args:\n        tree: The subtree to compute dimensions for.\n        img_width: The width of the molecule image.\n        img_height: The height of the molecule image.\n        y_offset: The vertical offset between nodes.\n\n    Returns:\n        The dimensions of the subtree.\n    \"\"\"\n    if not tree.children:\n        return TreeDimensions(img_width, img_height + y_offset)\n\n    width = img_width\n    height = img_height + y_offset\n\n    for child in tree.children:\n        child_dims = compute_subtree_dimensions(child, img_width, img_height, y_offset)\n        width += child_dims.width\n        height = max(height, child_dims.height + img_height + y_offset)\n\n    return TreeDimensions(width, height)\n</code></pre>"},{"location":"DirectMultiStep/visualizations/#directmultistep.utils.web_visualize.compute_canvas_dimensions","title":"<code>compute_canvas_dimensions(tree, img_width, img_height, y_offset)</code>","text":"<p>Compute overall canvas dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>RetroSynthesisTree</code> <p>The retrosynthesis tree.</p> required <code>img_width</code> <code>int</code> <p>The width of the molecule image.</p> required <code>img_height</code> <code>int</code> <p>The height of the molecule image.</p> required <code>y_offset</code> <code>int</code> <p>The vertical offset between nodes.</p> required <p>Returns:</p> Type Description <code>TreeDimensions</code> <p>The dimensions of the canvas.</p> Source code in <code>src/directmultistep/utils/web_visualize.py</code> <pre><code>def compute_canvas_dimensions(\n    tree: RetroSynthesisTree, img_width: int, img_height: int, y_offset: int\n) -&gt; TreeDimensions:\n    \"\"\"Compute overall canvas dimensions.\n\n    Args:\n        tree: The retrosynthesis tree.\n        img_width: The width of the molecule image.\n        img_height: The height of the molecule image.\n        y_offset: The vertical offset between nodes.\n\n    Returns:\n        The dimensions of the canvas.\n    \"\"\"\n    child_dims = [compute_subtree_dimensions(child, img_width, img_height, y_offset) for child in tree.children]\n    width = sum(d.width for d in child_dims)\n    height = max((d.height for d in child_dims), default=0) + img_height + y_offset\n    return TreeDimensions(width, height + 100)\n</code></pre>"},{"location":"DirectMultiStep/visualizations/#directmultistep.utils.web_visualize.check_overlap","title":"<code>check_overlap(new_x, new_y, existing_boxes, img_width, img_height)</code>","text":"<p>Check if a new node overlaps with existing nodes.</p> <p>Parameters:</p> Name Type Description Default <code>new_x</code> <code>int</code> <p>The x-coordinate of the new node.</p> required <code>new_y</code> <code>int</code> <p>The y-coordinate of the new node.</p> required <code>existing_boxes</code> <code>list[tuple[int, int]]</code> <p>A list of tuples representing the coordinates of existing nodes.</p> required <code>img_width</code> <code>int</code> <p>The width of the molecule image.</p> required <code>img_height</code> <code>int</code> <p>The height of the molecule image.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if there is an overlap, False otherwise.</p> Source code in <code>src/directmultistep/utils/web_visualize.py</code> <pre><code>def check_overlap(\n    new_x: int,\n    new_y: int,\n    existing_boxes: list[tuple[int, int]],\n    img_width: int,\n    img_height: int,\n) -&gt; bool:\n    \"\"\"Check if a new node overlaps with existing nodes.\n\n    Args:\n        new_x: The x-coordinate of the new node.\n        new_y: The y-coordinate of the new node.\n        existing_boxes: A list of tuples representing the coordinates of existing nodes.\n        img_width: The width of the molecule image.\n        img_height: The height of the molecule image.\n\n    Returns:\n        True if there is an overlap, False otherwise.\n    \"\"\"\n    return any(\n        (x - img_width &lt; new_x &lt; x + img_width) and (y - img_height &lt; new_y &lt; y + img_height) for x, y in existing_boxes\n    )\n</code></pre>"},{"location":"DirectMultiStep/visualizations/#directmultistep.utils.web_visualize.draw_molecule","title":"<code>draw_molecule(smiles, size, theme)</code>","text":"<p>Render a SMILES string as base64-encoded PNG.</p> <p>Parameters:</p> Name Type Description Default <code>smiles</code> <code>str</code> <p>The SMILES string of the molecule.</p> required <code>size</code> <code>tuple[int, int]</code> <p>The desired size (width, height) of the image.</p> required <code>theme</code> <code>ThemeType</code> <p>The color theme (\"light\" or \"dark\").</p> required <p>Returns:</p> Type Description <code>str</code> <p>The base64-encoded PNG image data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the SMILES string is invalid.</p> Source code in <code>src/directmultistep/utils/web_visualize.py</code> <pre><code>def draw_molecule(smiles: str, size: tuple[int, int], theme: ThemeType) -&gt; str:\n    \"\"\"Render a SMILES string as base64-encoded PNG.\n\n    Args:\n        smiles: The SMILES string of the molecule.\n        size: The desired size (width, height) of the image.\n        theme: The color theme (\"light\" or \"dark\").\n\n    Returns:\n        The base64-encoded PNG image data.\n\n    Raises:\n        ValueError: If the SMILES string is invalid.\n    \"\"\"\n    mol = Chem.MolFromSmiles(smiles)\n    if mol is None:\n        raise ValueError(f\"Invalid SMILES string: {smiles}\")\n\n    draw_width, draw_height = size\n    drawer = rdMolDraw2D.MolDraw2DCairo(draw_width, draw_height)\n    opts = drawer.drawOptions()\n\n    palette = DARK_PALETTE if theme == \"dark\" else LIGHT_PALETTE\n    background_color = palette.background\n    if not __support_pdf__:\n        background_color = (\n            background_color[0],\n            background_color[1],\n            background_color[2],\n            0,\n        )\n    opts.setBackgroundColour(background_color)\n    opts.setAtomPalette(palette.atom_colors)\n    opts.setAnnotationColour(palette.annotation)\n\n    drawer.DrawMolecule(mol)\n    drawer.FinishDrawing()\n\n    png_data = drawer.GetDrawingText()\n    return base64.b64encode(png_data).decode(\"utf-8\")\n</code></pre>"},{"location":"DirectMultiStep/visualizations/#directmultistep.utils.web_visualize.draw_tree_svg","title":"<code>draw_tree_svg(tree, width, height, x_margin, y_margin, theme, force_canvas_width=None)</code>","text":"<p>Create SVG visualization of the retrosynthesis tree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>RetroSynthesisTree</code> <p>The retrosynthesis tree to visualize.</p> required <code>width</code> <code>int</code> <p>The width of each molecule image.</p> required <code>height</code> <code>int</code> <p>The height of each molecule image.</p> required <code>x_margin</code> <code>int</code> <p>The horizontal margin between nodes.</p> required <code>y_margin</code> <code>int</code> <p>The vertical margin between nodes.</p> required <code>theme</code> <code>ThemeType</code> <p>The color theme (\"light\" or \"dark\").</p> required <code>force_canvas_width</code> <code>int | None</code> <p>An optional width to force for the canvas.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The SVG content as a string.</p> Source code in <code>src/directmultistep/utils/web_visualize.py</code> <pre><code>def draw_tree_svg(\n    tree: RetroSynthesisTree,\n    width: int,\n    height: int,\n    x_margin: int,\n    y_margin: int,\n    theme: ThemeType,\n    force_canvas_width: int | None = None,\n) -&gt; str:\n    \"\"\"Create SVG visualization of the retrosynthesis tree.\n\n    Args:\n        tree: The retrosynthesis tree to visualize.\n        width: The width of each molecule image.\n        height: The height of each molecule image.\n        x_margin: The horizontal margin between nodes.\n        y_margin: The vertical margin between nodes.\n        theme: The color theme (\"light\" or \"dark\").\n        force_canvas_width: An optional width to force for the canvas.\n\n    Returns:\n        The SVG content as a string.\n    \"\"\"\n\n    initial_dims = compute_canvas_dimensions(tree, width, height, y_margin)\n    canvas_width = force_canvas_width if force_canvas_width else initial_dims.width\n    drawing = svgwrite.Drawing(size=(canvas_width, initial_dims.height))\n\n    existing_boxes: list[tuple[int, int]] = []\n    memo = {\"left_x\": float(\"inf\"), \"right_x\": float(\"-inf\")}\n\n    def draw_node(node: RetroSynthesisTree, nx: int, ny: int) -&gt; None:\n        \"\"\"Draws a single node of the retrosynthesis tree.\n\n        Args:\n            node: The tree node to draw.\n            nx: The x-coordinate of the node.\n            ny: The y-coordinate of the node.\n        \"\"\"\n        while check_overlap(nx, ny, existing_boxes, width, height) or check_overlap(\n            nx, ny - y_margin, existing_boxes, width, height\n        ):\n            nx += width // 2\n\n        existing_boxes.append((nx, ny))\n        memo[\"left_x\"] = min(memo[\"left_x\"], nx - width // 2)\n        memo[\"right_x\"] = max(memo[\"right_x\"], nx + width // 2)\n\n        # Draw molecule\n        b64_img = draw_molecule(node.smiles, (width, height), theme)\n        drawing.add(\n            drawing.image(\n                href=f\"data:image/png;base64,{b64_img}\",\n                insert=(nx, ny),\n                size=(width, height),\n            )\n        )\n\n        # Draw border\n        palette = DARK_PALETTE if theme == \"dark\" else LIGHT_PALETTE\n        border_color = svgwrite.rgb(*[c * 255 for c in palette.border])\n\n        box = drawing.rect(\n            insert=(nx, ny),\n            size=(width, height),\n            rx=20,\n            ry=20,\n            fill=\"none\",\n            stroke=border_color,\n            stroke_width=4,\n        )\n        drawing.add(box)\n\n        # Draw node ID\n        text_color = svgwrite.rgb(*[c * 255 for c in palette.text])\n        node_label = drawing.text(\n            f\"ID: {node.node_id}\",\n            insert=(nx, ny + height + 35),\n            fill=text_color,\n            font_size=20,\n            font_family=\"Arial\",\n        )\n        drawing.add(node_label)\n\n        # Draw children\n        child_count = len(node.children)\n        if child_count &gt; 0:\n            next_x = nx if child_count == 1 else nx - (child_count - 1) * width // 2\n            next_y = ny + y_margin + height\n\n            for child in node.children:\n                # Draw connecting line\n                line = drawing.line(\n                    start=(nx + width / 2, ny + height),\n                    end=(next_x + width / 2, next_y),\n                    stroke=border_color,\n                    stroke_width=4,\n                )\n                drawing.add(line)\n\n                draw_node(child, next_x, next_y)\n                next_x += x_margin + width\n\n    # Draw the root\n    root_x = (canvas_width - width) // 2\n    draw_node(tree, root_x, 50)\n\n    # Adjust canvas if needed\n    final_width = int(memo[\"right_x\"] - memo[\"left_x\"] + width * 2 + x_margin * 2)\n    if final_width &gt; canvas_width and force_canvas_width is None:\n        return draw_tree_svg(\n            tree,\n            width,\n            height,\n            x_margin,\n            y_margin,\n            theme,\n            force_canvas_width=final_width,\n        )\n\n    return cast(str, drawing.tostring())\n</code></pre>"},{"location":"DirectMultiStep/visualizations/#directmultistep.utils.web_visualize.create_tree_from_path_string","title":"<code>create_tree_from_path_string(path_string)</code>","text":"<p>Parse a dictionary-like string into a RetroSynthesisTree.</p> <p>Parameters:</p> Name Type Description Default <code>path_string</code> <code>str</code> <p>A string representing the tree structure as a dictionary.</p> required <p>Returns:</p> Type Description <code>RetroSynthesisTree</code> <p>A RetroSynthesisTree object.</p> Source code in <code>src/directmultistep/utils/web_visualize.py</code> <pre><code>def create_tree_from_path_string(path_string: str) -&gt; RetroSynthesisTree:\n    \"\"\"Parse a dictionary-like string into a RetroSynthesisTree.\n\n    Args:\n        path_string: A string representing the tree structure as a dictionary.\n\n    Returns:\n        A RetroSynthesisTree object.\n    \"\"\"\n    path_dict: FilteredDict = eval(path_string)  # TODO: Use safer parsing\n    retro_tree = RetroSynthesisTree()\n    retro_tree.build_tree(path_dict=path_dict)\n    return retro_tree\n</code></pre>"},{"location":"DirectMultiStep/visualizations/#directmultistep.utils.web_visualize.draw_tree_from_path_string","title":"<code>draw_tree_from_path_string(path_string, save_path, width=400, height=400, x_margin=50, y_margin=100, theme='light')</code>","text":"<p>Generate SVG and PDF visualizations from a path string.</p> <p>Parameters:</p> Name Type Description Default <code>path_string</code> <code>str</code> <p>A string representing the tree structure as a dictionary.</p> required <code>save_path</code> <code>Path</code> <p>The path to save the generated SVG and PDF files.</p> required <code>width</code> <code>int</code> <p>The width of each molecule image.</p> <code>400</code> <code>height</code> <code>int</code> <p>The height of each molecule image.</p> <code>400</code> <code>x_margin</code> <code>int</code> <p>The horizontal margin between nodes.</p> <code>50</code> <code>y_margin</code> <code>int</code> <p>The vertical margin between nodes.</p> <code>100</code> <code>theme</code> <code>str</code> <p>The color theme (\"light\" or \"dark\").</p> <code>'light'</code> <p>Returns:</p> Type Description <code>str</code> <p>The SVG content as a string.</p> Source code in <code>src/directmultistep/utils/web_visualize.py</code> <pre><code>def draw_tree_from_path_string(\n    path_string: str,\n    save_path: Path,\n    width: int = 400,\n    height: int = 400,\n    x_margin: int = 50,\n    y_margin: int = 100,\n    theme: str = \"light\",\n) -&gt; str:\n    \"\"\"Generate SVG and PDF visualizations from a path string.\n\n    Args:\n        path_string: A string representing the tree structure as a dictionary.\n        save_path: The path to save the generated SVG and PDF files.\n        width: The width of each molecule image.\n        height: The height of each molecule image.\n        x_margin: The horizontal margin between nodes.\n        y_margin: The vertical margin between nodes.\n        theme: The color theme (\"light\" or \"dark\").\n\n    Returns:\n        The SVG content as a string.\n    \"\"\"\n    assert theme in [\"light\", \"dark\"]\n    theme = cast(ThemeType, theme)\n\n    retro_tree = create_tree_from_path_string(path_string)\n    svg_content = draw_tree_svg(\n        retro_tree,\n        width=width,\n        height=height,\n        x_margin=x_margin,\n        y_margin=y_margin,\n        theme=theme,\n    )\n\n    svg_path = save_path.with_suffix(\".svg\")\n    with open(svg_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(svg_content)\n\n    # Convert to PDF\n    drawing = svg2rlg(str(svg_path))\n    renderPDF.drawToFile(drawing, str(save_path.with_suffix(\".pdf\")))\n    # remove SVG file\n    svg_path.unlink()\n    return svg_content\n</code></pre>"},{"location":"DirectMultiStep/components/attention/","title":"Attention","text":"<p>This document describes the attention mechanisms used in the DMS model.</p>"},{"location":"DirectMultiStep/components/attention/#summary","title":"Summary","text":"<p>The core mechanism of attention emerges from needing to selectively focus on relevant information while processing sequences. When encoding tokens, each position must consider its relationship with all others to capture context. Attention computes similarity scores between each query position and all possible key positions, essentially asking \"how relevant is each key to my current query?\" These raw similarity scores are normalized through softmax to produce attention weights that sum to 1, creating a probability distribution over the keys for each query. The weighted sum of values according to these attention weights produces the final attention output, allowing the model to synthesize information from multiple positions with varying degrees of influence.</p>"},{"location":"DirectMultiStep/components/attention/#flash-attention","title":"Flash Attention","text":"<p>Flash Attention reformulates attention computation to maximize use of fast SRAM cache while minimizing slower DRAM memory access. Rather than computing and storing the full attention matrix at once, it splits the computation into smaller blocks that fit in SRAM, computing partial attention scores and incrementally aggregating them. This tiling approach, combined with local softmax normalization within blocks, achieves mathematically equivalent results while drastically reducing memory bandwidth requirements. The key insight is maintaining rolling statistics of softmax normalization terms across blocks, allowing processing of long sequences without materializing the full attention matrix in memory \u2013 trading increased computation for reduced memory usage, which is favorable on modern hardware where memory bandwidth often constrains performance more than computational capacity.</p>"},{"location":"DirectMultiStep/components/attention/#shape-convention","title":"Shape Convention","text":"<p>The shape suffixes follow a consistent convention:</p> <ul> <li><code>B</code>: Batch size</li> <li><code>L</code>: Target sequence length</li> <li><code>M</code>: Memory/source sequence length</li> <li><code>D</code>: Model hidden dimension</li> <li><code>H</code>: Number of attention heads</li> </ul>"},{"location":"DirectMultiStep/components/attention/#source-code","title":"Source Code","text":""},{"location":"DirectMultiStep/components/attention/#directmultistep.model.components.attention","title":"<code>directmultistep.model.components.attention</code>","text":""},{"location":"DirectMultiStep/components/attention/#directmultistep.model.components.attention.MultiHeadAttentionLayer","title":"<code>MultiHeadAttentionLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Multi-head attention layer.</p> <p>This layer applies multi-head attention to the input tensors.</p> Shape suffixes convention <p>B: batch size L: sequence length for decoder M: memory length (length of sequence being attended to) D: model dimension (sometimes called d_model or embedding_dim) H: number of attention heads in a layer</p> <p>Parameters:</p> Name Type Description Default <code>hid_dim</code> <code>int</code> <p>The hidden dimension size.</p> required <code>n_heads</code> <code>int</code> <p>The number of attention heads.</p> required <code>dropout</code> <code>float</code> <p>The dropout rate.</p> required <code>attn_bias</code> <code>bool</code> <p>Whether to use bias in the linear layers.</p> required Source code in <code>src/directmultistep/model/components/attention.py</code> <pre><code>class MultiHeadAttentionLayer(nn.Module):\n    \"\"\"\n    Multi-head attention layer.\n\n    This layer applies multi-head attention to the input tensors.\n\n    Shape suffixes convention:\n        B: batch size\n        L: sequence length for decoder\n        M: memory length (length of sequence being attended to)\n        D: model dimension (sometimes called d_model or embedding_dim)\n        H: number of attention heads in a layer\n\n    Args:\n        hid_dim: The hidden dimension size.\n        n_heads: The number of attention heads.\n        dropout: The dropout rate.\n        attn_bias: Whether to use bias in the linear layers.\n    \"\"\"\n\n    def __init__(\n        self,\n        hid_dim: int,\n        n_heads: int,\n        dropout: float,\n        attn_bias: bool,\n        # device: torch.device,\n    ):\n        super().__init__()\n\n        self.hid_dim = hid_dim\n        self.n_heads = n_heads\n        self.head_dim = hid_dim // n_heads\n\n        self.query = nn.Linear(hid_dim, hid_dim, bias=attn_bias)\n        self.key = nn.Linear(hid_dim, hid_dim, bias=attn_bias)\n        self.value = nn.Linear(hid_dim, hid_dim, bias=attn_bias)\n\n        self.projection = nn.Linear(hid_dim, hid_dim)\n\n        self.dropout = nn.Dropout(dropout)\n        # self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n\n    def forward(\n        self,\n        query_BLD: Tensor,\n        key_BMD: Tensor,\n        value_BMD: Tensor,\n        mask_B11M: Tensor | None = None,\n    ) -&gt; Tensor:\n        \"\"\"\n        Forward pass of the multi-head attention layer.\n\n        Shape suffixes convention:\n            B: batch size\n            L: sequence length for decoder\n            M: memory length (length of sequence being attended to)\n            D: model dimension (sometimes called d_model or embedding_dim)\n            H: number of attention heads in a layer\n\n        Args:\n            query_BLD: The query tensor of shape (B, L, D).\n            key_BMD: The key tensor of shape (B, M, D).\n            value_BMD: The value tensor of shape (B, M, D).\n            mask_B11M: The attention mask of shape (B, 1, 1, M).\n\n        Returns:\n            The output tensor of shape (B, L, D).\n        \"\"\"\n        B, L, _ = query_BLD.shape\n        Q_BLD = self.query(query_BLD)\n        K_BMD = self.key(key_BMD)\n        V_BMD = self.value(value_BMD)\n        # Reshape into multiple heads\n        Q_BHLD = Q_BLD.view(B, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n        K_BHMD = K_BMD.view(B, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n        V_BHMD = V_BMD.view(B, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n\n        if mask_B11M is not None:\n            # Expand mask for all heads\n            mask_BHLM = mask_B11M.expand(B, self.n_heads, L, -1)\n            is_causal = False\n        else:\n            mask_BHLM = None\n            is_causal = True\n\n        attn_output_BHLD = nn.functional.scaled_dot_product_attention(\n            query=Q_BHLD,\n            key=K_BHMD,\n            value=V_BHMD,\n            attn_mask=mask_BHLM,\n            dropout_p=self.dropout.p if self.training else 0.0,\n            is_causal=is_causal,\n            # scale=self.scale.item(),\n        )\n        attn_output_BLD = attn_output_BHLD.permute(0, 2, 1, 3).contiguous().view(B, L, self.hid_dim)\n        output_BLD = cast(Tensor, self.projection(attn_output_BLD))\n        return output_BLD\n</code></pre>"},{"location":"DirectMultiStep/components/attention/#directmultistep.model.components.attention.MultiHeadAttentionLayer.forward","title":"<code>forward(query_BLD, key_BMD, value_BMD, mask_B11M=None)</code>","text":"<p>Forward pass of the multi-head attention layer.</p> Shape suffixes convention <p>B: batch size L: sequence length for decoder M: memory length (length of sequence being attended to) D: model dimension (sometimes called d_model or embedding_dim) H: number of attention heads in a layer</p> <p>Parameters:</p> Name Type Description Default <code>query_BLD</code> <code>Tensor</code> <p>The query tensor of shape (B, L, D).</p> required <code>key_BMD</code> <code>Tensor</code> <p>The key tensor of shape (B, M, D).</p> required <code>value_BMD</code> <code>Tensor</code> <p>The value tensor of shape (B, M, D).</p> required <code>mask_B11M</code> <code>Tensor | None</code> <p>The attention mask of shape (B, 1, 1, M).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (B, L, D).</p> Source code in <code>src/directmultistep/model/components/attention.py</code> <pre><code>def forward(\n    self,\n    query_BLD: Tensor,\n    key_BMD: Tensor,\n    value_BMD: Tensor,\n    mask_B11M: Tensor | None = None,\n) -&gt; Tensor:\n    \"\"\"\n    Forward pass of the multi-head attention layer.\n\n    Shape suffixes convention:\n        B: batch size\n        L: sequence length for decoder\n        M: memory length (length of sequence being attended to)\n        D: model dimension (sometimes called d_model or embedding_dim)\n        H: number of attention heads in a layer\n\n    Args:\n        query_BLD: The query tensor of shape (B, L, D).\n        key_BMD: The key tensor of shape (B, M, D).\n        value_BMD: The value tensor of shape (B, M, D).\n        mask_B11M: The attention mask of shape (B, 1, 1, M).\n\n    Returns:\n        The output tensor of shape (B, L, D).\n    \"\"\"\n    B, L, _ = query_BLD.shape\n    Q_BLD = self.query(query_BLD)\n    K_BMD = self.key(key_BMD)\n    V_BMD = self.value(value_BMD)\n    # Reshape into multiple heads\n    Q_BHLD = Q_BLD.view(B, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n    K_BHMD = K_BMD.view(B, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n    V_BHMD = V_BMD.view(B, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n\n    if mask_B11M is not None:\n        # Expand mask for all heads\n        mask_BHLM = mask_B11M.expand(B, self.n_heads, L, -1)\n        is_causal = False\n    else:\n        mask_BHLM = None\n        is_causal = True\n\n    attn_output_BHLD = nn.functional.scaled_dot_product_attention(\n        query=Q_BHLD,\n        key=K_BHMD,\n        value=V_BHMD,\n        attn_mask=mask_BHLM,\n        dropout_p=self.dropout.p if self.training else 0.0,\n        is_causal=is_causal,\n        # scale=self.scale.item(),\n    )\n    attn_output_BLD = attn_output_BHLD.permute(0, 2, 1, 3).contiguous().view(B, L, self.hid_dim)\n    output_BLD = cast(Tensor, self.projection(attn_output_BLD))\n    return output_BLD\n</code></pre>"},{"location":"DirectMultiStep/components/decoder/","title":"Decoder","text":"<p>This document describes the decoder components used in the DMS model.</p>"},{"location":"DirectMultiStep/components/decoder/#base-decoder-layer","title":"Base Decoder Layer","text":"<p>The basic building block of the decoder that processes target sequences.</p>"},{"location":"DirectMultiStep/components/decoder/#components","title":"Components","text":""},{"location":"DirectMultiStep/components/decoder/#self-attention-block","title":"Self-Attention Block","text":"<ul> <li>Multi-head self-attention mechanism</li> <li>Causal masking to prevent looking ahead</li> <li>Layer normalization</li> <li>Residual connection</li> </ul>"},{"location":"DirectMultiStep/components/decoder/#cross-attention-block","title":"Cross-Attention Block","text":"<ul> <li>Multi-head attention over encoder outputs</li> <li>Allows decoder to focus on relevant input parts</li> <li>Layer normalization</li> <li>Residual connection</li> </ul>"},{"location":"DirectMultiStep/components/decoder/#feed-forward-block","title":"Feed-Forward Block","text":"<ul> <li>Two-layer feed-forward network</li> <li>Configurable activation function (ReLU or GELU)</li> <li>Layer normalization</li> <li>Residual connection</li> </ul>"},{"location":"DirectMultiStep/components/decoder/#source-code","title":"Source Code","text":""},{"location":"DirectMultiStep/components/decoder/#directmultistep.model.components.decoder","title":"<code>directmultistep.model.components.decoder</code>","text":""},{"location":"DirectMultiStep/components/decoder/#directmultistep.model.components.decoder.Decoder","title":"<code>Decoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>The decoder module.</p> Shape suffixes convention <p>B: batch size C: the length of the input on which conditioning is done    (in our case input_max_length) L: sequence length for decoder, in our case output_max_length D: model dimension (sometimes called d_model or embedding_dim) V: vocabulary size</p> Source code in <code>src/directmultistep/model/components/decoder.py</code> <pre><code>class Decoder(nn.Module):\n    \"\"\"The decoder module.\n\n    Shape suffixes convention:\n        B: batch size\n        C: the length of the input on which conditioning is done\n           (in our case input_max_length)\n        L: sequence length for decoder, in our case output_max_length\n        D: model dimension (sometimes called d_model or embedding_dim)\n        V: vocabulary size\n    \"\"\"\n\n    def __init__(\n        self,\n        vocab_dim: int,\n        hid_dim: int,\n        context_window: int,\n        n_layers: int,\n        n_heads: int,\n        dropout: float,\n        attn_bias: bool,\n        ff_mult: int,\n        ff_activation: str,\n    ) -&gt; None:\n        \"\"\"Initializes the Decoder.\n\n        Args:\n            vocab_dim: The vocabulary size.\n            hid_dim: The hidden dimension size.\n            context_window: The context window size.\n            n_layers: The number of decoder layers.\n            n_heads: The number of attention heads.\n            dropout: The dropout rate.\n            attn_bias: Whether to use bias in the attention layers.\n            ff_mult: The feed-forward expansion factor.\n            ff_activation: The activation function type.\n        \"\"\"\n        super().__init__()\n        self.hid_dim = hid_dim\n        self.tok_embedding = nn.Embedding(vocab_dim, hid_dim)\n        self.pos_embedding = nn.Embedding(context_window, hid_dim)\n\n        self.layers = nn.ModuleList(\n            [\n                DecoderLayer(\n                    hid_dim=hid_dim,\n                    n_heads=n_heads,\n                    dropout=dropout,\n                    attn_bias=attn_bias,\n                    ff_mult=ff_mult,\n                    ff_activation=ff_activation,\n                )\n                for _ in range(n_layers)\n            ]\n        )\n\n        self.fc_out = nn.Linear(hid_dim, vocab_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.scale = torch.sqrt(torch.FloatTensor([hid_dim]))\n\n    def forward(\n        self,\n        trg_BL: Tensor,\n        enc_src_BCD: Tensor,\n        src_mask_B11C: Tensor,\n        trg_mask_B1LL: Tensor | None = None,\n    ) -&gt; Tensor:\n        \"\"\"Forward pass of the Decoder.\n\n        Args:\n            trg_BL: The target sequence tensor of shape (B, L).\n            enc_src_BCD: The encoder output tensor of shape (B, C, D).\n            src_mask_B11C: The source mask tensor of shape (B, 1, 1, C).\n            trg_mask_B1LL: The target mask tensor of shape (B, 1, L, L).\n\n        Returns:\n            The output tensor of shape (B, L, V).\n        \"\"\"\n        B, L = trg_BL.shape\n        # below: [L] -&gt; [1, L] -&gt; [B, L]\n        pos_BL = torch.arange(0, L).unsqueeze(0).repeat(B, 1).to(trg_BL)\n        tok_emb_BLD = self.tok_embedding(trg_BL) * self.scale.to(trg_BL)\n        pos_emb_BLD = self.pos_embedding(pos_BL)\n        trg_BLD = self.dropout(tok_emb_BLD + pos_emb_BLD)\n        for layer in self.layers:\n            trg_BLD = layer(trg_BLD, enc_src_BCD, src_mask_B11C, trg_mask_B1LL)\n        output_BLV = self.fc_out(trg_BLD)\n        return cast(Tensor, output_BLV)\n</code></pre>"},{"location":"DirectMultiStep/components/decoder/#directmultistep.model.components.decoder.Decoder.__init__","title":"<code>__init__(vocab_dim, hid_dim, context_window, n_layers, n_heads, dropout, attn_bias, ff_mult, ff_activation)</code>","text":"<p>Initializes the Decoder.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_dim</code> <code>int</code> <p>The vocabulary size.</p> required <code>hid_dim</code> <code>int</code> <p>The hidden dimension size.</p> required <code>context_window</code> <code>int</code> <p>The context window size.</p> required <code>n_layers</code> <code>int</code> <p>The number of decoder layers.</p> required <code>n_heads</code> <code>int</code> <p>The number of attention heads.</p> required <code>dropout</code> <code>float</code> <p>The dropout rate.</p> required <code>attn_bias</code> <code>bool</code> <p>Whether to use bias in the attention layers.</p> required <code>ff_mult</code> <code>int</code> <p>The feed-forward expansion factor.</p> required <code>ff_activation</code> <code>str</code> <p>The activation function type.</p> required Source code in <code>src/directmultistep/model/components/decoder.py</code> <pre><code>def __init__(\n    self,\n    vocab_dim: int,\n    hid_dim: int,\n    context_window: int,\n    n_layers: int,\n    n_heads: int,\n    dropout: float,\n    attn_bias: bool,\n    ff_mult: int,\n    ff_activation: str,\n) -&gt; None:\n    \"\"\"Initializes the Decoder.\n\n    Args:\n        vocab_dim: The vocabulary size.\n        hid_dim: The hidden dimension size.\n        context_window: The context window size.\n        n_layers: The number of decoder layers.\n        n_heads: The number of attention heads.\n        dropout: The dropout rate.\n        attn_bias: Whether to use bias in the attention layers.\n        ff_mult: The feed-forward expansion factor.\n        ff_activation: The activation function type.\n    \"\"\"\n    super().__init__()\n    self.hid_dim = hid_dim\n    self.tok_embedding = nn.Embedding(vocab_dim, hid_dim)\n    self.pos_embedding = nn.Embedding(context_window, hid_dim)\n\n    self.layers = nn.ModuleList(\n        [\n            DecoderLayer(\n                hid_dim=hid_dim,\n                n_heads=n_heads,\n                dropout=dropout,\n                attn_bias=attn_bias,\n                ff_mult=ff_mult,\n                ff_activation=ff_activation,\n            )\n            for _ in range(n_layers)\n        ]\n    )\n\n    self.fc_out = nn.Linear(hid_dim, vocab_dim)\n    self.dropout = nn.Dropout(dropout)\n    self.scale = torch.sqrt(torch.FloatTensor([hid_dim]))\n</code></pre>"},{"location":"DirectMultiStep/components/decoder/#directmultistep.model.components.decoder.Decoder.forward","title":"<code>forward(trg_BL, enc_src_BCD, src_mask_B11C, trg_mask_B1LL=None)</code>","text":"<p>Forward pass of the Decoder.</p> <p>Parameters:</p> Name Type Description Default <code>trg_BL</code> <code>Tensor</code> <p>The target sequence tensor of shape (B, L).</p> required <code>enc_src_BCD</code> <code>Tensor</code> <p>The encoder output tensor of shape (B, C, D).</p> required <code>src_mask_B11C</code> <code>Tensor</code> <p>The source mask tensor of shape (B, 1, 1, C).</p> required <code>trg_mask_B1LL</code> <code>Tensor | None</code> <p>The target mask tensor of shape (B, 1, L, L).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (B, L, V).</p> Source code in <code>src/directmultistep/model/components/decoder.py</code> <pre><code>def forward(\n    self,\n    trg_BL: Tensor,\n    enc_src_BCD: Tensor,\n    src_mask_B11C: Tensor,\n    trg_mask_B1LL: Tensor | None = None,\n) -&gt; Tensor:\n    \"\"\"Forward pass of the Decoder.\n\n    Args:\n        trg_BL: The target sequence tensor of shape (B, L).\n        enc_src_BCD: The encoder output tensor of shape (B, C, D).\n        src_mask_B11C: The source mask tensor of shape (B, 1, 1, C).\n        trg_mask_B1LL: The target mask tensor of shape (B, 1, L, L).\n\n    Returns:\n        The output tensor of shape (B, L, V).\n    \"\"\"\n    B, L = trg_BL.shape\n    # below: [L] -&gt; [1, L] -&gt; [B, L]\n    pos_BL = torch.arange(0, L).unsqueeze(0).repeat(B, 1).to(trg_BL)\n    tok_emb_BLD = self.tok_embedding(trg_BL) * self.scale.to(trg_BL)\n    pos_emb_BLD = self.pos_embedding(pos_BL)\n    trg_BLD = self.dropout(tok_emb_BLD + pos_emb_BLD)\n    for layer in self.layers:\n        trg_BLD = layer(trg_BLD, enc_src_BCD, src_mask_B11C, trg_mask_B1LL)\n    output_BLV = self.fc_out(trg_BLD)\n    return cast(Tensor, output_BLV)\n</code></pre>"},{"location":"DirectMultiStep/components/decoder/#directmultistep.model.components.decoder.DecoderLayer","title":"<code>DecoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A single layer of the decoder.</p> Shape suffixes convention <p>B: batch size C: the length of the input on which conditioning is done    (in our case input_max_length) L: sequence length for decoder, in our case output_max_length D: model dimension (sometimes called d_model or embedding_dim)</p> Source code in <code>src/directmultistep/model/components/decoder.py</code> <pre><code>class DecoderLayer(nn.Module):\n    \"\"\"A single layer of the decoder.\n\n    Shape suffixes convention:\n        B: batch size\n        C: the length of the input on which conditioning is done\n           (in our case input_max_length)\n        L: sequence length for decoder, in our case output_max_length\n        D: model dimension (sometimes called d_model or embedding_dim)\n    \"\"\"\n\n    def __init__(\n        self,\n        hid_dim: int,\n        n_heads: int,\n        dropout: float,\n        attn_bias: bool,\n        ff_mult: int,\n        ff_activation: str,\n    ) -&gt; None:\n        \"\"\"Initializes the DecoderLayer.\n\n        Args:\n            hid_dim: The hidden dimension size.\n            n_heads: The number of attention heads.\n            dropout: The dropout rate.\n            attn_bias: Whether to use bias in the attention layers.\n            ff_mult: The feed-forward expansion factor.\n            ff_activation: The activation function type.\n        \"\"\"\n        super().__init__()\n        self.self_attn_ln = nn.LayerNorm(hid_dim)\n        self.enc_attn_ln = nn.LayerNorm(hid_dim)\n        self.ff_ln = nn.LayerNorm(hid_dim)\n        self.self_attn = MultiHeadAttentionLayer(\n            hid_dim=hid_dim,\n            n_heads=n_heads,\n            dropout=dropout,\n            attn_bias=attn_bias,\n        )\n        self.encoder_attn = MultiHeadAttentionLayer(\n            hid_dim=hid_dim,\n            n_heads=n_heads,\n            dropout=dropout,\n            attn_bias=attn_bias,\n        )\n        self.mlp: nn.Module = PositionwiseFeedforwardLayer(\n            hid_dim=hid_dim,\n            ff_mult=ff_mult,\n            ff_activation=activation_dict[ff_activation],\n            dropout=dropout,\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(\n        self,\n        trg_BLD: Tensor,\n        enc_src_BCD: Tensor,\n        src_mask_B11C: Tensor,\n        trg_mask_B1LL: Tensor,\n    ) -&gt; Tensor:\n        \"\"\"Forward pass of the DecoderLayer.\n\n        Args:\n            trg_BLD: The target sequence tensor of shape (B, L, D).\n            enc_src_BCD: The encoder output tensor of shape (B, C, D).\n            src_mask_B11C: The source mask tensor of shape (B, 1, 1, C).\n            trg_mask_B1LL: The target mask tensor of shape (B, 1, L, L).\n\n        Returns:\n            The output tensor of shape (B, L, D).\n        \"\"\"\n        self_attn_BLD = self.self_attn(trg_BLD, trg_BLD, trg_BLD, trg_mask_B1LL)\n        trg_BLD = self.self_attn_ln(trg_BLD + self.dropout(self_attn_BLD))\n        # Encoder-Decoder Attetion\n        enc_attn_BLD = self.encoder_attn(trg_BLD, enc_src_BCD, enc_src_BCD, src_mask_B11C)\n        trg_BLD = self.enc_attn_ln(trg_BLD + self.dropout(enc_attn_BLD))\n        ff_out_BLD = self.mlp(trg_BLD)\n        trg_BLD = self.ff_ln(trg_BLD + self.dropout(ff_out_BLD))\n        return trg_BLD\n</code></pre>"},{"location":"DirectMultiStep/components/decoder/#directmultistep.model.components.decoder.DecoderLayer.__init__","title":"<code>__init__(hid_dim, n_heads, dropout, attn_bias, ff_mult, ff_activation)</code>","text":"<p>Initializes the DecoderLayer.</p> <p>Parameters:</p> Name Type Description Default <code>hid_dim</code> <code>int</code> <p>The hidden dimension size.</p> required <code>n_heads</code> <code>int</code> <p>The number of attention heads.</p> required <code>dropout</code> <code>float</code> <p>The dropout rate.</p> required <code>attn_bias</code> <code>bool</code> <p>Whether to use bias in the attention layers.</p> required <code>ff_mult</code> <code>int</code> <p>The feed-forward expansion factor.</p> required <code>ff_activation</code> <code>str</code> <p>The activation function type.</p> required Source code in <code>src/directmultistep/model/components/decoder.py</code> <pre><code>def __init__(\n    self,\n    hid_dim: int,\n    n_heads: int,\n    dropout: float,\n    attn_bias: bool,\n    ff_mult: int,\n    ff_activation: str,\n) -&gt; None:\n    \"\"\"Initializes the DecoderLayer.\n\n    Args:\n        hid_dim: The hidden dimension size.\n        n_heads: The number of attention heads.\n        dropout: The dropout rate.\n        attn_bias: Whether to use bias in the attention layers.\n        ff_mult: The feed-forward expansion factor.\n        ff_activation: The activation function type.\n    \"\"\"\n    super().__init__()\n    self.self_attn_ln = nn.LayerNorm(hid_dim)\n    self.enc_attn_ln = nn.LayerNorm(hid_dim)\n    self.ff_ln = nn.LayerNorm(hid_dim)\n    self.self_attn = MultiHeadAttentionLayer(\n        hid_dim=hid_dim,\n        n_heads=n_heads,\n        dropout=dropout,\n        attn_bias=attn_bias,\n    )\n    self.encoder_attn = MultiHeadAttentionLayer(\n        hid_dim=hid_dim,\n        n_heads=n_heads,\n        dropout=dropout,\n        attn_bias=attn_bias,\n    )\n    self.mlp: nn.Module = PositionwiseFeedforwardLayer(\n        hid_dim=hid_dim,\n        ff_mult=ff_mult,\n        ff_activation=activation_dict[ff_activation],\n        dropout=dropout,\n    )\n    self.dropout = nn.Dropout(dropout)\n</code></pre>"},{"location":"DirectMultiStep/components/decoder/#directmultistep.model.components.decoder.DecoderLayer.forward","title":"<code>forward(trg_BLD, enc_src_BCD, src_mask_B11C, trg_mask_B1LL)</code>","text":"<p>Forward pass of the DecoderLayer.</p> <p>Parameters:</p> Name Type Description Default <code>trg_BLD</code> <code>Tensor</code> <p>The target sequence tensor of shape (B, L, D).</p> required <code>enc_src_BCD</code> <code>Tensor</code> <p>The encoder output tensor of shape (B, C, D).</p> required <code>src_mask_B11C</code> <code>Tensor</code> <p>The source mask tensor of shape (B, 1, 1, C).</p> required <code>trg_mask_B1LL</code> <code>Tensor</code> <p>The target mask tensor of shape (B, 1, L, L).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (B, L, D).</p> Source code in <code>src/directmultistep/model/components/decoder.py</code> <pre><code>def forward(\n    self,\n    trg_BLD: Tensor,\n    enc_src_BCD: Tensor,\n    src_mask_B11C: Tensor,\n    trg_mask_B1LL: Tensor,\n) -&gt; Tensor:\n    \"\"\"Forward pass of the DecoderLayer.\n\n    Args:\n        trg_BLD: The target sequence tensor of shape (B, L, D).\n        enc_src_BCD: The encoder output tensor of shape (B, C, D).\n        src_mask_B11C: The source mask tensor of shape (B, 1, 1, C).\n        trg_mask_B1LL: The target mask tensor of shape (B, 1, L, L).\n\n    Returns:\n        The output tensor of shape (B, L, D).\n    \"\"\"\n    self_attn_BLD = self.self_attn(trg_BLD, trg_BLD, trg_BLD, trg_mask_B1LL)\n    trg_BLD = self.self_attn_ln(trg_BLD + self.dropout(self_attn_BLD))\n    # Encoder-Decoder Attetion\n    enc_attn_BLD = self.encoder_attn(trg_BLD, enc_src_BCD, enc_src_BCD, src_mask_B11C)\n    trg_BLD = self.enc_attn_ln(trg_BLD + self.dropout(enc_attn_BLD))\n    ff_out_BLD = self.mlp(trg_BLD)\n    trg_BLD = self.ff_ln(trg_BLD + self.dropout(ff_out_BLD))\n    return trg_BLD\n</code></pre>"},{"location":"DirectMultiStep/components/decoder/#directmultistep.model.components.decoder.MoEDecoder","title":"<code>MoEDecoder</code>","text":"<p>               Bases: <code>Decoder</code></p> <p>The decoder module with Mixture of Experts in the feedforward layers.</p> Source code in <code>src/directmultistep/model/components/decoder.py</code> <pre><code>class MoEDecoder(Decoder):\n    \"\"\"The decoder module with Mixture of Experts in the feedforward layers.\"\"\"\n\n    def __init__(\n        self,\n        vocab_dim: int,\n        hid_dim: int,\n        context_window: int,\n        n_layers: int,\n        n_heads: int,\n        dropout: float,\n        attn_bias: bool,\n        ff_mult: int,\n        ff_activation: str,\n        n_experts: int,\n        top_k: int,\n        capacity_factor: float,\n    ):\n        \"\"\"Initializes the MoEDecoder.\n\n        Args:\n            vocab_dim: The vocabulary size.\n            hid_dim: The hidden dimension size.\n            context_window: The context window size.\n            n_layers: The number of decoder layers.\n            n_heads: The number of attention heads.\n            dropout: The dropout rate.\n            attn_bias: Whether to use bias in the attention layers.\n            ff_mult: The feed-forward expansion factor.\n            ff_activation: The activation function type.\n            n_experts: The number of experts in the MoE layer.\n            top_k: The number of experts to use in the MoE layer.\n            capacity_factor: The capacity factor for the MoE layer.\n        \"\"\"\n        super().__init__(\n            vocab_dim=vocab_dim,\n            hid_dim=hid_dim,\n            context_window=context_window,\n            n_layers=n_layers,\n            n_heads=n_heads,\n            dropout=dropout,\n            attn_bias=attn_bias,\n            ff_mult=ff_mult,\n            ff_activation=ff_activation,\n        )\n        # Override layers with MoE layers\n        self.layers = nn.ModuleList(\n            [\n                MoEDecoderLayer(\n                    hid_dim=hid_dim,\n                    n_heads=n_heads,\n                    dropout=dropout,\n                    attn_bias=attn_bias,\n                    ff_mult=ff_mult,\n                    ff_activation=ff_activation,\n                    n_experts=n_experts,\n                    top_k=top_k,\n                    capacity_factor=capacity_factor,\n                )\n                for _ in range(n_layers)\n            ]\n        )\n</code></pre>"},{"location":"DirectMultiStep/components/decoder/#directmultistep.model.components.decoder.MoEDecoder.__init__","title":"<code>__init__(vocab_dim, hid_dim, context_window, n_layers, n_heads, dropout, attn_bias, ff_mult, ff_activation, n_experts, top_k, capacity_factor)</code>","text":"<p>Initializes the MoEDecoder.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_dim</code> <code>int</code> <p>The vocabulary size.</p> required <code>hid_dim</code> <code>int</code> <p>The hidden dimension size.</p> required <code>context_window</code> <code>int</code> <p>The context window size.</p> required <code>n_layers</code> <code>int</code> <p>The number of decoder layers.</p> required <code>n_heads</code> <code>int</code> <p>The number of attention heads.</p> required <code>dropout</code> <code>float</code> <p>The dropout rate.</p> required <code>attn_bias</code> <code>bool</code> <p>Whether to use bias in the attention layers.</p> required <code>ff_mult</code> <code>int</code> <p>The feed-forward expansion factor.</p> required <code>ff_activation</code> <code>str</code> <p>The activation function type.</p> required <code>n_experts</code> <code>int</code> <p>The number of experts in the MoE layer.</p> required <code>top_k</code> <code>int</code> <p>The number of experts to use in the MoE layer.</p> required <code>capacity_factor</code> <code>float</code> <p>The capacity factor for the MoE layer.</p> required Source code in <code>src/directmultistep/model/components/decoder.py</code> <pre><code>def __init__(\n    self,\n    vocab_dim: int,\n    hid_dim: int,\n    context_window: int,\n    n_layers: int,\n    n_heads: int,\n    dropout: float,\n    attn_bias: bool,\n    ff_mult: int,\n    ff_activation: str,\n    n_experts: int,\n    top_k: int,\n    capacity_factor: float,\n):\n    \"\"\"Initializes the MoEDecoder.\n\n    Args:\n        vocab_dim: The vocabulary size.\n        hid_dim: The hidden dimension size.\n        context_window: The context window size.\n        n_layers: The number of decoder layers.\n        n_heads: The number of attention heads.\n        dropout: The dropout rate.\n        attn_bias: Whether to use bias in the attention layers.\n        ff_mult: The feed-forward expansion factor.\n        ff_activation: The activation function type.\n        n_experts: The number of experts in the MoE layer.\n        top_k: The number of experts to use in the MoE layer.\n        capacity_factor: The capacity factor for the MoE layer.\n    \"\"\"\n    super().__init__(\n        vocab_dim=vocab_dim,\n        hid_dim=hid_dim,\n        context_window=context_window,\n        n_layers=n_layers,\n        n_heads=n_heads,\n        dropout=dropout,\n        attn_bias=attn_bias,\n        ff_mult=ff_mult,\n        ff_activation=ff_activation,\n    )\n    # Override layers with MoE layers\n    self.layers = nn.ModuleList(\n        [\n            MoEDecoderLayer(\n                hid_dim=hid_dim,\n                n_heads=n_heads,\n                dropout=dropout,\n                attn_bias=attn_bias,\n                ff_mult=ff_mult,\n                ff_activation=ff_activation,\n                n_experts=n_experts,\n                top_k=top_k,\n                capacity_factor=capacity_factor,\n            )\n            for _ in range(n_layers)\n        ]\n    )\n</code></pre>"},{"location":"DirectMultiStep/components/decoder/#directmultistep.model.components.decoder.MoEDecoderLayer","title":"<code>MoEDecoderLayer</code>","text":"<p>               Bases: <code>DecoderLayer</code></p> <p>A single layer of the decoder with Mixture of Experts in the feedforward layer.</p> Source code in <code>src/directmultistep/model/components/decoder.py</code> <pre><code>class MoEDecoderLayer(DecoderLayer):\n    \"\"\"A single layer of the decoder with Mixture of Experts in the feedforward layer.\"\"\"\n\n    def __init__(\n        self,\n        hid_dim: int,\n        n_heads: int,\n        dropout: float,\n        attn_bias: bool,\n        ff_mult: int,\n        ff_activation: str,\n        n_experts: int,\n        top_k: int,\n        capacity_factor: float,\n    ) -&gt; None:\n        \"\"\"Initializes the MoEDecoderLayer.\n\n        Args:\n            hid_dim: The hidden dimension size.\n            n_heads: The number of attention heads.\n            dropout: The dropout rate.\n            attn_bias: Whether to use bias in the attention layers.\n            ff_mult: The feed-forward expansion factor.\n            ff_activation: The activation function type.\n            n_experts: The number of experts in the MoE layer.\n            top_k: The number of experts to use in the MoE layer.\n            capacity_factor: The capacity factor for the MoE layer.\n        \"\"\"\n        super().__init__(\n            hid_dim=hid_dim,\n            n_heads=n_heads,\n            dropout=dropout,\n            attn_bias=attn_bias,\n            ff_mult=ff_mult,\n            ff_activation=ff_activation,\n        )\n        # Override the MLP with MoE\n        self.mlp = SparseMoE(\n            hid_dim=hid_dim,\n            n_experts=n_experts,\n            top_k=top_k,\n            ff_mult=ff_mult,\n            ff_activation=ff_activation,\n            dropout=dropout,\n            capacity_factor=capacity_factor,\n        )\n</code></pre>"},{"location":"DirectMultiStep/components/decoder/#directmultistep.model.components.decoder.MoEDecoderLayer.__init__","title":"<code>__init__(hid_dim, n_heads, dropout, attn_bias, ff_mult, ff_activation, n_experts, top_k, capacity_factor)</code>","text":"<p>Initializes the MoEDecoderLayer.</p> <p>Parameters:</p> Name Type Description Default <code>hid_dim</code> <code>int</code> <p>The hidden dimension size.</p> required <code>n_heads</code> <code>int</code> <p>The number of attention heads.</p> required <code>dropout</code> <code>float</code> <p>The dropout rate.</p> required <code>attn_bias</code> <code>bool</code> <p>Whether to use bias in the attention layers.</p> required <code>ff_mult</code> <code>int</code> <p>The feed-forward expansion factor.</p> required <code>ff_activation</code> <code>str</code> <p>The activation function type.</p> required <code>n_experts</code> <code>int</code> <p>The number of experts in the MoE layer.</p> required <code>top_k</code> <code>int</code> <p>The number of experts to use in the MoE layer.</p> required <code>capacity_factor</code> <code>float</code> <p>The capacity factor for the MoE layer.</p> required Source code in <code>src/directmultistep/model/components/decoder.py</code> <pre><code>def __init__(\n    self,\n    hid_dim: int,\n    n_heads: int,\n    dropout: float,\n    attn_bias: bool,\n    ff_mult: int,\n    ff_activation: str,\n    n_experts: int,\n    top_k: int,\n    capacity_factor: float,\n) -&gt; None:\n    \"\"\"Initializes the MoEDecoderLayer.\n\n    Args:\n        hid_dim: The hidden dimension size.\n        n_heads: The number of attention heads.\n        dropout: The dropout rate.\n        attn_bias: Whether to use bias in the attention layers.\n        ff_mult: The feed-forward expansion factor.\n        ff_activation: The activation function type.\n        n_experts: The number of experts in the MoE layer.\n        top_k: The number of experts to use in the MoE layer.\n        capacity_factor: The capacity factor for the MoE layer.\n    \"\"\"\n    super().__init__(\n        hid_dim=hid_dim,\n        n_heads=n_heads,\n        dropout=dropout,\n        attn_bias=attn_bias,\n        ff_mult=ff_mult,\n        ff_activation=ff_activation,\n    )\n    # Override the MLP with MoE\n    self.mlp = SparseMoE(\n        hid_dim=hid_dim,\n        n_experts=n_experts,\n        top_k=top_k,\n        ff_mult=ff_mult,\n        ff_activation=ff_activation,\n        dropout=dropout,\n        capacity_factor=capacity_factor,\n    )\n</code></pre>"},{"location":"DirectMultiStep/components/encoder/","title":"Encoder","text":"<p>This document describes the encoder components used in the DMS model.</p>"},{"location":"DirectMultiStep/components/encoder/#base-encoder-layer","title":"Base Encoder Layer","text":"<p>The basic building block of the encoder that processes input sequences.</p>"},{"location":"DirectMultiStep/components/encoder/#components","title":"Components","text":""},{"location":"DirectMultiStep/components/encoder/#self-attention-block","title":"Self-Attention Block","text":"<ul> <li>Multi-head self-attention mechanism</li> <li>Layer normalization</li> <li>Residual connection</li> </ul>"},{"location":"DirectMultiStep/components/encoder/#feed-forward-block","title":"Feed-Forward Block","text":"<ul> <li>Two-layer feed-forward network</li> <li>Configurable activation function (ReLU or GELU)</li> <li>Layer normalization</li> <li>Residual connection</li> </ul>"},{"location":"DirectMultiStep/components/encoder/#source-code","title":"Source Code","text":""},{"location":"DirectMultiStep/components/encoder/#directmultistep.model.components.encoder","title":"<code>directmultistep.model.components.encoder</code>","text":""},{"location":"DirectMultiStep/components/encoder/#directmultistep.model.components.encoder.Encoder","title":"<code>Encoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>The encoder module.</p> Shape suffixes convention <p>B: batch size C: the length of the input on which conditioning is done    (in our case input_max_length) D: model dimension (sometimes called d_model or embedding_dim)</p> Source code in <code>src/directmultistep/model/components/encoder.py</code> <pre><code>class Encoder(nn.Module):\n    \"\"\"The encoder module.\n\n    Shape suffixes convention:\n        B: batch size\n        C: the length of the input on which conditioning is done\n           (in our case input_max_length)\n        D: model dimension (sometimes called d_model or embedding_dim)\n    \"\"\"\n\n    def __init__(\n        self,\n        vocab_dim: int,\n        hid_dim: int,\n        context_window: int,\n        n_layers: int,\n        n_heads: int,\n        ff_mult: int,\n        ff_activation: str,\n        dropout: float,\n        attn_bias: bool,\n        initiate_steps: bool,\n        include_steps: bool,\n    ):\n        \"\"\"Initializes the Encoder.\n\n        Args:\n            vocab_dim: The vocabulary dimension size.\n            hid_dim: The hidden dimension size.\n            context_window: The context window size.\n            n_layers: The number of encoder layers.\n            n_heads: The number of attention heads.\n            ff_mult: The feed-forward expansion factor.\n            ff_activation: The activation function type.\n            dropout: The dropout rate.\n            attn_bias: Whether to use bias in the attention layers.\n            initiate_steps: Whether to initiate step embeddings.\n            include_steps: Whether to include step embeddings.\n        \"\"\"\n        super().__init__()\n\n        self.tok_embedding = nn.Embedding(vocab_dim, hid_dim)\n        self.pos_embedding = nn.Embedding(context_window, hid_dim)\n        if initiate_steps:\n            self.step_embedding = nn.Embedding(1, hid_dim)\n        self.include_steps = include_steps\n\n        self.layers = nn.ModuleList(\n            [\n                EncoderLayer(\n                    hid_dim=hid_dim,\n                    n_heads=n_heads,\n                    ff_mult=ff_mult,\n                    ff_activation=ff_activation,\n                    dropout=dropout,\n                    attn_bias=attn_bias,\n                )\n                for _ in range(n_layers)\n            ]\n        )\n        self.dropout = nn.Dropout(dropout)\n        self.scale = torch.sqrt(torch.FloatTensor([hid_dim]))\n\n    def forward(self, src_BC: Tensor, src_mask_B11C: Tensor, steps_B1: Tensor) -&gt; Tensor:\n        \"\"\"Forward pass of the Encoder.\n\n        Args:\n            src_BC: The source input tensor of shape (B, C).\n            src_mask_B11C: The source mask tensor of shape (B, 1, 1, C).\n            steps_B1: The step tensor of shape (B, 1).\n\n        Returns:\n            The output tensor of shape (B, C, D).\n        \"\"\"\n        B, C = src_BC.shape\n        tok_emb_BCD = self.tok_embedding(src_BC) * self.scale.to(src_BC)\n        # below [C] -&gt; [1, C] -&gt; [B, C]\n        pos_BC = torch.arange(0, C).unsqueeze(0).repeat(B, 1).to(src_BC)\n        pos_emb_BCD = self.pos_embedding(pos_BC)\n        comb_BCD = tok_emb_BCD + pos_emb_BCD\n        if self.include_steps:\n            # [C] -&gt; [1, C] -&gt; [B, C]\n            step_BC = torch.zeros(C).unsqueeze(0).repeat(B, 1).long().to(src_BC)\n            step_emb_BCD = self.step_embedding(step_BC) * steps_B1.view(-1, 1, 1)\n            comb_BCD += step_emb_BCD\n        src_BCD = self.dropout(comb_BCD)\n        for layer in self.layers:\n            src_BCD = layer(src_BCD, src_mask_B11C)\n        return cast(Tensor, src_BCD)\n</code></pre>"},{"location":"DirectMultiStep/components/encoder/#directmultistep.model.components.encoder.Encoder.__init__","title":"<code>__init__(vocab_dim, hid_dim, context_window, n_layers, n_heads, ff_mult, ff_activation, dropout, attn_bias, initiate_steps, include_steps)</code>","text":"<p>Initializes the Encoder.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_dim</code> <code>int</code> <p>The vocabulary dimension size.</p> required <code>hid_dim</code> <code>int</code> <p>The hidden dimension size.</p> required <code>context_window</code> <code>int</code> <p>The context window size.</p> required <code>n_layers</code> <code>int</code> <p>The number of encoder layers.</p> required <code>n_heads</code> <code>int</code> <p>The number of attention heads.</p> required <code>ff_mult</code> <code>int</code> <p>The feed-forward expansion factor.</p> required <code>ff_activation</code> <code>str</code> <p>The activation function type.</p> required <code>dropout</code> <code>float</code> <p>The dropout rate.</p> required <code>attn_bias</code> <code>bool</code> <p>Whether to use bias in the attention layers.</p> required <code>initiate_steps</code> <code>bool</code> <p>Whether to initiate step embeddings.</p> required <code>include_steps</code> <code>bool</code> <p>Whether to include step embeddings.</p> required Source code in <code>src/directmultistep/model/components/encoder.py</code> <pre><code>def __init__(\n    self,\n    vocab_dim: int,\n    hid_dim: int,\n    context_window: int,\n    n_layers: int,\n    n_heads: int,\n    ff_mult: int,\n    ff_activation: str,\n    dropout: float,\n    attn_bias: bool,\n    initiate_steps: bool,\n    include_steps: bool,\n):\n    \"\"\"Initializes the Encoder.\n\n    Args:\n        vocab_dim: The vocabulary dimension size.\n        hid_dim: The hidden dimension size.\n        context_window: The context window size.\n        n_layers: The number of encoder layers.\n        n_heads: The number of attention heads.\n        ff_mult: The feed-forward expansion factor.\n        ff_activation: The activation function type.\n        dropout: The dropout rate.\n        attn_bias: Whether to use bias in the attention layers.\n        initiate_steps: Whether to initiate step embeddings.\n        include_steps: Whether to include step embeddings.\n    \"\"\"\n    super().__init__()\n\n    self.tok_embedding = nn.Embedding(vocab_dim, hid_dim)\n    self.pos_embedding = nn.Embedding(context_window, hid_dim)\n    if initiate_steps:\n        self.step_embedding = nn.Embedding(1, hid_dim)\n    self.include_steps = include_steps\n\n    self.layers = nn.ModuleList(\n        [\n            EncoderLayer(\n                hid_dim=hid_dim,\n                n_heads=n_heads,\n                ff_mult=ff_mult,\n                ff_activation=ff_activation,\n                dropout=dropout,\n                attn_bias=attn_bias,\n            )\n            for _ in range(n_layers)\n        ]\n    )\n    self.dropout = nn.Dropout(dropout)\n    self.scale = torch.sqrt(torch.FloatTensor([hid_dim]))\n</code></pre>"},{"location":"DirectMultiStep/components/encoder/#directmultistep.model.components.encoder.Encoder.forward","title":"<code>forward(src_BC, src_mask_B11C, steps_B1)</code>","text":"<p>Forward pass of the Encoder.</p> <p>Parameters:</p> Name Type Description Default <code>src_BC</code> <code>Tensor</code> <p>The source input tensor of shape (B, C).</p> required <code>src_mask_B11C</code> <code>Tensor</code> <p>The source mask tensor of shape (B, 1, 1, C).</p> required <code>steps_B1</code> <code>Tensor</code> <p>The step tensor of shape (B, 1).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (B, C, D).</p> Source code in <code>src/directmultistep/model/components/encoder.py</code> <pre><code>def forward(self, src_BC: Tensor, src_mask_B11C: Tensor, steps_B1: Tensor) -&gt; Tensor:\n    \"\"\"Forward pass of the Encoder.\n\n    Args:\n        src_BC: The source input tensor of shape (B, C).\n        src_mask_B11C: The source mask tensor of shape (B, 1, 1, C).\n        steps_B1: The step tensor of shape (B, 1).\n\n    Returns:\n        The output tensor of shape (B, C, D).\n    \"\"\"\n    B, C = src_BC.shape\n    tok_emb_BCD = self.tok_embedding(src_BC) * self.scale.to(src_BC)\n    # below [C] -&gt; [1, C] -&gt; [B, C]\n    pos_BC = torch.arange(0, C).unsqueeze(0).repeat(B, 1).to(src_BC)\n    pos_emb_BCD = self.pos_embedding(pos_BC)\n    comb_BCD = tok_emb_BCD + pos_emb_BCD\n    if self.include_steps:\n        # [C] -&gt; [1, C] -&gt; [B, C]\n        step_BC = torch.zeros(C).unsqueeze(0).repeat(B, 1).long().to(src_BC)\n        step_emb_BCD = self.step_embedding(step_BC) * steps_B1.view(-1, 1, 1)\n        comb_BCD += step_emb_BCD\n    src_BCD = self.dropout(comb_BCD)\n    for layer in self.layers:\n        src_BCD = layer(src_BCD, src_mask_B11C)\n    return cast(Tensor, src_BCD)\n</code></pre>"},{"location":"DirectMultiStep/components/encoder/#directmultistep.model.components.encoder.EncoderLayer","title":"<code>EncoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A single layer of the encoder.</p> Shape suffixes convention <p>B: batch size C: the length of the input on which conditioning is done    (in our case input_max_length) D: model dimension (sometimes called d_model or embedding_dim)</p> Source code in <code>src/directmultistep/model/components/encoder.py</code> <pre><code>class EncoderLayer(nn.Module):\n    \"\"\"A single layer of the encoder.\n\n    Shape suffixes convention:\n        B: batch size\n        C: the length of the input on which conditioning is done\n           (in our case input_max_length)\n        D: model dimension (sometimes called d_model or embedding_dim)\n    \"\"\"\n\n    def __init__(\n        self,\n        hid_dim: int,\n        n_heads: int,\n        ff_mult: int,\n        ff_activation: str,\n        dropout: float,\n        attn_bias: bool,\n    ):\n        \"\"\"Initializes the EncoderLayer.\n\n        Args:\n            hid_dim: The hidden dimension size.\n            n_heads: The number of attention heads.\n            ff_mult: The feed-forward expansion factor.\n            ff_activation: The activation function type.\n            dropout: The dropout rate.\n            attn_bias: Whether to use bias in the attention layers.\n        \"\"\"\n        super().__init__()\n\n        self.attn_ln = nn.LayerNorm(hid_dim)\n        self.ff_ln = nn.LayerNorm(hid_dim)\n        self.attention = MultiHeadAttentionLayer(\n            hid_dim=hid_dim,\n            n_heads=n_heads,\n            dropout=dropout,\n            attn_bias=attn_bias,\n        )\n        self.mlp = PositionwiseFeedforwardLayer(\n            hid_dim=hid_dim,\n            ff_mult=ff_mult,\n            ff_activation=activation_dict[ff_activation],\n            dropout=dropout,\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_BCD: Tensor, src_mask_B11C: Tensor) -&gt; Tensor:\n        \"\"\"Forward pass of the EncoderLayer.\n\n        Args:\n            input_BCD: The input tensor of shape (B, C, D).\n            src_mask_B11C: The source mask tensor of shape (B, 1, 1, C).\n\n        Returns:\n            The output tensor of shape (B, C, D).\n        \"\"\"\n        attn_output_BCD = self.attention(input_BCD, input_BCD, input_BCD, src_mask_B11C)\n        src_BCD = self.attn_ln(input_BCD + self.dropout(attn_output_BCD))\n        ff_out_BCD = self.mlp(src_BCD)\n        final_out_BLD = self.ff_ln(src_BCD + self.dropout(ff_out_BCD))\n        return cast(Tensor, final_out_BLD)\n</code></pre>"},{"location":"DirectMultiStep/components/encoder/#directmultistep.model.components.encoder.EncoderLayer.__init__","title":"<code>__init__(hid_dim, n_heads, ff_mult, ff_activation, dropout, attn_bias)</code>","text":"<p>Initializes the EncoderLayer.</p> <p>Parameters:</p> Name Type Description Default <code>hid_dim</code> <code>int</code> <p>The hidden dimension size.</p> required <code>n_heads</code> <code>int</code> <p>The number of attention heads.</p> required <code>ff_mult</code> <code>int</code> <p>The feed-forward expansion factor.</p> required <code>ff_activation</code> <code>str</code> <p>The activation function type.</p> required <code>dropout</code> <code>float</code> <p>The dropout rate.</p> required <code>attn_bias</code> <code>bool</code> <p>Whether to use bias in the attention layers.</p> required Source code in <code>src/directmultistep/model/components/encoder.py</code> <pre><code>def __init__(\n    self,\n    hid_dim: int,\n    n_heads: int,\n    ff_mult: int,\n    ff_activation: str,\n    dropout: float,\n    attn_bias: bool,\n):\n    \"\"\"Initializes the EncoderLayer.\n\n    Args:\n        hid_dim: The hidden dimension size.\n        n_heads: The number of attention heads.\n        ff_mult: The feed-forward expansion factor.\n        ff_activation: The activation function type.\n        dropout: The dropout rate.\n        attn_bias: Whether to use bias in the attention layers.\n    \"\"\"\n    super().__init__()\n\n    self.attn_ln = nn.LayerNorm(hid_dim)\n    self.ff_ln = nn.LayerNorm(hid_dim)\n    self.attention = MultiHeadAttentionLayer(\n        hid_dim=hid_dim,\n        n_heads=n_heads,\n        dropout=dropout,\n        attn_bias=attn_bias,\n    )\n    self.mlp = PositionwiseFeedforwardLayer(\n        hid_dim=hid_dim,\n        ff_mult=ff_mult,\n        ff_activation=activation_dict[ff_activation],\n        dropout=dropout,\n    )\n    self.dropout = nn.Dropout(dropout)\n</code></pre>"},{"location":"DirectMultiStep/components/encoder/#directmultistep.model.components.encoder.EncoderLayer.forward","title":"<code>forward(input_BCD, src_mask_B11C)</code>","text":"<p>Forward pass of the EncoderLayer.</p> <p>Parameters:</p> Name Type Description Default <code>input_BCD</code> <code>Tensor</code> <p>The input tensor of shape (B, C, D).</p> required <code>src_mask_B11C</code> <code>Tensor</code> <p>The source mask tensor of shape (B, 1, 1, C).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (B, C, D).</p> Source code in <code>src/directmultistep/model/components/encoder.py</code> <pre><code>def forward(self, input_BCD: Tensor, src_mask_B11C: Tensor) -&gt; Tensor:\n    \"\"\"Forward pass of the EncoderLayer.\n\n    Args:\n        input_BCD: The input tensor of shape (B, C, D).\n        src_mask_B11C: The source mask tensor of shape (B, 1, 1, C).\n\n    Returns:\n        The output tensor of shape (B, C, D).\n    \"\"\"\n    attn_output_BCD = self.attention(input_BCD, input_BCD, input_BCD, src_mask_B11C)\n    src_BCD = self.attn_ln(input_BCD + self.dropout(attn_output_BCD))\n    ff_out_BCD = self.mlp(src_BCD)\n    final_out_BLD = self.ff_ln(src_BCD + self.dropout(ff_out_BCD))\n    return cast(Tensor, final_out_BLD)\n</code></pre>"},{"location":"DirectMultiStep/components/encoder/#directmultistep.model.components.encoder.MoEEncoder","title":"<code>MoEEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>The MoE encoder module.</p> Shape suffixes convention <p>B: batch size C: the length of the input on which conditioning is done    (in our case input_max_length) D: model dimension (sometimes called d_model or embedding_dim)</p> Source code in <code>src/directmultistep/model/components/encoder.py</code> <pre><code>class MoEEncoder(nn.Module):\n    \"\"\"The MoE encoder module.\n\n    Shape suffixes convention:\n        B: batch size\n        C: the length of the input on which conditioning is done\n           (in our case input_max_length)\n        D: model dimension (sometimes called d_model or embedding_dim)\n    \"\"\"\n\n    def __init__(\n        self,\n        vocab_dim: int,\n        hid_dim: int,\n        n_layers: int,\n        n_heads: int,\n        n_experts: int,\n        top_k: int,\n        ff_mult: int,\n        ff_activation: str,\n        dropout: float,\n        attn_bias: bool,\n        context_window: int,\n        initiate_steps: bool,\n        include_steps: bool,\n        capacity_factor: float,\n    ):\n        \"\"\"Initializes the MoEEncoder.\n\n        Args:\n            vocab_dim: The vocabulary dimension size.\n            hid_dim: The hidden dimension size.\n            n_layers: The number of encoder layers.\n            n_heads: The number of attention heads.\n            n_experts: The number of experts in the MoE layer.\n            top_k: The number of experts to use in the MoE layer.\n            ff_mult: The feed-forward expansion factor.\n            ff_activation: The activation function type.\n            dropout: The dropout rate.\n            attn_bias: Whether to use bias in the attention layers.\n            context_window: The context window size.\n            initiate_steps: Whether to initiate step embeddings.\n            include_steps: Whether to include step embeddings.\n            capacity_factor: The capacity factor for the MoE layer.\n        \"\"\"\n        super().__init__()\n        self.tok_embedding = nn.Embedding(vocab_dim, hid_dim)\n        self.pos_embedding = nn.Embedding(context_window, hid_dim)\n        if initiate_steps:\n            self.step_embedding = nn.Embedding(1, hid_dim)\n        self.include_steps = include_steps\n\n        self.layers = nn.ModuleList(\n            [\n                MoEEncoderLayer(\n                    hid_dim=hid_dim,\n                    n_heads=n_heads,\n                    n_experts=n_experts,\n                    top_k=top_k,\n                    ff_mult=ff_mult,\n                    ff_activation=ff_activation,\n                    dropout=dropout,\n                    attn_bias=attn_bias,\n                    capacity_factor=capacity_factor,\n                )\n                for _ in range(n_layers)\n            ]\n        )\n        self.dropout = nn.Dropout(dropout)\n        self.scale = torch.sqrt(torch.FloatTensor([hid_dim]))\n\n    def forward(self, src_BC: Tensor, src_mask_B11C: Tensor, steps_B1: Tensor) -&gt; Tensor:\n        \"\"\"Forward pass of the MoEEncoder.\n\n        Args:\n            src_BC: The source input tensor of shape (B, C).\n            src_mask_B11C: The source mask tensor of shape (B, 1, 1, C).\n            steps_B1: The step tensor of shape (B, 1).\n\n        Returns:\n            The output tensor of shape (B, C, D).\n        \"\"\"\n        B, C = src_BC.shape\n        tok_emb_BCD = self.tok_embedding(src_BC) * self.scale.to(src_BC)\n        # below [C] -&gt; [1, C] -&gt; [B, C]\n        pos_BC = torch.arange(0, C).unsqueeze(0).repeat(B, 1).to(src_BC)\n        pos_emb_BCD = self.pos_embedding(pos_BC)\n        comb_BCD = tok_emb_BCD + pos_emb_BCD\n        if self.include_steps:\n            # [C] -&gt; [1, C] -&gt; [B, C]\n            step_BC = torch.zeros(C).unsqueeze(0).repeat(B, 1).long().to(src_BC)\n            step_emb_BCD = self.step_embedding(step_BC) * steps_B1.view(-1, 1, 1)\n            comb_BCD += step_emb_BCD\n        src_BCD = self.dropout(comb_BCD)\n        for layer in self.layers:\n            src_BCD = layer(src_BCD, src_mask_B11C)\n        return cast(Tensor, src_BCD)\n</code></pre>"},{"location":"DirectMultiStep/components/encoder/#directmultistep.model.components.encoder.MoEEncoder.__init__","title":"<code>__init__(vocab_dim, hid_dim, n_layers, n_heads, n_experts, top_k, ff_mult, ff_activation, dropout, attn_bias, context_window, initiate_steps, include_steps, capacity_factor)</code>","text":"<p>Initializes the MoEEncoder.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_dim</code> <code>int</code> <p>The vocabulary dimension size.</p> required <code>hid_dim</code> <code>int</code> <p>The hidden dimension size.</p> required <code>n_layers</code> <code>int</code> <p>The number of encoder layers.</p> required <code>n_heads</code> <code>int</code> <p>The number of attention heads.</p> required <code>n_experts</code> <code>int</code> <p>The number of experts in the MoE layer.</p> required <code>top_k</code> <code>int</code> <p>The number of experts to use in the MoE layer.</p> required <code>ff_mult</code> <code>int</code> <p>The feed-forward expansion factor.</p> required <code>ff_activation</code> <code>str</code> <p>The activation function type.</p> required <code>dropout</code> <code>float</code> <p>The dropout rate.</p> required <code>attn_bias</code> <code>bool</code> <p>Whether to use bias in the attention layers.</p> required <code>context_window</code> <code>int</code> <p>The context window size.</p> required <code>initiate_steps</code> <code>bool</code> <p>Whether to initiate step embeddings.</p> required <code>include_steps</code> <code>bool</code> <p>Whether to include step embeddings.</p> required <code>capacity_factor</code> <code>float</code> <p>The capacity factor for the MoE layer.</p> required Source code in <code>src/directmultistep/model/components/encoder.py</code> <pre><code>def __init__(\n    self,\n    vocab_dim: int,\n    hid_dim: int,\n    n_layers: int,\n    n_heads: int,\n    n_experts: int,\n    top_k: int,\n    ff_mult: int,\n    ff_activation: str,\n    dropout: float,\n    attn_bias: bool,\n    context_window: int,\n    initiate_steps: bool,\n    include_steps: bool,\n    capacity_factor: float,\n):\n    \"\"\"Initializes the MoEEncoder.\n\n    Args:\n        vocab_dim: The vocabulary dimension size.\n        hid_dim: The hidden dimension size.\n        n_layers: The number of encoder layers.\n        n_heads: The number of attention heads.\n        n_experts: The number of experts in the MoE layer.\n        top_k: The number of experts to use in the MoE layer.\n        ff_mult: The feed-forward expansion factor.\n        ff_activation: The activation function type.\n        dropout: The dropout rate.\n        attn_bias: Whether to use bias in the attention layers.\n        context_window: The context window size.\n        initiate_steps: Whether to initiate step embeddings.\n        include_steps: Whether to include step embeddings.\n        capacity_factor: The capacity factor for the MoE layer.\n    \"\"\"\n    super().__init__()\n    self.tok_embedding = nn.Embedding(vocab_dim, hid_dim)\n    self.pos_embedding = nn.Embedding(context_window, hid_dim)\n    if initiate_steps:\n        self.step_embedding = nn.Embedding(1, hid_dim)\n    self.include_steps = include_steps\n\n    self.layers = nn.ModuleList(\n        [\n            MoEEncoderLayer(\n                hid_dim=hid_dim,\n                n_heads=n_heads,\n                n_experts=n_experts,\n                top_k=top_k,\n                ff_mult=ff_mult,\n                ff_activation=ff_activation,\n                dropout=dropout,\n                attn_bias=attn_bias,\n                capacity_factor=capacity_factor,\n            )\n            for _ in range(n_layers)\n        ]\n    )\n    self.dropout = nn.Dropout(dropout)\n    self.scale = torch.sqrt(torch.FloatTensor([hid_dim]))\n</code></pre>"},{"location":"DirectMultiStep/components/encoder/#directmultistep.model.components.encoder.MoEEncoder.forward","title":"<code>forward(src_BC, src_mask_B11C, steps_B1)</code>","text":"<p>Forward pass of the MoEEncoder.</p> <p>Parameters:</p> Name Type Description Default <code>src_BC</code> <code>Tensor</code> <p>The source input tensor of shape (B, C).</p> required <code>src_mask_B11C</code> <code>Tensor</code> <p>The source mask tensor of shape (B, 1, 1, C).</p> required <code>steps_B1</code> <code>Tensor</code> <p>The step tensor of shape (B, 1).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (B, C, D).</p> Source code in <code>src/directmultistep/model/components/encoder.py</code> <pre><code>def forward(self, src_BC: Tensor, src_mask_B11C: Tensor, steps_B1: Tensor) -&gt; Tensor:\n    \"\"\"Forward pass of the MoEEncoder.\n\n    Args:\n        src_BC: The source input tensor of shape (B, C).\n        src_mask_B11C: The source mask tensor of shape (B, 1, 1, C).\n        steps_B1: The step tensor of shape (B, 1).\n\n    Returns:\n        The output tensor of shape (B, C, D).\n    \"\"\"\n    B, C = src_BC.shape\n    tok_emb_BCD = self.tok_embedding(src_BC) * self.scale.to(src_BC)\n    # below [C] -&gt; [1, C] -&gt; [B, C]\n    pos_BC = torch.arange(0, C).unsqueeze(0).repeat(B, 1).to(src_BC)\n    pos_emb_BCD = self.pos_embedding(pos_BC)\n    comb_BCD = tok_emb_BCD + pos_emb_BCD\n    if self.include_steps:\n        # [C] -&gt; [1, C] -&gt; [B, C]\n        step_BC = torch.zeros(C).unsqueeze(0).repeat(B, 1).long().to(src_BC)\n        step_emb_BCD = self.step_embedding(step_BC) * steps_B1.view(-1, 1, 1)\n        comb_BCD += step_emb_BCD\n    src_BCD = self.dropout(comb_BCD)\n    for layer in self.layers:\n        src_BCD = layer(src_BCD, src_mask_B11C)\n    return cast(Tensor, src_BCD)\n</code></pre>"},{"location":"DirectMultiStep/components/encoder/#directmultistep.model.components.encoder.MoEEncoderLayer","title":"<code>MoEEncoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A single layer of the MoE encoder.</p> Shape suffixes convention <p>B: batch size C: the length of the input on which conditioning is done    (in our case input_max_length) D: model dimension (sometimes called d_model or embedding_dim)</p> Source code in <code>src/directmultistep/model/components/encoder.py</code> <pre><code>class MoEEncoderLayer(nn.Module):\n    \"\"\"A single layer of the MoE encoder.\n\n    Shape suffixes convention:\n        B: batch size\n        C: the length of the input on which conditioning is done\n           (in our case input_max_length)\n        D: model dimension (sometimes called d_model or embedding_dim)\n    \"\"\"\n\n    def __init__(\n        self,\n        hid_dim: int,\n        n_heads: int,\n        n_experts: int,\n        top_k: int,\n        ff_mult: int,\n        ff_activation: str,\n        dropout: float,\n        attn_bias: bool,\n        capacity_factor: float,\n    ):\n        \"\"\"Initializes the MoEEncoderLayer.\n\n        Args:\n            hid_dim: The hidden dimension size.\n            n_heads: The number of attention heads.\n            n_experts: The number of experts in the MoE layer.\n            top_k: The number of experts to use in the MoE layer.\n            ff_mult: The feed-forward expansion factor.\n            ff_activation: The activation function type.\n            dropout: The dropout rate.\n            attn_bias: Whether to use bias in the attention layers.\n            capacity_factor: The capacity factor for the MoE layer.\n        \"\"\"\n        super().__init__()\n\n        self.attn_ln = nn.LayerNorm(hid_dim)\n        self.ff_ln = nn.LayerNorm(hid_dim)\n        self.attention = MultiHeadAttentionLayer(\n            hid_dim=hid_dim,\n            n_heads=n_heads,\n            dropout=dropout,\n            attn_bias=attn_bias,\n        )\n        self.mlp = SparseMoE(\n            hid_dim=hid_dim,\n            n_experts=n_experts,\n            top_k=top_k,\n            ff_mult=ff_mult,\n            ff_activation=ff_activation,\n            dropout=dropout,\n            capacity_factor=capacity_factor,\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_BCD: Tensor, src_mask_B11C: Tensor) -&gt; Tensor:\n        \"\"\"Forward pass of the MoEEncoderLayer.\n\n        Args:\n            input_BCD: The input tensor of shape (B, C, D).\n            src_mask_B11C: The source mask tensor of shape (B, 1, 1, C).\n\n        Returns:\n            The output tensor of shape (B, C, D).\n        \"\"\"\n        attn_output_BCD = self.attention(input_BCD, input_BCD, input_BCD, src_mask_B11C)\n        src_BCD = self.attn_ln(input_BCD + self.dropout(attn_output_BCD))\n        ff_out_BCD = self.mlp(src_BCD)\n        final_out_BLD = self.ff_ln(src_BCD + self.dropout(ff_out_BCD))\n        return cast(Tensor, final_out_BLD)\n</code></pre>"},{"location":"DirectMultiStep/components/encoder/#directmultistep.model.components.encoder.MoEEncoderLayer.__init__","title":"<code>__init__(hid_dim, n_heads, n_experts, top_k, ff_mult, ff_activation, dropout, attn_bias, capacity_factor)</code>","text":"<p>Initializes the MoEEncoderLayer.</p> <p>Parameters:</p> Name Type Description Default <code>hid_dim</code> <code>int</code> <p>The hidden dimension size.</p> required <code>n_heads</code> <code>int</code> <p>The number of attention heads.</p> required <code>n_experts</code> <code>int</code> <p>The number of experts in the MoE layer.</p> required <code>top_k</code> <code>int</code> <p>The number of experts to use in the MoE layer.</p> required <code>ff_mult</code> <code>int</code> <p>The feed-forward expansion factor.</p> required <code>ff_activation</code> <code>str</code> <p>The activation function type.</p> required <code>dropout</code> <code>float</code> <p>The dropout rate.</p> required <code>attn_bias</code> <code>bool</code> <p>Whether to use bias in the attention layers.</p> required <code>capacity_factor</code> <code>float</code> <p>The capacity factor for the MoE layer.</p> required Source code in <code>src/directmultistep/model/components/encoder.py</code> <pre><code>def __init__(\n    self,\n    hid_dim: int,\n    n_heads: int,\n    n_experts: int,\n    top_k: int,\n    ff_mult: int,\n    ff_activation: str,\n    dropout: float,\n    attn_bias: bool,\n    capacity_factor: float,\n):\n    \"\"\"Initializes the MoEEncoderLayer.\n\n    Args:\n        hid_dim: The hidden dimension size.\n        n_heads: The number of attention heads.\n        n_experts: The number of experts in the MoE layer.\n        top_k: The number of experts to use in the MoE layer.\n        ff_mult: The feed-forward expansion factor.\n        ff_activation: The activation function type.\n        dropout: The dropout rate.\n        attn_bias: Whether to use bias in the attention layers.\n        capacity_factor: The capacity factor for the MoE layer.\n    \"\"\"\n    super().__init__()\n\n    self.attn_ln = nn.LayerNorm(hid_dim)\n    self.ff_ln = nn.LayerNorm(hid_dim)\n    self.attention = MultiHeadAttentionLayer(\n        hid_dim=hid_dim,\n        n_heads=n_heads,\n        dropout=dropout,\n        attn_bias=attn_bias,\n    )\n    self.mlp = SparseMoE(\n        hid_dim=hid_dim,\n        n_experts=n_experts,\n        top_k=top_k,\n        ff_mult=ff_mult,\n        ff_activation=ff_activation,\n        dropout=dropout,\n        capacity_factor=capacity_factor,\n    )\n    self.dropout = nn.Dropout(dropout)\n</code></pre>"},{"location":"DirectMultiStep/components/encoder/#directmultistep.model.components.encoder.MoEEncoderLayer.forward","title":"<code>forward(input_BCD, src_mask_B11C)</code>","text":"<p>Forward pass of the MoEEncoderLayer.</p> <p>Parameters:</p> Name Type Description Default <code>input_BCD</code> <code>Tensor</code> <p>The input tensor of shape (B, C, D).</p> required <code>src_mask_B11C</code> <code>Tensor</code> <p>The source mask tensor of shape (B, 1, 1, C).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (B, C, D).</p> Source code in <code>src/directmultistep/model/components/encoder.py</code> <pre><code>def forward(self, input_BCD: Tensor, src_mask_B11C: Tensor) -&gt; Tensor:\n    \"\"\"Forward pass of the MoEEncoderLayer.\n\n    Args:\n        input_BCD: The input tensor of shape (B, C, D).\n        src_mask_B11C: The source mask tensor of shape (B, 1, 1, C).\n\n    Returns:\n        The output tensor of shape (B, C, D).\n    \"\"\"\n    attn_output_BCD = self.attention(input_BCD, input_BCD, input_BCD, src_mask_B11C)\n    src_BCD = self.attn_ln(input_BCD + self.dropout(attn_output_BCD))\n    ff_out_BCD = self.mlp(src_BCD)\n    final_out_BLD = self.ff_ln(src_BCD + self.dropout(ff_out_BCD))\n    return cast(Tensor, final_out_BLD)\n</code></pre>"},{"location":"DirectMultiStep/components/moe/","title":"Mixture of Experts","text":"<p>This document describes the Mixture of Experts (MoE) components used in the DMS model. MoE is a technique that improves model capacity and efficiency by routing different inputs to specialized sub-networks (experts).</p>"},{"location":"DirectMultiStep/components/moe/#position-wise-feed-forward-layer","title":"Position-wise Feed-forward Layer","text":"<p>The standard feed-forward network serves as our baseline for comparison with MoE layers. It processes each position in the sequence independently through a simple two-layer network with expansion and projection. This is the traditional architecture used in transformer models.</p>"},{"location":"DirectMultiStep/components/moe/#noisy-top-k-router","title":"Noisy Top-k Router","text":"<p>The router is the brain of the MoE system - it decides which experts should process each token. Key features:</p> <ul> <li>Uses learned routing weights to match tokens with relevant experts</li> <li>Adds learned noise to encourage exploration and prevent expert collapse</li> <li>Selects top-k experts per token to enable specialization while maintaining redundancy</li> <li>Produces sparse routing probabilities to enable efficient computation</li> </ul> <p>The noise mechanism is particularly important as it:</p> <ol> <li>Prevents tokens from always taking the same path</li> <li>Helps balance load across experts</li> <li>Improves training stability</li> </ol>"},{"location":"DirectMultiStep/components/moe/#expert-network","title":"Expert Network","text":"<p>Each expert is a specialized feed-forward network that becomes tuned to handle specific types of tokens or patterns. The expert architecture mirrors the standard feed-forward layer, but each expert can learn different specializations. For example:</p> <ul> <li>Some experts might focus on syntax</li> <li>Others on specific vocabulary domains</li> <li>Others on particular transformation patterns</li> </ul>"},{"location":"DirectMultiStep/components/moe/#sparse-moe-layer","title":"Sparse MoE Layer","text":"<p>This is where everything comes together into an efficient, scalable system:</p> <ol> <li>Token Routing: The router examines each token and decides which experts should process it</li> <li>Load Balancing:<ul> <li>Uses capacity factors to prevent expert overload</li> <li>Ensures even utilization of experts</li> <li>Handles cases where too many tokens want the same expert</li> </ul> </li> <li>Parallel Processing:<ul> <li>Tokens are grouped by assigned expert</li> <li>Each expert processes its assigned group</li> <li>Results are combined based on routing weights</li> </ul> </li> </ol> <p>The sparse computation pattern makes MoE layers much more efficient than simply running multiple full-size feed-forward layers.</p>"},{"location":"DirectMultiStep/components/moe/#intuition-behind-moe","title":"Intuition Behind MoE","text":"<p>Think of MoE like a team of specialists:</p> <ul> <li>Instead of every token going through the same general-purpose network</li> <li>Tokens are routed to experts that are best suited to process them</li> <li>Each expert becomes specialized in handling certain types of patterns</li> <li>The router learns to match tokens with the right experts</li> </ul> <p>This specialization allows the model to:</p> <ul> <li>Handle a wider range of patterns effectively</li> <li>Scale capacity without scaling computation for every token</li> <li>Develop focused expertise in different aspects of the task</li> </ul>"},{"location":"DirectMultiStep/components/moe/#source-code","title":"Source Code","text":""},{"location":"DirectMultiStep/components/moe/#directmultistep.model.components.moe","title":"<code>directmultistep.model.components.moe</code>","text":""},{"location":"DirectMultiStep/components/moe/#directmultistep.model.components.moe.Expert","title":"<code>Expert</code>","text":"<p>               Bases: <code>Module</code></p> <p>A single expert in the MoE layer.</p> <p>Applies a two-layer feedforward network to the input.</p> Shape suffixes <p>B: batch size L: sequence length D: model dimension F: feed-forward subnetwork hidden size</p> Source code in <code>src/directmultistep/model/components/moe.py</code> <pre><code>class Expert(nn.Module):\n    \"\"\"A single expert in the MoE layer.\n\n    Applies a two-layer feedforward network to the input.\n\n    Shape suffixes:\n        B: batch size\n        L: sequence length\n        D: model dimension\n        F: feed-forward subnetwork hidden size\n    \"\"\"\n\n    def __init__(\n        self,\n        hid_dim: int,\n        ff_mult: int,\n        ff_activation: str,\n        dropout: float,\n    ):\n        \"\"\"Initializes the Expert.\n\n        Args:\n            hid_dim: The hidden dimension size (D).\n            ff_mult: The feed-forward expansion factor.\n            ff_activation: The activation function type.\n            dropout: The dropout rate.\n        \"\"\"\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(hid_dim, ff_mult * hid_dim),\n            activation_dict[ff_activation],\n            nn.Linear(ff_mult * hid_dim, hid_dim),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x_BLD: Tensor) -&gt; Tensor:\n        \"\"\"Forward pass of the Expert.\n\n        Args:\n            x_BLD: The input tensor of shape (B, L, D).\n\n        Returns:\n            The output tensor of shape (B, L, D).\n        \"\"\"\n        return self.net(x_BLD)  # type: ignore\n</code></pre>"},{"location":"DirectMultiStep/components/moe/#directmultistep.model.components.moe.Expert.__init__","title":"<code>__init__(hid_dim, ff_mult, ff_activation, dropout)</code>","text":"<p>Initializes the Expert.</p> <p>Parameters:</p> Name Type Description Default <code>hid_dim</code> <code>int</code> <p>The hidden dimension size (D).</p> required <code>ff_mult</code> <code>int</code> <p>The feed-forward expansion factor.</p> required <code>ff_activation</code> <code>str</code> <p>The activation function type.</p> required <code>dropout</code> <code>float</code> <p>The dropout rate.</p> required Source code in <code>src/directmultistep/model/components/moe.py</code> <pre><code>def __init__(\n    self,\n    hid_dim: int,\n    ff_mult: int,\n    ff_activation: str,\n    dropout: float,\n):\n    \"\"\"Initializes the Expert.\n\n    Args:\n        hid_dim: The hidden dimension size (D).\n        ff_mult: The feed-forward expansion factor.\n        ff_activation: The activation function type.\n        dropout: The dropout rate.\n    \"\"\"\n    super().__init__()\n    self.net = nn.Sequential(\n        nn.Linear(hid_dim, ff_mult * hid_dim),\n        activation_dict[ff_activation],\n        nn.Linear(ff_mult * hid_dim, hid_dim),\n        nn.Dropout(dropout),\n    )\n</code></pre>"},{"location":"DirectMultiStep/components/moe/#directmultistep.model.components.moe.Expert.forward","title":"<code>forward(x_BLD)</code>","text":"<p>Forward pass of the Expert.</p> <p>Parameters:</p> Name Type Description Default <code>x_BLD</code> <code>Tensor</code> <p>The input tensor of shape (B, L, D).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (B, L, D).</p> Source code in <code>src/directmultistep/model/components/moe.py</code> <pre><code>def forward(self, x_BLD: Tensor) -&gt; Tensor:\n    \"\"\"Forward pass of the Expert.\n\n    Args:\n        x_BLD: The input tensor of shape (B, L, D).\n\n    Returns:\n        The output tensor of shape (B, L, D).\n    \"\"\"\n    return self.net(x_BLD)  # type: ignore\n</code></pre>"},{"location":"DirectMultiStep/components/moe/#directmultistep.model.components.moe.NoisyTopkRouter","title":"<code>NoisyTopkRouter</code>","text":"<p>               Bases: <code>Module</code></p> <p>Noisy top-k router for MoE.</p> <p>Routes inputs to the top-k experts based on noisy logits.</p> Shape suffixes <p>B: batch size L: sequence length D: model dimension E: number of experts K: top_k</p> Source code in <code>src/directmultistep/model/components/moe.py</code> <pre><code>class NoisyTopkRouter(nn.Module):\n    \"\"\"Noisy top-k router for MoE.\n\n    Routes inputs to the top-k experts based on noisy logits.\n\n    Shape suffixes:\n        B: batch size\n        L: sequence length\n        D: model dimension\n        E: number of experts\n        K: top_k\n    \"\"\"\n\n    def __init__(self, hid_dim: int, n_experts: int, top_k: int):\n        \"\"\"Initializes the NoisyTopkRouter.\n\n        Args:\n            hid_dim: The hidden dimension size (D).\n            n_experts: The number of experts (E).\n            top_k: The number of top experts to route to (K).\n        \"\"\"\n        super().__init__()\n        self.top_k = top_k\n        self.topkroute_linear = nn.Linear(hid_dim, n_experts)\n        self.noise_linear = nn.Linear(hid_dim, n_experts)\n\n    def forward(self, x_BLD: Tensor) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Forward pass of the NoisyTopkRouter.\n\n        Args:\n            x_BLD: The input tensor of shape (B, L, D).\n\n        Returns:\n            A tuple containing:\n                - The router output tensor of shape (B, L, E).\n                - The indices of the top-k experts of shape (B, L, K).\n        \"\"\"\n        logits_BLE = self.topkroute_linear(x_BLD)\n        noise_logits_BLE = self.noise_linear(x_BLD)\n        # Adding scaled unit gaussian noise to the logits\n        noise_BLE = torch.randn_like(logits_BLE) * F.softplus(noise_logits_BLE)\n        noisy_logits_BLE = logits_BLE + noise_BLE\n\n        top_k_logits_BLE, indices_BLK = noisy_logits_BLE.topk(self.top_k, dim=-1)\n        zeros_BLE = torch.full_like(noisy_logits_BLE, float(\"-inf\"))\n        # creating a sparse tensor with top-k logits\n        sparse_logits_BLE = zeros_BLE.scatter(-1, indices_BLK, top_k_logits_BLE)\n        router_output_BLE = F.softmax(sparse_logits_BLE, dim=-1)\n        return router_output_BLE, indices_BLK\n</code></pre>"},{"location":"DirectMultiStep/components/moe/#directmultistep.model.components.moe.NoisyTopkRouter.__init__","title":"<code>__init__(hid_dim, n_experts, top_k)</code>","text":"<p>Initializes the NoisyTopkRouter.</p> <p>Parameters:</p> Name Type Description Default <code>hid_dim</code> <code>int</code> <p>The hidden dimension size (D).</p> required <code>n_experts</code> <code>int</code> <p>The number of experts (E).</p> required <code>top_k</code> <code>int</code> <p>The number of top experts to route to (K).</p> required Source code in <code>src/directmultistep/model/components/moe.py</code> <pre><code>def __init__(self, hid_dim: int, n_experts: int, top_k: int):\n    \"\"\"Initializes the NoisyTopkRouter.\n\n    Args:\n        hid_dim: The hidden dimension size (D).\n        n_experts: The number of experts (E).\n        top_k: The number of top experts to route to (K).\n    \"\"\"\n    super().__init__()\n    self.top_k = top_k\n    self.topkroute_linear = nn.Linear(hid_dim, n_experts)\n    self.noise_linear = nn.Linear(hid_dim, n_experts)\n</code></pre>"},{"location":"DirectMultiStep/components/moe/#directmultistep.model.components.moe.NoisyTopkRouter.forward","title":"<code>forward(x_BLD)</code>","text":"<p>Forward pass of the NoisyTopkRouter.</p> <p>Parameters:</p> Name Type Description Default <code>x_BLD</code> <code>Tensor</code> <p>The input tensor of shape (B, L, D).</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>A tuple containing: - The router output tensor of shape (B, L, E). - The indices of the top-k experts of shape (B, L, K).</p> Source code in <code>src/directmultistep/model/components/moe.py</code> <pre><code>def forward(self, x_BLD: Tensor) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Forward pass of the NoisyTopkRouter.\n\n    Args:\n        x_BLD: The input tensor of shape (B, L, D).\n\n    Returns:\n        A tuple containing:\n            - The router output tensor of shape (B, L, E).\n            - The indices of the top-k experts of shape (B, L, K).\n    \"\"\"\n    logits_BLE = self.topkroute_linear(x_BLD)\n    noise_logits_BLE = self.noise_linear(x_BLD)\n    # Adding scaled unit gaussian noise to the logits\n    noise_BLE = torch.randn_like(logits_BLE) * F.softplus(noise_logits_BLE)\n    noisy_logits_BLE = logits_BLE + noise_BLE\n\n    top_k_logits_BLE, indices_BLK = noisy_logits_BLE.topk(self.top_k, dim=-1)\n    zeros_BLE = torch.full_like(noisy_logits_BLE, float(\"-inf\"))\n    # creating a sparse tensor with top-k logits\n    sparse_logits_BLE = zeros_BLE.scatter(-1, indices_BLK, top_k_logits_BLE)\n    router_output_BLE = F.softmax(sparse_logits_BLE, dim=-1)\n    return router_output_BLE, indices_BLK\n</code></pre>"},{"location":"DirectMultiStep/components/moe/#directmultistep.model.components.moe.PositionwiseFeedforwardLayer","title":"<code>PositionwiseFeedforwardLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Positionwise feedforward layer.</p> <p>Applies a two-layer feedforward network to the input.</p> Shape suffixes <p>B: batch size L: sequence length D: model dimension F: feed-forward subnetwork hidden size</p> Source code in <code>src/directmultistep/model/components/moe.py</code> <pre><code>class PositionwiseFeedforwardLayer(nn.Module):\n    \"\"\"Positionwise feedforward layer.\n\n    Applies a two-layer feedforward network to the input.\n\n    Shape suffixes:\n        B: batch size\n        L: sequence length\n        D: model dimension\n        F: feed-forward subnetwork hidden size\n    \"\"\"\n\n    def __init__(\n        self,\n        hid_dim: int,\n        ff_mult: int,\n        ff_activation: nn.Module,\n        dropout: float,\n    ):\n        \"\"\"Initializes the PositionwiseFeedforwardLayer.\n\n        Args:\n            hid_dim: The hidden dimension size (D).\n            ff_mult: The feed-forward expansion factor.\n            ff_activation: The activation function.\n            dropout: The dropout rate.\n        \"\"\"\n        super().__init__()\n\n        self.fc_1 = nn.Linear(hid_dim, ff_mult * hid_dim)\n        self.activ = ff_activation\n        self.fc_2 = nn.Linear(hid_dim * ff_mult, hid_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x_BLD: Tensor) -&gt; Tensor:\n        \"\"\"Forward pass of the PositionwiseFeedforwardLayer.\n\n        Args:\n            x_BLD: The input tensor of shape (B, L, D).\n\n        Returns:\n            The output tensor of shape (B, L, D).\n        \"\"\"\n        x_BLF = self.dropout(self.activ(self.fc_1(x_BLD)))\n        x_BLD = self.fc_2(x_BLF)\n        return x_BLD\n</code></pre>"},{"location":"DirectMultiStep/components/moe/#directmultistep.model.components.moe.PositionwiseFeedforwardLayer.__init__","title":"<code>__init__(hid_dim, ff_mult, ff_activation, dropout)</code>","text":"<p>Initializes the PositionwiseFeedforwardLayer.</p> <p>Parameters:</p> Name Type Description Default <code>hid_dim</code> <code>int</code> <p>The hidden dimension size (D).</p> required <code>ff_mult</code> <code>int</code> <p>The feed-forward expansion factor.</p> required <code>ff_activation</code> <code>Module</code> <p>The activation function.</p> required <code>dropout</code> <code>float</code> <p>The dropout rate.</p> required Source code in <code>src/directmultistep/model/components/moe.py</code> <pre><code>def __init__(\n    self,\n    hid_dim: int,\n    ff_mult: int,\n    ff_activation: nn.Module,\n    dropout: float,\n):\n    \"\"\"Initializes the PositionwiseFeedforwardLayer.\n\n    Args:\n        hid_dim: The hidden dimension size (D).\n        ff_mult: The feed-forward expansion factor.\n        ff_activation: The activation function.\n        dropout: The dropout rate.\n    \"\"\"\n    super().__init__()\n\n    self.fc_1 = nn.Linear(hid_dim, ff_mult * hid_dim)\n    self.activ = ff_activation\n    self.fc_2 = nn.Linear(hid_dim * ff_mult, hid_dim)\n\n    self.dropout = nn.Dropout(dropout)\n</code></pre>"},{"location":"DirectMultiStep/components/moe/#directmultistep.model.components.moe.PositionwiseFeedforwardLayer.forward","title":"<code>forward(x_BLD)</code>","text":"<p>Forward pass of the PositionwiseFeedforwardLayer.</p> <p>Parameters:</p> Name Type Description Default <code>x_BLD</code> <code>Tensor</code> <p>The input tensor of shape (B, L, D).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (B, L, D).</p> Source code in <code>src/directmultistep/model/components/moe.py</code> <pre><code>def forward(self, x_BLD: Tensor) -&gt; Tensor:\n    \"\"\"Forward pass of the PositionwiseFeedforwardLayer.\n\n    Args:\n        x_BLD: The input tensor of shape (B, L, D).\n\n    Returns:\n        The output tensor of shape (B, L, D).\n    \"\"\"\n    x_BLF = self.dropout(self.activ(self.fc_1(x_BLD)))\n    x_BLD = self.fc_2(x_BLF)\n    return x_BLD\n</code></pre>"},{"location":"DirectMultiStep/components/moe/#directmultistep.model.components.moe.SparseMoE","title":"<code>SparseMoE</code>","text":"<p>               Bases: <code>Module</code></p> <p>Sparse Mixture of Experts layer.</p> <p>Routes inputs to a subset of experts and combines their outputs.</p> Shape suffixes <p>B: batch size L: sequence length D: model dimension E: number of experts K: top_k S: number of selected tokens for an expert</p> Source code in <code>src/directmultistep/model/components/moe.py</code> <pre><code>class SparseMoE(nn.Module):\n    \"\"\"Sparse Mixture of Experts layer.\n\n    Routes inputs to a subset of experts and combines their outputs.\n\n    Shape suffixes:\n        B: batch size\n        L: sequence length\n        D: model dimension\n        E: number of experts\n        K: top_k\n        S: number of selected tokens for an expert\n    \"\"\"\n\n    def __init__(\n        self,\n        hid_dim: int,\n        n_experts: int,\n        top_k: int,\n        ff_mult: int,\n        ff_activation: str,\n        dropout: float,\n        capacity_factor: float,\n    ):\n        \"\"\"Initializes the SparseMoE layer.\n\n        Args:\n            hid_dim: The hidden dimension size (D).\n            n_experts: The number of experts (E).\n            top_k: The number of top experts to route to (K).\n            ff_mult: The feed-forward expansion factor.\n            ff_activation: The activation function type.\n            dropout: The dropout rate.\n            capacity_factor: The capacity factor for each expert.\n        \"\"\"\n        super(SparseMoE, self).__init__()\n        self.router = NoisyTopkRouter(hid_dim, n_experts, top_k)\n        self.experts = nn.ModuleList([Expert(hid_dim, ff_mult, ff_activation, dropout) for _ in range(n_experts)])\n        self.n_experts = n_experts\n        self.top_k = top_k\n        self.capacity_factor = capacity_factor\n\n    def forward(self, x_BLD: Tensor) -&gt; Tensor:\n        \"\"\"Forward pass of the SparseMoE layer.\n\n        Args:\n            x_BLD: The input tensor of shape (B, L, D).\n\n        Returns:\n            The output tensor of shape (B, L, D).\n        \"\"\"\n        B, L, _ = x_BLD.shape\n        gating_output_BLE, indices_BLK = self.router(x_BLD)\n        final_output_BLD = torch.zeros_like(x_BLD)\n\n        flat_x_FD = x_BLD.view(-1, x_BLD.size(-1))  # [B*L, D], define B*L=F\n        flat_gating_output_FE = gating_output_BLE.view(-1, gating_output_BLE.size(-1))\n        n_tkns = B * L * self.top_k\n        capacity = int((n_tkns / self.n_experts) * self.capacity_factor)\n\n        updates_FD = torch.zeros_like(flat_x_FD)\n        for i, expert in enumerate(self.experts):\n            # Create a mask for the inputs where the current expert is in top-k\n            expert_mask_BL = (indices_BLK == i).any(dim=-1)\n            flat_mask_F = expert_mask_BL.view(-1)\n            selected_idxs_F = torch.nonzero(flat_mask_F).squeeze(-1)\n\n            if selected_idxs_F.numel() &gt; capacity:\n                limited_idxs_F = selected_idxs_F[:capacity]\n            else:\n                limited_idxs_F = selected_idxs_F\n\n            if limited_idxs_F.numel() &gt; 0:\n                expert_input_SD = flat_x_FD[limited_idxs_F]  # S = sum(flat_mask_F)\n                expert_output_SD = expert(expert_input_SD)\n\n                # Extract and apply gating scores, [S] -&gt; [S, 1]\n                gating_scores_S1 = flat_gating_output_FE[limited_idxs_F, i].unsqueeze(1)\n                weighted_output_SD = expert_output_SD * gating_scores_S1\n\n                updates_FD.index_add_(0, limited_idxs_F, weighted_output_SD)\n\n        final_output_BLD += updates_FD.view(B, L, -1)\n\n        return final_output_BLD\n</code></pre>"},{"location":"DirectMultiStep/components/moe/#directmultistep.model.components.moe.SparseMoE.__init__","title":"<code>__init__(hid_dim, n_experts, top_k, ff_mult, ff_activation, dropout, capacity_factor)</code>","text":"<p>Initializes the SparseMoE layer.</p> <p>Parameters:</p> Name Type Description Default <code>hid_dim</code> <code>int</code> <p>The hidden dimension size (D).</p> required <code>n_experts</code> <code>int</code> <p>The number of experts (E).</p> required <code>top_k</code> <code>int</code> <p>The number of top experts to route to (K).</p> required <code>ff_mult</code> <code>int</code> <p>The feed-forward expansion factor.</p> required <code>ff_activation</code> <code>str</code> <p>The activation function type.</p> required <code>dropout</code> <code>float</code> <p>The dropout rate.</p> required <code>capacity_factor</code> <code>float</code> <p>The capacity factor for each expert.</p> required Source code in <code>src/directmultistep/model/components/moe.py</code> <pre><code>def __init__(\n    self,\n    hid_dim: int,\n    n_experts: int,\n    top_k: int,\n    ff_mult: int,\n    ff_activation: str,\n    dropout: float,\n    capacity_factor: float,\n):\n    \"\"\"Initializes the SparseMoE layer.\n\n    Args:\n        hid_dim: The hidden dimension size (D).\n        n_experts: The number of experts (E).\n        top_k: The number of top experts to route to (K).\n        ff_mult: The feed-forward expansion factor.\n        ff_activation: The activation function type.\n        dropout: The dropout rate.\n        capacity_factor: The capacity factor for each expert.\n    \"\"\"\n    super(SparseMoE, self).__init__()\n    self.router = NoisyTopkRouter(hid_dim, n_experts, top_k)\n    self.experts = nn.ModuleList([Expert(hid_dim, ff_mult, ff_activation, dropout) for _ in range(n_experts)])\n    self.n_experts = n_experts\n    self.top_k = top_k\n    self.capacity_factor = capacity_factor\n</code></pre>"},{"location":"DirectMultiStep/components/moe/#directmultistep.model.components.moe.SparseMoE.forward","title":"<code>forward(x_BLD)</code>","text":"<p>Forward pass of the SparseMoE layer.</p> <p>Parameters:</p> Name Type Description Default <code>x_BLD</code> <code>Tensor</code> <p>The input tensor of shape (B, L, D).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (B, L, D).</p> Source code in <code>src/directmultistep/model/components/moe.py</code> <pre><code>def forward(self, x_BLD: Tensor) -&gt; Tensor:\n    \"\"\"Forward pass of the SparseMoE layer.\n\n    Args:\n        x_BLD: The input tensor of shape (B, L, D).\n\n    Returns:\n        The output tensor of shape (B, L, D).\n    \"\"\"\n    B, L, _ = x_BLD.shape\n    gating_output_BLE, indices_BLK = self.router(x_BLD)\n    final_output_BLD = torch.zeros_like(x_BLD)\n\n    flat_x_FD = x_BLD.view(-1, x_BLD.size(-1))  # [B*L, D], define B*L=F\n    flat_gating_output_FE = gating_output_BLE.view(-1, gating_output_BLE.size(-1))\n    n_tkns = B * L * self.top_k\n    capacity = int((n_tkns / self.n_experts) * self.capacity_factor)\n\n    updates_FD = torch.zeros_like(flat_x_FD)\n    for i, expert in enumerate(self.experts):\n        # Create a mask for the inputs where the current expert is in top-k\n        expert_mask_BL = (indices_BLK == i).any(dim=-1)\n        flat_mask_F = expert_mask_BL.view(-1)\n        selected_idxs_F = torch.nonzero(flat_mask_F).squeeze(-1)\n\n        if selected_idxs_F.numel() &gt; capacity:\n            limited_idxs_F = selected_idxs_F[:capacity]\n        else:\n            limited_idxs_F = selected_idxs_F\n\n        if limited_idxs_F.numel() &gt; 0:\n            expert_input_SD = flat_x_FD[limited_idxs_F]  # S = sum(flat_mask_F)\n            expert_output_SD = expert(expert_input_SD)\n\n            # Extract and apply gating scores, [S] -&gt; [S, 1]\n            gating_scores_S1 = flat_gating_output_FE[limited_idxs_F, i].unsqueeze(1)\n            weighted_output_SD = expert_output_SD * gating_scores_S1\n\n            updates_FD.index_add_(0, limited_idxs_F, weighted_output_SD)\n\n    final_output_BLD += updates_FD.view(B, L, -1)\n\n    return final_output_BLD\n</code></pre>"},{"location":"DirectMultiStep/utils/io/","title":"Input/Output Utilities","text":"<p>This module provides functions for loading and saving datasets, as well as converting between different data formats. It is useful for preparing data for training and testing DirectMultiStep models.</p>"},{"location":"DirectMultiStep/utils/io/#example-use","title":"Example Use","text":"<p>The most useful functions are <code>load_dataset_sm</code>, <code>load_dataset_nosm</code>, <code>save_dataset_sm</code>, and <code>load_pharma_compounds</code>. These functions allow you to load and save datasets in a variety of formats.</p> <pre><code>from pathlib import Path\nfrom directmultistep.utils.io import load_pharma_compounds\n\ndata_path = Path.cwd() / \"data\"\n\n_products, _sms, _path_strings, _steps_list, nameToIdx = load_pharma_compounds(data_path / \"pharma_compounds.json\")\n</code></pre>"},{"location":"DirectMultiStep/utils/io/#source-code","title":"Source Code","text":""},{"location":"DirectMultiStep/utils/io/#directmultistep.utils.io","title":"<code>directmultistep.utils.io</code>","text":""},{"location":"DirectMultiStep/utils/io/#directmultistep.utils.io.DatasetDict","title":"<code>DatasetDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>A dictionary type for storing dataset information.</p> <p>Attributes:</p> Name Type Description <code>products</code> <code>list[str]</code> <p>List of product SMILES strings.</p> <code>starting_materials</code> <code>list[str]</code> <p>List of starting material SMILES strings.</p> <code>path_strings</code> <code>list[str]</code> <p>List of string representations of reaction paths.</p> <code>n_steps_list</code> <code>list[int]</code> <p>List of integers representing the number of steps in each path.</p> <code>ds_name</code> <code>str</code> <p>Name of the dataset.</p> <code>nameToIdx</code> <code>dict[str, list[int]] | None</code> <p>A dictionary mapping names to lists of indices.</p> Source code in <code>src/directmultistep/utils/io.py</code> <pre><code>class DatasetDict(TypedDict, total=False):\n    \"\"\"\n    A dictionary type for storing dataset information.\n\n    Attributes:\n        products: List of product SMILES strings.\n        starting_materials: List of starting material SMILES strings.\n        path_strings: List of string representations of reaction paths.\n        n_steps_list: List of integers representing the number of steps in each path.\n        ds_name: Name of the dataset.\n        nameToIdx: A dictionary mapping names to lists of indices.\n    \"\"\"\n\n    products: list[str]\n    starting_materials: list[str]\n    path_strings: list[str]\n    n_steps_list: list[int]\n    ds_name: str\n    nameToIdx: dict[str, list[int]] | None\n</code></pre>"},{"location":"DirectMultiStep/utils/io/#directmultistep.utils.io.load_dataset_sm","title":"<code>load_dataset_sm(path)</code>","text":"<p>Loads a dataset from a pickle file containing starting materials.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the pickle file.</p> required <p>Returns:</p> Type Description <code>DatasetDict</code> <p>A dictionary containing the loaded dataset.</p> Source code in <code>src/directmultistep/utils/io.py</code> <pre><code>def load_dataset_sm(path: Path) -&gt; DatasetDict:\n    \"\"\"Loads a dataset from a pickle file containing starting materials.\n\n    Args:\n        path: The path to the pickle file.\n\n    Returns:\n        A dictionary containing the loaded dataset.\n    \"\"\"\n    with open(path, \"rb\") as file:\n        products, starting_materials, path_strings, n_steps_list = pickle.load(file)\n    ds_name = path.stem.split(\"_\")[0]\n    return {\n        \"products\": products,\n        \"starting_materials\": starting_materials,\n        \"path_strings\": path_strings,\n        \"n_steps_list\": n_steps_list,\n        \"ds_name\": ds_name,\n    }\n</code></pre>"},{"location":"DirectMultiStep/utils/io/#directmultistep.utils.io.load_dataset_nosm","title":"<code>load_dataset_nosm(path)</code>","text":"<p>Loads a dataset from a pickle file without starting materials.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the pickle file.</p> required <p>Returns:</p> Type Description <code>DatasetDict</code> <p>A dictionary containing the loaded dataset.</p> Source code in <code>src/directmultistep/utils/io.py</code> <pre><code>def load_dataset_nosm(path: Path) -&gt; DatasetDict:\n    \"\"\"Loads a dataset from a pickle file without starting materials.\n\n    Args:\n        path: The path to the pickle file.\n\n    Returns:\n        A dictionary containing the loaded dataset.\n    \"\"\"\n    with open(path, \"rb\") as file:\n        products, _, path_strings, n_steps_list = pickle.load(file)\n    ds_name = path.stem.split(\"_\")[0]\n    return {\n        \"products\": products,\n        \"path_strings\": path_strings,\n        \"n_steps_list\": n_steps_list,\n        \"ds_name\": ds_name,\n    }\n</code></pre>"},{"location":"DirectMultiStep/utils/io/#directmultistep.utils.io.save_dataset_sm","title":"<code>save_dataset_sm(data, path)</code>","text":"<p>Saves a dataset to a pickle file, including starting materials.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>The dataset dictionary to save.</p> required <code>path</code> <code>Path</code> <p>The path to save the pickle file.</p> required Source code in <code>src/directmultistep/utils/io.py</code> <pre><code>def save_dataset_sm(data: dict[str, Any], path: Path) -&gt; None:\n    \"\"\"Saves a dataset to a pickle file, including starting materials.\n\n    Args:\n        data: The dataset dictionary to save.\n        path: The path to save the pickle file.\n    \"\"\"\n    with open(path, \"wb\") as file:\n        p, sm, ps, ns = data[\"products\"], data.get(\"starting_materials\", []), data[\"path_strings\"], data[\"n_steps_list\"]\n        pickle.dump((p, sm, ps, ns), file)\n</code></pre>"},{"location":"DirectMultiStep/utils/io/#directmultistep.utils.io.convert_dict_of_lists_to_list_of_dicts","title":"<code>convert_dict_of_lists_to_list_of_dicts(dict_of_lists)</code>","text":"<p>Converts a dictionary of lists to a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>dict_of_lists</code> <code>DatasetDict</code> <p>The dictionary of lists to convert.</p> required <p>Returns:</p> Type Description <code>list[dict[str, str]]</code> <p>A list of dictionaries.</p> Source code in <code>src/directmultistep/utils/io.py</code> <pre><code>def convert_dict_of_lists_to_list_of_dicts(dict_of_lists: DatasetDict) -&gt; list[dict[str, str]]:\n    \"\"\"Converts a dictionary of lists to a list of dictionaries.\n\n    Args:\n        dict_of_lists: The dictionary of lists to convert.\n\n    Returns:\n        A list of dictionaries.\n    \"\"\"\n    return [dict(zip(dict_of_lists.keys(), values)) for values in zip(*dict_of_lists.values())]\n</code></pre>"},{"location":"DirectMultiStep/utils/io/#directmultistep.utils.io.convert_list_of_dicts_to_dict_of_lists","title":"<code>convert_list_of_dicts_to_dict_of_lists(list_of_dicts)</code>","text":"<p>Converts a list of dictionaries to a dictionary of lists.</p> <p>Parameters:</p> Name Type Description Default <code>list_of_dicts</code> <code>list[dict[str, str]]</code> <p>The list of dictionaries to convert.</p> required <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>A dictionary of lists.</p> Source code in <code>src/directmultistep/utils/io.py</code> <pre><code>def convert_list_of_dicts_to_dict_of_lists(list_of_dicts: list[dict[str, str]]) -&gt; dict[str, list[str]]:\n    \"\"\"Converts a list of dictionaries to a dictionary of lists.\n\n    Args:\n        list_of_dicts: The list of dictionaries to convert.\n\n    Returns:\n        A dictionary of lists.\n    \"\"\"\n    return {key: [item[key] for item in list_of_dicts] for key in list_of_dicts[0].keys()}\n</code></pre>"},{"location":"DirectMultiStep/utils/io/#directmultistep.utils.io.load_pharma_compounds","title":"<code>load_pharma_compounds(path_to_json, load_sm=True)</code>","text":"<p>Loads pharmaceutical compounds from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path_to_json</code> <code>Path</code> <p>The path to the JSON file.</p> required <code>load_sm</code> <code>bool</code> <p>Whether to load starting materials.</p> <code>True</code> <p>Returns:</p> Type Description <code>DatasetDict</code> <p>A dictionary containing the loaded dataset.</p> Source code in <code>src/directmultistep/utils/io.py</code> <pre><code>def load_pharma_compounds(\n    path_to_json: Path,\n    load_sm: bool = True,\n) -&gt; DatasetDict:\n    \"\"\"Loads pharmaceutical compounds from a JSON file.\n\n    Args:\n        path_to_json: The path to the JSON file.\n        load_sm: Whether to load starting materials.\n\n    Returns:\n        A dictionary containing the loaded dataset.\n    \"\"\"\n    with open(path_to_json, \"r\") as file:\n        data = json.load(file)\n    _products, _sms, _path_strings, _steps_list = [], [], [], []\n    name_idx: dict[str, list[int]] = {}\n    idx = 0\n    for item in data:\n        path_dict = eval(item[\"path\"])\n        all_sm = find_leaves(path_dict)\n        if load_sm:\n            for sm in all_sm:\n                name_idx.setdefault(item[\"name\"], []).append(idx)\n                _path_strings.append(item[\"path\"])\n                _products.append(eval(item[\"path\"])[\"smiles\"])\n                _sms.append(sm)\n                _steps_list.append(max_tree_depth(path_dict))\n                idx += 1\n        else:\n            name_idx.setdefault(item[\"name\"], []).append(idx)\n            _path_strings.append(item[\"path\"])\n            _products.append(eval(item[\"path\"])[\"smiles\"])\n            _steps_list.append(max_tree_depth(path_dict))\n            idx += 1\n\n    if load_sm:\n        return {\n            \"products\": _products,\n            \"starting_materials\": _sms,\n            \"path_strings\": _path_strings,\n            \"n_steps_list\": _steps_list,\n            \"nameToIdx\": name_idx,\n        }\n    else:\n        return {\n            \"products\": _products,\n            \"path_strings\": _path_strings,\n            \"n_steps_list\": _steps_list,\n            \"nameToIdx\": name_idx,\n        }\n</code></pre>"},{"location":"DirectMultiStep/utils/io/#directmultistep.utils.io.load_commercial_stock","title":"<code>load_commercial_stock(path)</code>","text":"<p>Loads a set of molecules from a file, canonicalizes them, and returns a set.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file containing molecules.</p> required <p>Returns:</p> Type Description <code>set[str]</code> <p>A set of canonicalized SMILES strings.</p> Source code in <code>src/directmultistep/utils/io.py</code> <pre><code>def load_commercial_stock(path: Path) -&gt; set[str]:\n    \"\"\"Loads a set of molecules from a file, canonicalizes them, and returns a set.\n\n    Args:\n        path: The path to the file containing molecules.\n\n    Returns:\n        A set of canonicalized SMILES strings.\n    \"\"\"\n    with open(path, \"r\") as file:\n        stock = file.readlines()\n    canonical_stock = set()\n    for molecule in stock:\n        canonical_stock.add(canonicalize_smiles(molecule.strip()))\n    return canonical_stock\n</code></pre>"},{"location":"DirectMultiStep/utils/post-process/","title":"Multistep Route Post-processing","text":"<p>This module provides useful data structure classes and helper functions for postprocessing beam search results and multistep routes generated by DirectMultiStep models.</p>"},{"location":"DirectMultiStep/utils/post-process/#example-use","title":"Example Use","text":"<p>The most useful functions are <code>canonicalize_path_dict</code>, <code>canonicalize_path_string</code>, and functions that start with <code>find_</code></p> <pre><code>from directmultistep.utils.pre_process import stringify_dict\nfrom directmultistep.utils.post_process import canonicalize_path_dict, canonicalize_path_string\n\npath_string = \"{'smiles':'CNCc1cc(-c2ccccc2F)n(S(=O)(=O)c2cccnc2)c1','children':[{'smiles':'O=Cc1cc(-c2ccccc2F)n(S(=O)(=O)c2cccnc2)c1','children':[{'smiles':'O=Cc1c[nH]c(-c2ccccc2F)c1'},{'smiles':'O=S(=O)(Cl)c1cccnc1'}]},{'smiles':'CN'}]}\"\n\ncano_path_dict = canonicalize_path_dict(eval(path_string))\ncano_path_string = stringify_dict(cano_path_dict)\n\nprint(cano_path_string == canonicalize_path_string(path_string))\n</code></pre>"},{"location":"DirectMultiStep/utils/post-process/#source-code","title":"Source Code","text":""},{"location":"DirectMultiStep/utils/post-process/#directmultistep.utils.post_process","title":"<code>directmultistep.utils.post_process</code>","text":""},{"location":"DirectMultiStep/utils/post-process/#directmultistep.utils.post_process.count_unsolved_targets","title":"<code>count_unsolved_targets(beam_results_NS2)</code>","text":"<p>Counts the number of unsolved targets in a list of beam results.</p> <p>An unsolved target is defined as a target for which the list of paths is empty. Note that this differs from the typical definition of a solved target. Typically, solved targets are defined as targets with routes where all starting materials (SMs) are in a given stock compound set.</p> <p>Parameters:</p> Name Type Description Default <code>beam_results_NS2</code> <code>BeamResultType | PathsProcessedType</code> <p>A list of beam results, where each beam result is a list of paths.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of unsolved targets.</p> Source code in <code>src/directmultistep/utils/post_process.py</code> <pre><code>def count_unsolved_targets(beam_results_NS2: BeamResultType | PathsProcessedType) -&gt; int:\n    \"\"\"Counts the number of unsolved targets in a list of beam results.\n\n    An unsolved target is defined as a target for which the list of paths is empty. Note that this\n    differs from the typical definition of a solved target. Typically, solved targets are\n    defined as targets with routes where all starting materials (SMs) are in a given stock compound\n    set.\n\n    Args:\n        beam_results_NS2: A list of beam results, where each beam result is a\n            list of paths.\n\n    Returns:\n        The number of unsolved targets.\n    \"\"\"\n    n_empty = 0\n    for path_list in beam_results_NS2:\n        if len(path_list) == 0:\n            n_empty += 1\n    return n_empty\n</code></pre>"},{"location":"DirectMultiStep/utils/post-process/#directmultistep.utils.post_process.find_valid_paths","title":"<code>find_valid_paths(beam_results_NS2)</code>","text":"<p>Finds valid paths from beam search results.</p> <p>This function processes beam search results, extracts the path string, canonicalizes the SMILES strings of the reactants, and returns a list of valid paths with canonicalized SMILES.</p> <p>Parameters:</p> Name Type Description Default <code>beam_results_NS2</code> <code>BeamResultType</code> <p>A list of beam results, where each beam result is a list of (path_string, score) tuples.</p> required <p>Returns:</p> Type Description <code>PathsProcessedType</code> <p>A list of valid paths, where each path is a tuple of</p> <code>PathsProcessedType</code> <p>(canonicalized_path_string, list_of_canonicalized_reactant_SMILES).</p> Source code in <code>src/directmultistep/utils/post_process.py</code> <pre><code>def find_valid_paths(beam_results_NS2: BeamResultType) -&gt; PathsProcessedType:\n    \"\"\"Finds valid paths from beam search results.\n\n    This function processes beam search results, extracts the path string,\n    canonicalizes the SMILES strings of the reactants, and returns a list of\n    valid paths with canonicalized SMILES.\n\n    Args:\n        beam_results_NS2: A list of beam results, where each beam result is a\n            list of (path_string, score) tuples.\n\n    Returns:\n        A list of valid paths, where each path is a tuple of\n        (canonicalized_path_string, list_of_canonicalized_reactant_SMILES).\n    \"\"\"\n    valid_pathreac_NS2n = []\n    iterator = tqdm(beam_results_NS2) if SHOW_PROGRESS_BARS else beam_results_NS2\n    for beam_result_S2 in cast(Iterator[list[tuple[str, float]]], iterator):\n        valid_pathreac_S2n = []\n        for path_string, _ in beam_result_S2:\n            try:\n                node = eval(path_string)\n                reactants = find_leaves(node)\n                canon_reactants = [canonicalize_smiles(reactant) for reactant in reactants]\n                canon_path = canonicalize_path_string(path_string)\n            except:  # noqa: E722\n                continue\n            valid_pathreac_S2n.append((canon_path, canon_reactants))\n        valid_pathreac_NS2n.append(valid_pathreac_S2n)\n    return valid_pathreac_NS2n\n</code></pre>"},{"location":"DirectMultiStep/utils/post-process/#directmultistep.utils.post_process.find_matching_paths","title":"<code>find_matching_paths(paths_NS2n, correct_paths, ignore_ids=None)</code>","text":"<p>Finds matching paths between predicted paths and correct paths.</p> <p>This function compares predicted paths with a list of correct paths and returns the rank at which the correct path was found. It also checks for matches after considering all permutations of the predicted path.</p> <p>Parameters:</p> Name Type Description Default <code>paths_NS2n</code> <code>PathsProcessedType</code> <p>A list of predicted paths, where each path is a list of (path_string, list_of_reactant_SMILES) tuples.</p> required <code>correct_paths</code> <code>list[str]</code> <p>A list of correct path strings.</p> required <code>ignore_ids</code> <code>set[int] | None</code> <p>A set of indices to ignore during matching.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[MatchList, MatchList]</code> <p>A tuple containing two lists: - match_accuracy_N: List of ranks at which the correct path was   found (None if not found). - perm_match_accuracy_N: List of ranks at which the correct path   was found after considering permutations (None if not found).</p> Source code in <code>src/directmultistep/utils/post_process.py</code> <pre><code>def find_matching_paths(\n    paths_NS2n: PathsProcessedType, correct_paths: list[str], ignore_ids: set[int] | None = None\n) -&gt; tuple[MatchList, MatchList]:\n    \"\"\"Finds matching paths between predicted paths and correct paths.\n\n    This function compares predicted paths with a list of correct paths and\n    returns the rank at which the correct path was found. It also checks for\n    matches after considering all permutations of the predicted path.\n\n    Args:\n        paths_NS2n: A list of predicted paths, where each path is a list of\n            (path_string, list_of_reactant_SMILES) tuples.\n        correct_paths: A list of correct path strings.\n        ignore_ids: A set of indices to ignore during matching.\n\n    Returns:\n        A tuple containing two lists:\n            - match_accuracy_N: List of ranks at which the correct path was\n              found (None if not found).\n            - perm_match_accuracy_N: List of ranks at which the correct path\n              was found after considering permutations (None if not found).\n    \"\"\"\n    if ignore_ids is None:\n        ignore_ids = set()\n    match_accuracy_N: MatchList = []\n    perm_match_accuracy_N: MatchList = []\n    iterator = (\n        tqdm(enumerate(zip(paths_NS2n, correct_paths)), total=len(paths_NS2n))\n        if SHOW_PROGRESS_BARS\n        else enumerate(zip(paths_NS2n, correct_paths))\n    )\n    for i, (pathreac_S2n, correct_path) in cast(Iterator[tuple[int, tuple[BeamProcessedType, str]]], iterator):\n        if i in ignore_ids:\n            continue\n        path_match = None\n        path_match_perm = None\n        for rank, (path, _) in enumerate(pathreac_S2n):\n            if path_match is None and path == correct_path:\n                path_match = rank + 1\n            if path_match_perm is None:\n                all_perms = generate_permutations(data=eval(path), max_perm=None)\n                if correct_path in all_perms:\n                    path_match_perm = rank + 1\n            if path_match and path_match_perm:\n                break\n        match_accuracy_N.append(path_match)\n        perm_match_accuracy_N.append(path_match_perm)\n    return match_accuracy_N, perm_match_accuracy_N\n</code></pre>"},{"location":"DirectMultiStep/utils/post-process/#directmultistep.utils.post_process.find_top_n_accuracy","title":"<code>find_top_n_accuracy(match_accuracy, n_vals, dec_digs=1)</code>","text":"<p>Calculates top-n accuracy for a list of match ranks.</p> <p>This function calculates the fraction of paths that were found within the top-n ranks for a given list of n values.</p> <p>Parameters:</p> Name Type Description Default <code>match_accuracy</code> <code>MatchList</code> <p>A list of ranks at which the correct path was found (None if not found).</p> required <code>n_vals</code> <code>list[int]</code> <p>A list of n values for which to calculate top-n accuracy.</p> required <code>dec_digs</code> <code>int</code> <p>The number of decimal digits to round the accuracy to.</p> <code>1</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>A dictionary mapping \"Top n\" to the corresponding accuracy fraction</p> <code>dict[str, str]</code> <p>(as a string).</p> Source code in <code>src/directmultistep/utils/post_process.py</code> <pre><code>def find_top_n_accuracy(match_accuracy: MatchList, n_vals: list[int], dec_digs: int = 1) -&gt; dict[str, str]:\n    \"\"\"Calculates top-n accuracy for a list of match ranks.\n\n    This function calculates the fraction of paths that were found within the\n    top-n ranks for a given list of n values.\n\n    Args:\n        match_accuracy: A list of ranks at which the correct path was found\n            (None if not found).\n        n_vals: A list of n values for which to calculate top-n accuracy.\n        dec_digs: The number of decimal digits to round the accuracy to.\n\n    Returns:\n        A dictionary mapping \"Top n\" to the corresponding accuracy fraction\n        (as a string).\n    \"\"\"\n    n_vals = sorted(n_vals)\n    top_counts = {f\"Top {n}\": 0 for n in n_vals}\n    for rank in match_accuracy:\n        if rank is None:\n            continue\n        for n in n_vals:\n            if rank &lt;= n:\n                top_counts[f\"Top {n}\"] += 1\n    top_fractions = {k: f\"{(v / len(match_accuracy)* 100):.{dec_digs}f}\" for k, v in top_counts.items()}\n    return top_fractions\n</code></pre>"},{"location":"DirectMultiStep/utils/post-process/#directmultistep.utils.post_process.remove_repetitions_within_beam_result","title":"<code>remove_repetitions_within_beam_result(paths_NS2n)</code>","text":"<p>Removes duplicate paths within each beam result.</p> <p>This function iterates through each beam result and removes duplicate paths based on their stringified representation after considering all permutations.</p> <p>Parameters:</p> Name Type Description Default <code>paths_NS2n</code> <code>PathsProcessedType</code> <p>A list of beam results, where each beam result is a list of (path_string, list_of_reactant_SMILES) tuples.</p> required <p>Returns:</p> Type Description <code>PathsProcessedType</code> <p>A list of beam results with duplicate paths removed.</p> Source code in <code>src/directmultistep/utils/post_process.py</code> <pre><code>def remove_repetitions_within_beam_result(\n    paths_NS2n: PathsProcessedType,\n) -&gt; PathsProcessedType:\n    \"\"\"Removes duplicate paths within each beam result.\n\n    This function iterates through each beam result and removes duplicate paths\n    based on their stringified representation after considering all permutations.\n\n    Args:\n        paths_NS2n: A list of beam results, where each beam result is a list of\n            (path_string, list_of_reactant_SMILES) tuples.\n\n    Returns:\n        A list of beam results with duplicate paths removed.\n    \"\"\"\n    unique_paths_NS2n = []\n    iterator = tqdm(paths_NS2n) if SHOW_PROGRESS_BARS else paths_NS2n\n    for path_reac_S2 in cast(Iterator[BeamProcessedType], iterator):\n        unique_paths_S2n = []\n        seen = set()\n        for path, reacs_n in path_reac_S2:\n            for permuted_pathstring in generate_permutations(data=eval(path), max_perm=None):\n                if permuted_pathstring in seen:\n                    break\n            else:\n                seen.add(path)\n                unique_paths_S2n.append((path, reacs_n))\n        unique_paths_NS2n.append(unique_paths_S2n)\n    return unique_paths_NS2n\n</code></pre>"},{"location":"DirectMultiStep/utils/post-process/#directmultistep.utils.post_process.find_paths_with_commercial_sm","title":"<code>find_paths_with_commercial_sm(paths_NS2n, commercial_stock)</code>","text":"<p>Finds paths that use only commercially available starting materials.</p> <p>This function filters a list of paths, keeping only those where all reactants are present in the provided commercial stock.</p> <p>Parameters:</p> Name Type Description Default <code>paths_NS2n</code> <code>PathsProcessedType</code> <p>A list of beam results, where each beam result is a list of (path_string, list_of_reactant_SMILES) tuples.</p> required <code>commercial_stock</code> <code>set[str]</code> <p>A set of SMILES strings representing commercially available starting materials.</p> required <p>Returns:</p> Type Description <code>PathsProcessedType</code> <p>A list of beam results containing only paths with commercial starting</p> <code>PathsProcessedType</code> <p>materials.</p> Source code in <code>src/directmultistep/utils/post_process.py</code> <pre><code>def find_paths_with_commercial_sm(paths_NS2n: PathsProcessedType, commercial_stock: set[str]) -&gt; PathsProcessedType:\n    \"\"\"Finds paths that use only commercially available starting materials.\n\n    This function filters a list of paths, keeping only those where all\n    reactants are present in the provided commercial stock.\n\n    Args:\n        paths_NS2n: A list of beam results, where each beam result is a list of\n            (path_string, list_of_reactant_SMILES) tuples.\n        commercial_stock: A set of SMILES strings representing commercially\n            available starting materials.\n\n    Returns:\n        A list of beam results containing only paths with commercial starting\n        materials.\n    \"\"\"\n    available_paths_NS2n = []\n    iterator = tqdm(paths_NS2n) if SHOW_PROGRESS_BARS else paths_NS2n\n    for path_reac_S2 in cast(Iterator[BeamProcessedType], iterator):\n        available_paths_S2n = []\n        for path, reacs_n in path_reac_S2:\n            if all(reactant in commercial_stock for reactant in reacs_n):\n                available_paths_S2n.append((path, reacs_n))\n        available_paths_NS2n.append(available_paths_S2n)\n    return available_paths_NS2n\n</code></pre>"},{"location":"DirectMultiStep/utils/post-process/#directmultistep.utils.post_process.find_paths_with_correct_product_and_reactants","title":"<code>find_paths_with_correct_product_and_reactants(paths_NS2n, true_products, true_reacs=None)</code>","text":"<p>Finds paths that have the correct product and, optionally, the correct reactants.</p> <p>This function filters a list of paths, keeping only those where the product SMILES matches the corresponding true product SMILES, and optionally, where at least one of the reactants matches the corresponding true reactant SMILES.</p> <p>Parameters:</p> Name Type Description Default <code>paths_NS2n</code> <code>PathsProcessedType</code> <p>A list of beam results, where each beam result is a list of (path_string, list_of_reactant_SMILES) tuples.</p> required <code>true_products</code> <code>list[str]</code> <p>A list of SMILES strings representing the correct products.</p> required <code>true_reacs</code> <code>list[str] | None</code> <p>An optional list of SMILES strings representing the correct reactants.</p> <code>None</code> <p>Returns:</p> Type Description <code>PathsProcessedType</code> <p>A list of beam results containing only paths with the correct product</p> <code>PathsProcessedType</code> <p>and reactants (if provided).</p> Source code in <code>src/directmultistep/utils/post_process.py</code> <pre><code>def find_paths_with_correct_product_and_reactants(\n    paths_NS2n: PathsProcessedType,\n    true_products: list[str],\n    true_reacs: list[str] | None = None,\n) -&gt; PathsProcessedType:\n    \"\"\"Finds paths that have the correct product and, optionally, the correct reactants.\n\n    This function filters a list of paths, keeping only those where the product\n    SMILES matches the corresponding true product SMILES, and optionally,\n    where at least one of the reactants matches the corresponding true reactant\n    SMILES.\n\n    Args:\n        paths_NS2n: A list of beam results, where each beam result is a list of\n            (path_string, list_of_reactant_SMILES) tuples.\n        true_products: A list of SMILES strings representing the correct\n            products.\n        true_reacs: An optional list of SMILES strings representing the correct\n            reactants.\n\n    Returns:\n        A list of beam results containing only paths with the correct product\n        and reactants (if provided).\n    \"\"\"\n    f = canonicalize_smiles\n    correct_paths_NS2n = []\n    iterator = tqdm(enumerate(paths_NS2n)) if SHOW_PROGRESS_BARS else enumerate(paths_NS2n)\n    for idx, path_reac_S2 in cast(Iterator[tuple[int, BeamProcessedType]], iterator):\n        correct_paths_S2n = []\n        for path, reacs_n in path_reac_S2:\n            path_tree = eval(path)\n            if f(path_tree[\"smiles\"]) == f(true_products[idx]) and (\n                true_reacs is None or f(true_reacs[idx]) in reacs_n\n            ):\n                correct_paths_S2n.append((path, reacs_n))\n        correct_paths_NS2n.append(correct_paths_S2n)\n    return correct_paths_NS2n\n</code></pre>"},{"location":"DirectMultiStep/utils/post-process/#directmultistep.utils.post_process.canonicalize_path_dict","title":"<code>canonicalize_path_dict(path_dict)</code>","text":"<p>Canonicalizes a FilteredDict representing a path.</p> <p>This function recursively canonicalizes the SMILES strings in a FilteredDict and its children.</p> <p>Parameters:</p> Name Type Description Default <code>path_dict</code> <code>FilteredDict</code> <p>A FilteredDict representing a path.</p> required <p>Returns:</p> Type Description <code>FilteredDict</code> <p>A FilteredDict with canonicalized SMILES strings.</p> Source code in <code>src/directmultistep/utils/post_process.py</code> <pre><code>def canonicalize_path_dict(path_dict: FilteredDict) -&gt; FilteredDict:\n    \"\"\"Canonicalizes a FilteredDict representing a path.\n\n    This function recursively canonicalizes the SMILES strings in a\n    FilteredDict and its children.\n\n    Args:\n        path_dict: A FilteredDict representing a path.\n\n    Returns:\n        A FilteredDict with canonicalized SMILES strings.\n    \"\"\"\n    canon_dict: FilteredDict = {}\n    canon_dict[\"smiles\"] = canonicalize_smiles(path_dict[\"smiles\"])\n    if \"children\" in path_dict:\n        canon_dict[\"children\"] = []\n        for child in path_dict[\"children\"]:\n            canon_dict[\"children\"].append(canonicalize_path_dict(child))\n    return canon_dict\n</code></pre>"},{"location":"DirectMultiStep/utils/post-process/#directmultistep.utils.post_process.canonicalize_path_string","title":"<code>canonicalize_path_string(path_string)</code>","text":"<p>Canonicalizes a path string.</p> <p>This function converts a path string to a FilteredDict, canonicalizes it, and then converts it back to a string.</p> <p>Parameters:</p> Name Type Description Default <code>path_string</code> <code>str</code> <p>A string representing a path.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A canonicalized string representation of the path.</p> Source code in <code>src/directmultistep/utils/post_process.py</code> <pre><code>def canonicalize_path_string(path_string: str) -&gt; str:\n    \"\"\"Canonicalizes a path string.\n\n    This function converts a path string to a FilteredDict, canonicalizes it,\n    and then converts it back to a string.\n\n    Args:\n        path_string: A string representing a path.\n\n    Returns:\n        A canonicalized string representation of the path.\n    \"\"\"\n    canon_dict = canonicalize_path_dict(eval(path_string))\n    return stringify_dict(canon_dict)\n</code></pre>"},{"location":"DirectMultiStep/utils/post-process/#directmultistep.utils.post_process.process_paths","title":"<code>process_paths(paths_NS2n, true_products, true_reacs=None, commercial_stock=None)</code>","text":"<p>Processes a list of paths by canonicalizing, removing repetitions, and filtering.</p> <p>This function performs a series of processing steps on a list of paths, including canonicalization, removal of repetitions, filtering by commercial availability, and filtering by correct product and reactants.</p> <p>Parameters:</p> Name Type Description Default <code>paths_NS2n</code> <code>PathsProcessedType</code> <p>A list of beam results, where each beam result is a list of (path_string, list_of_reactant_SMILES) tuples.</p> required <code>true_products</code> <code>list[str]</code> <p>A list of SMILES strings representing the correct products.</p> required <code>true_reacs</code> <code>list[str] | None</code> <p>An optional list of SMILES strings representing the correct reactants.</p> <code>None</code> <code>commercial_stock</code> <code>set[str] | None</code> <p>An optional set of SMILES strings representing commercially available starting materials.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[PathsProcessedType, dict[str, int]]</code> <p>A tuple containing: - A list of beam results containing only the correct paths. - A dictionary containing the number of solved targets at each   stage of processing.</p> Source code in <code>src/directmultistep/utils/post_process.py</code> <pre><code>def process_paths(\n    paths_NS2n: PathsProcessedType,\n    true_products: list[str],\n    true_reacs: list[str] | None = None,\n    commercial_stock: set[str] | None = None,\n) -&gt; tuple[PathsProcessedType, dict[str, int]]:\n    \"\"\"Processes a list of paths by canonicalizing, removing repetitions, and filtering.\n\n    This function performs a series of processing steps on a list of paths,\n    including canonicalization, removal of repetitions, filtering by commercial\n    availability, and filtering by correct product and reactants.\n\n    Args:\n        paths_NS2n: A list of beam results, where each beam result is a list of\n            (path_string, list_of_reactant_SMILES) tuples.\n        true_products: A list of SMILES strings representing the correct\n            products.\n        true_reacs: An optional list of SMILES strings representing the correct\n            reactants.\n        commercial_stock: An optional set of SMILES strings representing\n            commercially available starting materials.\n\n    Returns:\n        A tuple containing:\n            - A list of beam results containing only the correct paths.\n            - A dictionary containing the number of solved targets at each\n              stage of processing.\n    \"\"\"\n    canon_paths_NS2n = canonicalize_paths(paths_NS2n)\n    unique_paths_NS2n = remove_repetitions_within_beam_result(canon_paths_NS2n)\n    if commercial_stock is None:\n        available_paths_NS2n = unique_paths_NS2n\n    else:\n        available_paths_NS2n = find_paths_with_commercial_sm(unique_paths_NS2n, commercial_stock)\n    correct_paths_NS2n = find_paths_with_correct_product_and_reactants(available_paths_NS2n, true_products, true_reacs)\n    total = len(true_products)\n    solvability = {\n        \"solved (canonicalized)\": total - count_unsolved_targets(canon_paths_NS2n),\n        \"solved (unique)\": total - count_unsolved_targets(unique_paths_NS2n),\n        \"solved (available)\": total - count_unsolved_targets(available_paths_NS2n),\n        \"solved (correct)\": total - count_unsolved_targets(correct_paths_NS2n),\n    }\n    return correct_paths_NS2n, solvability\n</code></pre>"},{"location":"DirectMultiStep/utils/post-process/#directmultistep.utils.post_process.process_path_single","title":"<code>process_path_single(paths_NS2n, true_products, true_reacs=None, commercial_stock=None)</code>","text":"<p>Processes a list of paths by canonicalizing, removing repetitions, and filtering.</p> <p>This function performs a series of processing steps on a list of paths, including canonicalization, removal of repetitions, filtering by commercial availability, and filtering by correct product and reactants. This function is similar to <code>process_paths</code> but does not return the solvability dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>paths_NS2n</code> <code>PathsProcessedType</code> <p>A list of beam results, where each beam result is a list of (path_string, list_of_reactant_SMILES) tuples.</p> required <code>true_products</code> <code>list[str]</code> <p>A list of SMILES strings representing the correct products.</p> required <code>true_reacs</code> <code>list[str] | None</code> <p>An optional list of SMILES strings representing the correct reactants.</p> <code>None</code> <code>commercial_stock</code> <code>set[str] | None</code> <p>An optional set of SMILES strings representing commercially available starting materials.</p> <code>None</code> <p>Returns:</p> Type Description <code>PathsProcessedType</code> <p>A list of beam results containing only the correct paths.</p> Source code in <code>src/directmultistep/utils/post_process.py</code> <pre><code>def process_path_single(\n    paths_NS2n: PathsProcessedType,\n    true_products: list[str],\n    true_reacs: list[str] | None = None,\n    commercial_stock: set[str] | None = None,\n) -&gt; PathsProcessedType:\n    \"\"\"Processes a list of paths by canonicalizing, removing repetitions, and filtering.\n\n    This function performs a series of processing steps on a list of paths,\n    including canonicalization, removal of repetitions, filtering by commercial\n    availability, and filtering by correct product and reactants.\n    This function is similar to `process_paths` but does not return the\n    solvability dictionary.\n\n    Args:\n        paths_NS2n: A list of beam results, where each beam result is a list of\n            (path_string, list_of_reactant_SMILES) tuples.\n        true_products: A list of SMILES strings representing the correct\n            products.\n        true_reacs: An optional list of SMILES strings representing the correct\n            reactants.\n        commercial_stock: An optional set of SMILES strings representing\n            commercially available starting materials.\n\n    Returns:\n        A list of beam results containing only the correct paths.\n    \"\"\"\n    canon_paths_NS2n = canonicalize_paths(paths_NS2n)\n    unique_paths_NS2n = remove_repetitions_within_beam_result(canon_paths_NS2n)\n    if commercial_stock is None:\n        available_paths_NS2n = unique_paths_NS2n\n    else:\n        available_paths_NS2n = find_paths_with_commercial_sm(unique_paths_NS2n, commercial_stock)\n    correct_paths_NS2n = find_paths_with_correct_product_and_reactants(available_paths_NS2n, true_products, true_reacs)\n    return correct_paths_NS2n\n</code></pre>"},{"location":"DirectMultiStep/utils/post-process/#directmultistep.utils.post_process.process_paths_post","title":"<code>process_paths_post(paths_NS2n, true_products, true_reacs, commercial_stock)</code>","text":"<p>Processes a list of paths by removing repetitions, filtering, and canonicalizing.</p> <p>This function performs a series of processing steps on a list of paths, including removal of repetitions, filtering by commercial availability, filtering by correct product and reactants, and canonicalization.</p> <p>Parameters:</p> Name Type Description Default <code>paths_NS2n</code> <code>PathsProcessedType</code> <p>A list of beam results, where each beam result is a list of (path_string, list_of_reactant_SMILES) tuples.</p> required <code>true_products</code> <code>list[str]</code> <p>A list of SMILES strings representing the correct products.</p> required <code>true_reacs</code> <code>list[str]</code> <p>A list of SMILES strings representing the correct reactants.</p> required <code>commercial_stock</code> <code>set[str]</code> <p>A set of SMILES strings representing commercially available starting materials.</p> required <p>Returns:</p> Type Description <code>PathsProcessedType</code> <p>A list of beam results containing only the correct paths, canonicalized.</p> Source code in <code>src/directmultistep/utils/post_process.py</code> <pre><code>def process_paths_post(\n    paths_NS2n: PathsProcessedType,\n    true_products: list[str],\n    true_reacs: list[str],\n    commercial_stock: set[str],\n) -&gt; PathsProcessedType:\n    \"\"\"Processes a list of paths by removing repetitions, filtering, and canonicalizing.\n\n    This function performs a series of processing steps on a list of paths,\n    including removal of repetitions, filtering by commercial availability,\n    filtering by correct product and reactants, and canonicalization.\n\n    Args:\n        paths_NS2n: A list of beam results, where each beam result is a list of\n            (path_string, list_of_reactant_SMILES) tuples.\n        true_products: A list of SMILES strings representing the correct\n            products.\n        true_reacs: A list of SMILES strings representing the correct reactants.\n        commercial_stock: A set of SMILES strings representing commercially\n            available starting materials.\n\n    Returns:\n        A list of beam results containing only the correct paths, canonicalized.\n    \"\"\"\n    unique_paths_NS2n = remove_repetitions_within_beam_result(paths_NS2n)\n    available_paths_NS2n = find_paths_with_commercial_sm(unique_paths_NS2n, commercial_stock)\n    correct_paths_NS2n = find_paths_with_correct_product_and_reactants(available_paths_NS2n, true_products, true_reacs)\n    canon_paths_NS2n = canonicalize_paths(correct_paths_NS2n)\n    return canon_paths_NS2n\n</code></pre>"},{"location":"DirectMultiStep/utils/post-process/#directmultistep.utils.post_process.calculate_top_k_counts_by_step_length","title":"<code>calculate_top_k_counts_by_step_length(match_accuracy, n_steps_list, k_vals)</code>","text":"<p>Calculate accuracy statistics grouped by number of steps.</p> <p>Parameters:</p> Name Type Description Default <code>match_accuracy</code> <code>list[int | None]</code> <p>List of ranks at which each path was found (None if not found)</p> required <code>n_steps_list</code> <code>list[int]</code> <p>List of number of steps for each path</p> required <code>k_vals</code> <code>list[int]</code> <p>List of k values to calculate top-k accuracy for</p> required <p>Returns:</p> Type Description <code>dict[int, dict[str, int]]</code> <p>Dictionary mapping step count to accuracy statistics</p> Source code in <code>src/directmultistep/utils/post_process.py</code> <pre><code>def calculate_top_k_counts_by_step_length(\n    match_accuracy: list[int | None], n_steps_list: list[int], k_vals: list[int]\n) -&gt; dict[int, dict[str, int]]:\n    \"\"\"Calculate accuracy statistics grouped by number of steps.\n\n    Args:\n        match_accuracy: List of ranks at which each path was found (None if not\n            found)\n        n_steps_list: List of number of steps for each path\n        k_vals: List of k values to calculate top-k accuracy for\n\n    Returns:\n        Dictionary mapping step count to accuracy statistics\n    \"\"\"\n    step_stats: dict[int, dict[str, int]] = {}\n\n    for rank, n_steps in zip(match_accuracy, n_steps_list):\n        if n_steps not in step_stats:\n            step_stats[n_steps] = {\"Total\": 0}\n\n        step_stats[n_steps][\"Total\"] += 1\n\n        if rank is None:\n            step_stats[n_steps][\"Not Found\"] = step_stats[n_steps].get(\"Not Found\", 0) + 1\n        else:\n            for k in k_vals:\n                if rank &lt;= k:\n                    step_stats[n_steps][f\"Top {k}\"] = step_stats[n_steps].get(f\"Top {k}\", 0) + 1\n\n    return step_stats\n</code></pre>"},{"location":"DirectMultiStep/utils/pre-process/","title":"Multistep Route Pre-processing","text":"<p>This module provides useful data structure classes and helper functions for preprocessing multistep routes for training and testing DirectMultiStep models.</p>"},{"location":"DirectMultiStep/utils/pre-process/#example-use","title":"Example Use","text":"<p>The most frequently used data structure is <code>FilteredDict</code>, a dictionary format for multistep routes used in DirectMultiStep models. Several useful functions are available, such as <code>canonicalize_smiles</code>, <code>max_tree_depth</code>, <code>find_leaves</code>, <code>stringify_dict</code>, and <code>generate_permutations</code>, among others. For example:</p> <pre><code>from directmultistep.utils.pre_process import stringify_dict\n\npath_string = \"{'smiles':'CNCc1cc(-c2ccccc2F)n(S(=O)(=O)c2cccnc2)c1','children':[{'smiles':'O=Cc1cc(-c2ccccc2F)n(S(=O)(=O)c2cccnc2)c1','children':[{'smiles':'O=Cc1c[nH]c(-c2ccccc2F)c1'},{'smiles':'O=S(=O)(Cl)c1cccnc1'}]},{'smiles':'CN'}]}\"\n\n# This should evaluate to True, as it compares the stringified version of your FilteredDict\nprint(stringify_dict(eval(path_string)) == path_string)\n</code></pre>"},{"location":"DirectMultiStep/utils/pre-process/#source-code","title":"Source Code","text":""},{"location":"DirectMultiStep/utils/pre-process/#directmultistep.utils.pre_process","title":"<code>directmultistep.utils.pre_process</code>","text":""},{"location":"DirectMultiStep/utils/pre-process/#directmultistep.utils.pre_process.PaRoutesDict","title":"<code>PaRoutesDict = dict[str, str | bool | list['PaRoutesDict']]</code>  <code>module-attribute</code>","text":""},{"location":"DirectMultiStep/utils/pre-process/#directmultistep.utils.pre_process.FilteredDict","title":"<code>FilteredDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>A dictionary format for multistep routes, used in DirectMultiStep models.</p> <p>This dictionary is designed to represent a node in a synthetic route tree. It contains the SMILES string of a molecule and a list of its child nodes. To get its string format, use <code>stringify_dict</code>.</p> <p>Attributes:</p> Name Type Description <code>smiles</code> <code>str</code> <p>SMILES string of the molecule.</p> <code>children</code> <code>list[FilteredDict]</code> <p>List of child nodes, each a FilteredDict.</p> Source code in <code>src/directmultistep/utils/pre_process.py</code> <pre><code>class FilteredDict(TypedDict, total=False):\n    \"\"\"A dictionary format for multistep routes, used in DirectMultiStep models.\n\n    This dictionary is designed to represent a node in a synthetic route tree.\n    It contains the SMILES string of a molecule and a list of its child nodes.\n    To get its string format, use `stringify_dict`.\n\n    Attributes:\n        smiles: SMILES string of the molecule.\n        children: List of child nodes, each a FilteredDict.\n    \"\"\"\n\n    smiles: str\n    children: list[\"FilteredDict\"]\n</code></pre>"},{"location":"DirectMultiStep/utils/pre-process/#directmultistep.utils.pre_process.filter_mol_nodes","title":"<code>filter_mol_nodes(node)</code>","text":"<p>Filters a PaRoutes dictionary to keep only 'smiles' and 'children' keys.</p> <p>This function removes extra information like 'metadata', 'rsmi', and 'reaction_hash', keeping only the 'smiles' and 'children' keys. It also canonicalizes the SMILES string using RDKit.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>PaRoutesDict</code> <p>A dictionary representing a node in a PaRoutes data structure.</p> required <p>Returns:</p> Type Description <code>FilteredDict</code> <p>A FilteredDict containing the canonicalized SMILES and filtered children.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the 'type' of the node is not 'mol' or if 'children' is not a list.</p> Source code in <code>src/directmultistep/utils/pre_process.py</code> <pre><code>def filter_mol_nodes(node: PaRoutesDict) -&gt; FilteredDict:\n    \"\"\"Filters a PaRoutes dictionary to keep only 'smiles' and 'children' keys.\n\n    This function removes extra information like 'metadata', 'rsmi', and\n    'reaction_hash', keeping only the 'smiles' and 'children' keys. It also\n    canonicalizes the SMILES string using RDKit.\n\n    Args:\n        node: A dictionary representing a node in a PaRoutes data structure.\n\n    Returns:\n        A FilteredDict containing the canonicalized SMILES and filtered children.\n\n    Raises:\n        ValueError: If the 'type' of the node is not 'mol' or if 'children' is not a list.\n    \"\"\"\n    # canonicalize smiles by passing through RDKit\n    canonical_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(node[\"smiles\"]))\n    if \"children\" not in node:\n        return {\"smiles\": canonical_smiles}\n    if node.get(\"type\") != \"mol\":\n        raise ValueError(f\"Expected 'type' to be 'mol', got {node.get('type', 'empty')}\")\n\n    filtered_node: FilteredDict = {\"smiles\": canonical_smiles, \"children\": []}\n    # we skip one level of the PaRoutes dictionary as it contains the reaction meta data\n    # assert isinstance(node[\"children\"], list), f\"Expected 'children' to be a list, got {type(node['children'])}\"\n    if not isinstance(node[\"children\"], list):\n        raise ValueError(f\"Expected 'children' to be a list, got {type(node['children'])}\")\n    reaction_meta: list[PaRoutesDict] = node[\"children\"]\n    first_child = reaction_meta[0]\n    for child in cast(list[PaRoutesDict], first_child[\"children\"]):\n        filtered_node[\"children\"].append(filter_mol_nodes(child))\n    return filtered_node\n</code></pre>"},{"location":"DirectMultiStep/utils/pre-process/#directmultistep.utils.pre_process.max_tree_depth","title":"<code>max_tree_depth(node)</code>","text":"<p>Calculates the maximum depth of a synthetic route tree.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>FilteredDict</code> <p>A FilteredDict representing a node in the route tree.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The maximum depth of the tree. Returns 0 for a leaf node.</p> Source code in <code>src/directmultistep/utils/pre_process.py</code> <pre><code>def max_tree_depth(node: FilteredDict) -&gt; int:\n    \"\"\"Calculates the maximum depth of a synthetic route tree.\n\n    Args:\n        node: A FilteredDict representing a node in the route tree.\n\n    Returns:\n        The maximum depth of the tree. Returns 0 for a leaf node.\n    \"\"\"\n    if \"children\" not in node:\n        return 0  # Leaf node, depth is 0\n    else:\n        child_depths = [\n            max_tree_depth(child)\n            for child in node[\"children\"]\n            # if isinstance(child, dict)\n        ]\n        return 1 + max(child_depths)\n</code></pre>"},{"location":"DirectMultiStep/utils/pre-process/#directmultistep.utils.pre_process.find_leaves","title":"<code>find_leaves(node)</code>","text":"<p>Finds the SMILES strings of all leaf nodes (starting materials) in a route tree.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>FilteredDict</code> <p>A FilteredDict representing a node in the route tree.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of SMILES strings representing the starting materials.</p> Source code in <code>src/directmultistep/utils/pre_process.py</code> <pre><code>def find_leaves(node: FilteredDict) -&gt; list[str]:\n    \"\"\"Finds the SMILES strings of all leaf nodes (starting materials) in a route tree.\n\n    Args:\n        node: A FilteredDict representing a node in the route tree.\n\n    Returns:\n        A list of SMILES strings representing the starting materials.\n    \"\"\"\n    leaves = []\n    if \"children\" in node:\n        for child in node[\"children\"]:\n            leaves.extend(find_leaves(child))\n    else:\n        leaves.append(node[\"smiles\"])\n    return leaves\n</code></pre>"},{"location":"DirectMultiStep/utils/pre-process/#directmultistep.utils.pre_process.canonicalize_smiles","title":"<code>canonicalize_smiles(smiles)</code>","text":"<p>Canonicalizes a SMILES string using RDKit.</p> <p>Parameters:</p> Name Type Description Default <code>smiles</code> <code>str</code> <p>The SMILES string to canonicalize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The canonicalized SMILES string.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the SMILES string cannot be parsed by RDKit.</p> Source code in <code>src/directmultistep/utils/pre_process.py</code> <pre><code>def canonicalize_smiles(smiles: str) -&gt; str:\n    \"\"\"Canonicalizes a SMILES string using RDKit.\n\n    Args:\n        smiles: The SMILES string to canonicalize.\n\n    Returns:\n        The canonicalized SMILES string.\n\n    Raises:\n        ValueError: If the SMILES string cannot be parsed by RDKit.\n    \"\"\"\n    mol = Chem.MolFromSmiles(smiles)\n    if mol is None:\n        raise ValueError(f\"Failed to parse SMILES: {smiles}\")\n    return cast(str, Chem.MolToSmiles(mol))\n</code></pre>"},{"location":"DirectMultiStep/utils/pre-process/#directmultistep.utils.pre_process.stringify_dict","title":"<code>stringify_dict(data)</code>","text":"<p>Converts a FilteredDict to a string, removing spaces.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>FilteredDict</code> <p>The FilteredDict to convert.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string representation of the FilteredDict with no spaces.</p> Source code in <code>src/directmultistep/utils/pre_process.py</code> <pre><code>def stringify_dict(data: FilteredDict) -&gt; str:\n    \"\"\"Converts a FilteredDict to a string, removing spaces.\n\n    Args:\n        data: The FilteredDict to convert.\n\n    Returns:\n        A string representation of the FilteredDict with no spaces.\n    \"\"\"\n    return str(data).replace(\" \", \"\")\n</code></pre>"},{"location":"DirectMultiStep/utils/pre-process/#directmultistep.utils.pre_process.generate_permutations","title":"<code>generate_permutations(data, max_perm=None)</code>","text":"<p>Generates permutations of a synthetic route by permuting the order of children.</p> <p>This function generates all possible permutations of a synthetic route by rearranging the order of child nodes at each level of the tree. It can optionally limit the number of permutations generated.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>FilteredDict</code> <p>A FilteredDict representing the synthetic route.</p> required <code>max_perm</code> <code>int | None</code> <p>An optional integer to limit the number of permutations generated.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of stringified FilteredDicts representing the permuted routes.</p> Source code in <code>src/directmultistep/utils/pre_process.py</code> <pre><code>def generate_permutations(data: FilteredDict, max_perm: int | None = None) -&gt; list[str]:\n    \"\"\"Generates permutations of a synthetic route by permuting the order of children.\n\n    This function generates all possible permutations of a synthetic route by\n    rearranging the order of child nodes at each level of the tree. It can\n    optionally limit the number of permutations generated.\n\n    Args:\n        data: A FilteredDict representing the synthetic route.\n        max_perm: An optional integer to limit the number of permutations generated.\n\n    Returns:\n        A list of stringified FilteredDicts representing the permuted routes.\n    \"\"\"\n    if \"children\" not in data or not data[\"children\"]:\n        return [stringify_dict(data)]\n\n    child_permutations = []\n    for child in data[\"children\"]:\n        child_permutations.append(generate_permutations(child, max_perm))\n\n    all_combos = []\n    # Conditionally apply permutation limit\n    permutation_generator = permutations(range(len(child_permutations)))\n    if max_perm is not None:\n        permutation_generator = islice(permutation_generator, max_perm)  # type:ignore\n\n    for combo in permutation_generator:\n        for product in itertools.product(*(child_permutations[i] for i in combo)):\n            new_data = data.copy()\n            new_data[\"children\"] = [eval(child_str) for child_str in product]\n            all_combos.append(stringify_dict(new_data))\n            if max_perm is not None and len(all_combos) &gt;= max_perm:\n                return all_combos  # Return early if maximum number of permutations is reached\n    return all_combos\n</code></pre>"},{"location":"DirectMultiStep/utils/pre-process/#directmultistep.utils.pre_process.is_convergent","title":"<code>is_convergent(route)</code>","text":"<p>Determines if a synthesis route is convergent (non-linear).</p> <p>A route is linear if for every transformation, at most one reactant has children (i.e., all other reactants are leaf nodes). A route is convergent if there exists at least one transformation where two or more reactants have children.</p> <p>Parameters:</p> Name Type Description Default <code>route</code> <code>FilteredDict</code> <p>The synthesis route to analyze.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the route is convergent (non-linear), False if it's linear.</p> Source code in <code>src/directmultistep/utils/pre_process.py</code> <pre><code>def is_convergent(route: FilteredDict) -&gt; bool:\n    \"\"\"Determines if a synthesis route is convergent (non-linear).\n\n    A route is linear if for every transformation, at most one reactant has children\n    (i.e., all other reactants are leaf nodes). A route is convergent if there exists\n    at least one transformation where two or more reactants have children.\n\n    Args:\n        route: The synthesis route to analyze.\n\n    Returns:\n        True if the route is convergent (non-linear), False if it's linear.\n    \"\"\"\n    if \"children\" not in route:\n        return False\n\n    # Check if current node's transformation has 2 or more children with their own children\n    children = route[\"children\"]\n    if len(children) &gt;= 2:  # Need at least 2 children for a transformation\n        children_with_children = sum(1 for child in children if \"children\" in child)\n        if children_with_children &gt;= 2:\n            return True\n\n    # Recursively check children\n    return any(is_convergent(child) for child in children)\n</code></pre>"},{"location":"DirectMultiStep/utils/torch-dataset/","title":"Torch Dataset for Routes","text":"<p>This module provides a custom PyTorch Dataset class for handling reaction routes. It includes functionalities for tokenizing SMILES strings, reaction paths, and context information, as well as preparing data for training and generation.</p>"},{"location":"DirectMultiStep/utils/torch-dataset/#example-use","title":"Example Use","text":"<p><code>tokenize_path_string</code> is the most important function. It tokenizes a reaction path string. It uses a regular expression to split the string into tokens, and it can optionally add start-of-sequence (<code>&lt;SOS&gt;</code>) and end-of-sequence (<code>?</code>) tokens.</p> <pre><code>from directmultistep.utils.dataset import tokenize_path_string\n\npath_string = \"{'smiles':'CC','children':[{'smiles':'CC(=O)O'}]}\"\ntokens = tokenize_path_string(path_string)\nprint(tokens)\n</code></pre>"},{"location":"DirectMultiStep/utils/torch-dataset/#notes-on-path-start","title":"Notes on Path Start","text":"<p>In the <code>RoutesDataset</code> class, the <code>get_generation_with_sm</code> and <code>get_generation_no_sm</code> methods return an initial path tensor. This tensor is created from a <code>path_start</code> string, which is a partial path string that the model will start generating from. The <code>path_start</code> is <code>\"{'smiles': 'product_smiles', 'children': [{'smiles':\"</code>. The model will generate the rest of the path string from this starting point.</p> <p>This design is important because a trained model always generates this <code>path_start</code> at the beginning of the sequence. By providing this as the initial input, we avoid wasting time generating this part and can focus on generating the rest of the reaction path.</p> <p>The <code>prepare_input_tensors</code> function in <code>directmultistep.generate</code> allows for the provision of a custom <code>path_start</code> string. This is useful when you want to initiate the generation process from a specific point in the reaction path, instead of the default starting point. By modifying the <code>path_start</code> argument, you can control the initial state of the generation and explore different reaction pathways with user-defined intermediates.</p>"},{"location":"DirectMultiStep/utils/torch-dataset/#source-code","title":"Source Code","text":""},{"location":"DirectMultiStep/utils/torch-dataset/#directmultistep.utils.dataset","title":"<code>directmultistep.utils.dataset</code>","text":""},{"location":"DirectMultiStep/utils/torch-dataset/#directmultistep.utils.dataset.RoutesDataset","title":"<code>RoutesDataset</code>","text":"<p>               Bases: <code>RoutesProcessing</code></p> <p>Dataset for multi-step reaction routes.</p> Source code in <code>src/directmultistep/utils/dataset.py</code> <pre><code>class RoutesDataset(RoutesProcessing):\n    \"\"\"Dataset for multi-step reaction routes.\"\"\"\n\n    def __init__(\n        self,\n        metadata_path: Path,\n        products: list[str],\n        path_strings: list[str],\n        n_steps_list: list[int],\n        starting_materials: list[str] | None = None,\n        mode: str = \"training\",\n        name_idx: dict[str, list[int]] | None = None,\n    ) -&gt; None:\n        \"\"\"Initializes the RoutesDataset.\n\n        Args:\n            metadata_path: Path to the metadata file (YAML).\n            products: List of product SMILES strings.\n            path_strings: List of reaction path strings.\n            n_steps_list: List of integers representing the number of steps in each path.\n            starting_materials: List of starting material SMILES strings.\n            mode: Either \"training\" or \"generation\".\n            name_idx: A dictionary mapping names to lists of indices.\n        \"\"\"\n        super().__init__(metadata_path)\n        self.products = products\n        self.path_strings = path_strings\n        self.step_lengths = n_steps_list\n        self.sms = starting_materials\n        # name_idx is an optional attribute that shows labels for items in the dataset\n        # currently used for evals on pharma compounds\n        self.name_idx = name_idx\n        assert mode in [\"training\", \"generation\"], \"mode must be either 'training' or 'generation'\"\n        self.mode = mode\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Returns a string representation of the dataset.\"\"\"\n        sms_str = \"SM (enabled)\" if self.sms is not None else \"SM (disabled)\"\n        return f\"RoutesDataset(mode={self.mode}, len={len(self)}, {sms_str})\"\n\n    def __getitem__(self, index: int) -&gt; tuple[Tensor, ...]:\n        \"\"\"Retrieves an item from the dataset.\n\n        Args:\n            index: The index of the item to retrieve.\n\n        Returns:\n            A tuple of tensors representing the input and output data.\n        \"\"\"\n        if self.mode == \"training\":\n            if self.sms is not None:\n                return self.get_training_with_sm(index)\n            else:\n                return self.get_training_no_sm(index)\n        elif self.mode == \"generation\":\n            if self.sms is not None:\n                return self.get_generation_with_sm(index)\n            else:\n                return self.get_generation_no_sm(index)\n        else:\n            raise ValueError(f\"Invalid mode: {self.mode}\")\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the number of items in the dataset.\"\"\"\n        return len(self.products)\n\n    def get_training_with_sm(self, index: int) -&gt; tuple[Tensor, ...]:\n        \"\"\"Retrieves a training item with starting materials.\n\n        Args:\n            index: The index of the item to retrieve.\n\n        Returns:\n            A tuple of tensors: encoder input, decoder input, and step length.\n        \"\"\"\n        assert self.sms is not None, \"starting materials are not provided\"\n        product_item = self.smile_to_tokens(self.products[index], self.product_max_length)\n        one_sm_item = self.smile_to_tokens(self.sms[index], self.sm_max_length)\n        seq_encoder_item = torch.cat((product_item, one_sm_item), dim=0)\n        seq_decoder_item = self.path_string_to_tokens(self.path_strings[index], self.seq_out_max_length)\n\n        step_item = Tensor([self.step_lengths[index]])\n        return seq_encoder_item, seq_decoder_item, step_item\n\n    def get_generation_with_sm(self, index: int) -&gt; tuple[Tensor, ...]:\n        \"\"\"Retrieves a generation item with starting materials.\n\n        Args:\n            index: The index of the item to retrieve.\n\n        Returns:\n            A tuple of tensors: encoder input, step length, and initial path tensor.\n        \"\"\"\n        assert self.sms is not None, \"starting materials are not provided\"\n        product_item = self.smile_to_tokens(self.products[index], self.product_max_length)\n        one_sm_item = self.smile_to_tokens(self.sms[index], self.sm_max_length)\n        seq_encoder_item = torch.cat((product_item, one_sm_item), dim=0)\n\n        step_item = Tensor([self.step_lengths[index]])\n        smile_dict = {\"smiles\": self.products[index], \"children\": [{\"smiles\": \"\"}]}\n        path_start = str(smile_dict).replace(\" \", \"\")[:-4]\n        path_tens = self.path_string_to_tokens(path_start, max_length=None, add_eos=False)\n        return seq_encoder_item, step_item, path_tens\n\n    def get_training_no_sm(self, index: int) -&gt; tuple[Tensor, ...]:\n        \"\"\"Retrieves a training item without starting materials.\n\n        Args:\n            index: The index of the item to retrieve.\n\n        Returns:\n            A tuple of tensors: encoder input, decoder input, and step length.\n        \"\"\"\n        seq_encoder_item = self.smile_to_tokens(self.products[index], self.product_max_length)\n        seq_decoder_item = self.path_string_to_tokens(self.path_strings[index], self.seq_out_max_length)\n\n        step_item = Tensor([self.step_lengths[index]])\n        # shapes: [product_max_length], [output_max_length], int\n        return seq_encoder_item, seq_decoder_item, step_item\n\n    def get_generation_no_sm(self, index: int) -&gt; tuple[Tensor, ...]:\n        \"\"\"Retrieves a generation item without starting materials.\n\n        Args:\n            index: The index of the item to retrieve.\n\n        Returns:\n            A tuple of tensors: encoder input, step length, and initial path tensor.\n        \"\"\"\n        seq_encoder_item = self.smile_to_tokens(self.products[index], self.product_max_length)\n\n        step_item = Tensor([self.step_lengths[index]])\n        # shapes: [product_max_length], [output_max_length], int\n        smile_dict = {\"smiles\": self.products[index], \"children\": [{\"smiles\": \"\"}]}\n        path_start = str(smile_dict).replace(\" \", \"\")[:-4]\n        # path_start = \"{'smiles':'\" + self.products[index] + \"','children':[\"\n        path_tens = self.path_string_to_tokens(path_start, max_length=None, add_eos=False)\n        return seq_encoder_item, step_item, path_tens\n</code></pre>"},{"location":"DirectMultiStep/utils/torch-dataset/#directmultistep.utils.dataset.RoutesDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Retrieves an item from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to retrieve.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, ...]</code> <p>A tuple of tensors representing the input and output data.</p> Source code in <code>src/directmultistep/utils/dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; tuple[Tensor, ...]:\n    \"\"\"Retrieves an item from the dataset.\n\n    Args:\n        index: The index of the item to retrieve.\n\n    Returns:\n        A tuple of tensors representing the input and output data.\n    \"\"\"\n    if self.mode == \"training\":\n        if self.sms is not None:\n            return self.get_training_with_sm(index)\n        else:\n            return self.get_training_no_sm(index)\n    elif self.mode == \"generation\":\n        if self.sms is not None:\n            return self.get_generation_with_sm(index)\n        else:\n            return self.get_generation_no_sm(index)\n    else:\n        raise ValueError(f\"Invalid mode: {self.mode}\")\n</code></pre>"},{"location":"DirectMultiStep/utils/torch-dataset/#directmultistep.utils.dataset.RoutesDataset.__init__","title":"<code>__init__(metadata_path, products, path_strings, n_steps_list, starting_materials=None, mode='training', name_idx=None)</code>","text":"<p>Initializes the RoutesDataset.</p> <p>Parameters:</p> Name Type Description Default <code>metadata_path</code> <code>Path</code> <p>Path to the metadata file (YAML).</p> required <code>products</code> <code>list[str]</code> <p>List of product SMILES strings.</p> required <code>path_strings</code> <code>list[str]</code> <p>List of reaction path strings.</p> required <code>n_steps_list</code> <code>list[int]</code> <p>List of integers representing the number of steps in each path.</p> required <code>starting_materials</code> <code>list[str] | None</code> <p>List of starting material SMILES strings.</p> <code>None</code> <code>mode</code> <code>str</code> <p>Either \"training\" or \"generation\".</p> <code>'training'</code> <code>name_idx</code> <code>dict[str, list[int]] | None</code> <p>A dictionary mapping names to lists of indices.</p> <code>None</code> Source code in <code>src/directmultistep/utils/dataset.py</code> <pre><code>def __init__(\n    self,\n    metadata_path: Path,\n    products: list[str],\n    path_strings: list[str],\n    n_steps_list: list[int],\n    starting_materials: list[str] | None = None,\n    mode: str = \"training\",\n    name_idx: dict[str, list[int]] | None = None,\n) -&gt; None:\n    \"\"\"Initializes the RoutesDataset.\n\n    Args:\n        metadata_path: Path to the metadata file (YAML).\n        products: List of product SMILES strings.\n        path_strings: List of reaction path strings.\n        n_steps_list: List of integers representing the number of steps in each path.\n        starting_materials: List of starting material SMILES strings.\n        mode: Either \"training\" or \"generation\".\n        name_idx: A dictionary mapping names to lists of indices.\n    \"\"\"\n    super().__init__(metadata_path)\n    self.products = products\n    self.path_strings = path_strings\n    self.step_lengths = n_steps_list\n    self.sms = starting_materials\n    # name_idx is an optional attribute that shows labels for items in the dataset\n    # currently used for evals on pharma compounds\n    self.name_idx = name_idx\n    assert mode in [\"training\", \"generation\"], \"mode must be either 'training' or 'generation'\"\n    self.mode = mode\n</code></pre>"},{"location":"DirectMultiStep/utils/torch-dataset/#directmultistep.utils.dataset.RoutesDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of items in the dataset.</p> Source code in <code>src/directmultistep/utils/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of items in the dataset.\"\"\"\n    return len(self.products)\n</code></pre>"},{"location":"DirectMultiStep/utils/torch-dataset/#directmultistep.utils.dataset.RoutesDataset.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns a string representation of the dataset.</p> Source code in <code>src/directmultistep/utils/dataset.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Returns a string representation of the dataset.\"\"\"\n    sms_str = \"SM (enabled)\" if self.sms is not None else \"SM (disabled)\"\n    return f\"RoutesDataset(mode={self.mode}, len={len(self)}, {sms_str})\"\n</code></pre>"},{"location":"DirectMultiStep/utils/torch-dataset/#directmultistep.utils.dataset.RoutesDataset.get_generation_no_sm","title":"<code>get_generation_no_sm(index)</code>","text":"<p>Retrieves a generation item without starting materials.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to retrieve.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, ...]</code> <p>A tuple of tensors: encoder input, step length, and initial path tensor.</p> Source code in <code>src/directmultistep/utils/dataset.py</code> <pre><code>def get_generation_no_sm(self, index: int) -&gt; tuple[Tensor, ...]:\n    \"\"\"Retrieves a generation item without starting materials.\n\n    Args:\n        index: The index of the item to retrieve.\n\n    Returns:\n        A tuple of tensors: encoder input, step length, and initial path tensor.\n    \"\"\"\n    seq_encoder_item = self.smile_to_tokens(self.products[index], self.product_max_length)\n\n    step_item = Tensor([self.step_lengths[index]])\n    # shapes: [product_max_length], [output_max_length], int\n    smile_dict = {\"smiles\": self.products[index], \"children\": [{\"smiles\": \"\"}]}\n    path_start = str(smile_dict).replace(\" \", \"\")[:-4]\n    # path_start = \"{'smiles':'\" + self.products[index] + \"','children':[\"\n    path_tens = self.path_string_to_tokens(path_start, max_length=None, add_eos=False)\n    return seq_encoder_item, step_item, path_tens\n</code></pre>"},{"location":"DirectMultiStep/utils/torch-dataset/#directmultistep.utils.dataset.RoutesDataset.get_generation_with_sm","title":"<code>get_generation_with_sm(index)</code>","text":"<p>Retrieves a generation item with starting materials.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to retrieve.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, ...]</code> <p>A tuple of tensors: encoder input, step length, and initial path tensor.</p> Source code in <code>src/directmultistep/utils/dataset.py</code> <pre><code>def get_generation_with_sm(self, index: int) -&gt; tuple[Tensor, ...]:\n    \"\"\"Retrieves a generation item with starting materials.\n\n    Args:\n        index: The index of the item to retrieve.\n\n    Returns:\n        A tuple of tensors: encoder input, step length, and initial path tensor.\n    \"\"\"\n    assert self.sms is not None, \"starting materials are not provided\"\n    product_item = self.smile_to_tokens(self.products[index], self.product_max_length)\n    one_sm_item = self.smile_to_tokens(self.sms[index], self.sm_max_length)\n    seq_encoder_item = torch.cat((product_item, one_sm_item), dim=0)\n\n    step_item = Tensor([self.step_lengths[index]])\n    smile_dict = {\"smiles\": self.products[index], \"children\": [{\"smiles\": \"\"}]}\n    path_start = str(smile_dict).replace(\" \", \"\")[:-4]\n    path_tens = self.path_string_to_tokens(path_start, max_length=None, add_eos=False)\n    return seq_encoder_item, step_item, path_tens\n</code></pre>"},{"location":"DirectMultiStep/utils/torch-dataset/#directmultistep.utils.dataset.RoutesDataset.get_training_no_sm","title":"<code>get_training_no_sm(index)</code>","text":"<p>Retrieves a training item without starting materials.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to retrieve.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, ...]</code> <p>A tuple of tensors: encoder input, decoder input, and step length.</p> Source code in <code>src/directmultistep/utils/dataset.py</code> <pre><code>def get_training_no_sm(self, index: int) -&gt; tuple[Tensor, ...]:\n    \"\"\"Retrieves a training item without starting materials.\n\n    Args:\n        index: The index of the item to retrieve.\n\n    Returns:\n        A tuple of tensors: encoder input, decoder input, and step length.\n    \"\"\"\n    seq_encoder_item = self.smile_to_tokens(self.products[index], self.product_max_length)\n    seq_decoder_item = self.path_string_to_tokens(self.path_strings[index], self.seq_out_max_length)\n\n    step_item = Tensor([self.step_lengths[index]])\n    # shapes: [product_max_length], [output_max_length], int\n    return seq_encoder_item, seq_decoder_item, step_item\n</code></pre>"},{"location":"DirectMultiStep/utils/torch-dataset/#directmultistep.utils.dataset.RoutesDataset.get_training_with_sm","title":"<code>get_training_with_sm(index)</code>","text":"<p>Retrieves a training item with starting materials.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to retrieve.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, ...]</code> <p>A tuple of tensors: encoder input, decoder input, and step length.</p> Source code in <code>src/directmultistep/utils/dataset.py</code> <pre><code>def get_training_with_sm(self, index: int) -&gt; tuple[Tensor, ...]:\n    \"\"\"Retrieves a training item with starting materials.\n\n    Args:\n        index: The index of the item to retrieve.\n\n    Returns:\n        A tuple of tensors: encoder input, decoder input, and step length.\n    \"\"\"\n    assert self.sms is not None, \"starting materials are not provided\"\n    product_item = self.smile_to_tokens(self.products[index], self.product_max_length)\n    one_sm_item = self.smile_to_tokens(self.sms[index], self.sm_max_length)\n    seq_encoder_item = torch.cat((product_item, one_sm_item), dim=0)\n    seq_decoder_item = self.path_string_to_tokens(self.path_strings[index], self.seq_out_max_length)\n\n    step_item = Tensor([self.step_lengths[index]])\n    return seq_encoder_item, seq_decoder_item, step_item\n</code></pre>"},{"location":"DirectMultiStep/utils/torch-dataset/#directmultistep.utils.dataset.tokenize_smile","title":"<code>tokenize_smile(smile)</code>","text":"<p>Tokenizes a SMILES string by character.</p> <p>Parameters:</p> Name Type Description Default <code>smile</code> <code>str</code> <p>The SMILES string to tokenize.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of tokens, including start and end of sequence tokens.</p> Source code in <code>src/directmultistep/utils/dataset.py</code> <pre><code>def tokenize_smile(smile: str) -&gt; list[str]:\n    \"\"\"Tokenizes a SMILES string by character.\n\n    Args:\n        smile: The SMILES string to tokenize.\n\n    Returns:\n        A list of tokens, including start and end of sequence tokens.\n    \"\"\"\n    return [\"&lt;SOS&gt;\"] + list(smile) + [\"?\"]\n</code></pre>"},{"location":"DirectMultiStep/utils/torch-dataset/#directmultistep.utils.dataset.tokenize_smile_atom","title":"<code>tokenize_smile_atom(smile, has_atom_types, mask=False)</code>","text":"<p>Tokenizes a SMILES string, considering atom types of up to two characters.</p> <p>Parameters:</p> Name Type Description Default <code>smile</code> <code>str</code> <p>The SMILES string to tokenize.</p> required <code>has_atom_types</code> <code>list[str]</code> <p>A list of atom types to consider (e.g., [\"Cl\", \"Br\"]).</p> required <code>mask</code> <code>bool</code> <p>If True, replaces all atom tokens with \"J\".</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of tokens, including start and end of sequence tokens.</p> Source code in <code>src/directmultistep/utils/dataset.py</code> <pre><code>def tokenize_smile_atom(smile: str, has_atom_types: list[str], mask: bool = False) -&gt; list[str]:\n    \"\"\"Tokenizes a SMILES string, considering atom types of up to two characters.\n\n    Args:\n        smile: The SMILES string to tokenize.\n        has_atom_types: A list of atom types to consider (e.g., [\"Cl\", \"Br\"]).\n        mask: If True, replaces all atom tokens with \"J\".\n\n    Returns:\n        A list of tokens, including start and end of sequence tokens.\n    \"\"\"\n    tokens = []\n    i = 0\n    while i &lt; len(smile):\n        if i &lt; len(smile) - 1 and smile[i : i + 2] in has_atom_types:\n            tokens.append(\"J\" if mask else smile[i : i + 2])\n            i += 2\n        else:\n            tokens.append(\"J\" if mask else smile[i])\n            i += 1\n    return [\"&lt;SOS&gt;\"] + tokens + [\"?\"]\n</code></pre>"},{"location":"DirectMultiStep/utils/torch-dataset/#directmultistep.utils.dataset.tokenize_context","title":"<code>tokenize_context(context_list)</code>","text":"<p>Tokenizes a list of context strings.</p> <p>Parameters:</p> Name Type Description Default <code>context_list</code> <code>list[str]</code> <p>A list of context strings to tokenize.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of tokens, including context start, separator, and end tokens.</p> Source code in <code>src/directmultistep/utils/dataset.py</code> <pre><code>def tokenize_context(context_list: list[str]) -&gt; list[str]:\n    \"\"\"Tokenizes a list of context strings.\n\n    Args:\n        context_list: A list of context strings to tokenize.\n\n    Returns:\n        A list of tokens, including context start, separator, and end tokens.\n    \"\"\"\n    tokens = [\"&lt;context&gt;\"]\n    for context in context_list:\n        tokens.extend(tokenize_path_string(context, add_sos=False, add_eos=False))\n        tokens.append(\"&lt;sep&gt;\")\n    tokens.append(\"&lt;/context&gt;\")\n    return tokens\n</code></pre>"},{"location":"DirectMultiStep/utils/torch-dataset/#directmultistep.utils.dataset.tokenize_path_string","title":"<code>tokenize_path_string(path_string, add_sos=True, add_eos=True)</code>","text":"<p>Tokenizes a path string based on a regular expression.</p> <p>Parameters:</p> Name Type Description Default <code>path_string</code> <code>str</code> <p>The path string to tokenize.</p> required <code>add_sos</code> <code>bool</code> <p>If True, adds a start of sequence token.</p> <code>True</code> <code>add_eos</code> <code>bool</code> <p>If True, adds an end of sequence token.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of tokens.</p> Source code in <code>src/directmultistep/utils/dataset.py</code> <pre><code>def tokenize_path_string(path_string: str, add_sos: bool = True, add_eos: bool = True) -&gt; list[str]:\n    \"\"\"Tokenizes a path string based on a regular expression.\n\n    Args:\n        path_string: The path string to tokenize.\n        add_sos: If True, adds a start of sequence token.\n        add_eos: If True, adds an end of sequence token.\n\n    Returns:\n        A list of tokens.\n    \"\"\"\n    pattern = re.compile(r\"('smiles':|'children':|\\[|\\]|{|}|.)\")\n    tokens = [\"&lt;SOS&gt;\"] if add_sos else []\n    tokens.extend(pattern.findall(path_string))\n    if add_eos:\n        tokens.append(\"?\")\n    return tokens\n</code></pre>"},{"location":"DirectMultiStep/utils/torch-dataset/#directmultistep.generate","title":"<code>directmultistep.generate</code>","text":""},{"location":"DirectMultiStep/utils/torch-dataset/#directmultistep.generate.prepare_input_tensors","title":"<code>prepare_input_tensors(target, n_steps, starting_material, rds, product_max_length, sm_max_length, use_fp16=False)</code>","text":"<p>Prepare input tensors for the model. Args:     target: SMILES string of the target molecule.     n_steps: Number of synthesis steps.     starting_material: SMILES string of the starting material, if any.     rds: RoutesProcessing object for tokenization.     product_max_length: Maximum length of the product SMILES sequence.     sm_max_length: Maximum length of the starting material SMILES sequence.     use_fp16: Whether to use half precision (FP16) for tensors.     path_start: Initial path string to start generation from. Returns:     A tuple containing:         - encoder_inp: Input tensor for the encoder.         - steps_tens: Tensor of the number of steps, or None if not provided.         - path_tens: Initial path tensor for the decoder.</p> Source code in <code>src/directmultistep/generate.py</code> <pre><code>def prepare_input_tensors(\n    target: str,\n    n_steps: int | None,\n    starting_material: str | None,\n    rds: RoutesProcessing,\n    product_max_length: int,\n    sm_max_length: int,\n    use_fp16: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None, torch.Tensor]:\n    \"\"\"Prepare input tensors for the model.\n    Args:\n        target: SMILES string of the target molecule.\n        n_steps: Number of synthesis steps.\n        starting_material: SMILES string of the starting material, if any.\n        rds: RoutesProcessing object for tokenization.\n        product_max_length: Maximum length of the product SMILES sequence.\n        sm_max_length: Maximum length of the starting material SMILES sequence.\n        use_fp16: Whether to use half precision (FP16) for tensors.\n        path_start: Initial path string to start generation from.\n    Returns:\n        A tuple containing:\n            - encoder_inp: Input tensor for the encoder.\n            - steps_tens: Tensor of the number of steps, or None if not provided.\n            - path_tens: Initial path tensor for the decoder.\n    \"\"\"\n    prod_tens = rds.smile_to_tokens(target, product_max_length)\n    if starting_material:\n        sm_tens = rds.smile_to_tokens(starting_material, sm_max_length)\n        encoder_inp = torch.cat([prod_tens, sm_tens], dim=0).unsqueeze(0)\n    else:\n        encoder_inp = torch.cat([prod_tens], dim=0).unsqueeze(0)\n\n    steps_tens = torch.tensor([n_steps]).unsqueeze(0) if n_steps is not None else None\n    path_start = \"{'smiles':'\" + target + \"','children':[{'smiles':'\"\n    path_tens = rds.path_string_to_tokens(path_start, max_length=None, add_eos=False).unsqueeze(0)\n\n    if use_fp16:\n        encoder_inp = encoder_inp.half()\n        if steps_tens is not None:\n            steps_tens = steps_tens.half()\n        path_tens = path_tens.half()\n\n    return encoder_inp, steps_tens, path_tens\n</code></pre>"},{"location":"analysis/monitoring-training/","title":"Monitoring Training","text":"<p>This guide explains how to monitor and visualize training progress for DMS models.</p>"},{"location":"analysis/monitoring-training/#basic-usage","title":"Basic Usage","text":"<p>The simplest way to visualize training progress is using the provided plotting utilities in <code>use-examples/visualize_train_curves.py</code></p>"},{"location":"analysis/monitoring-training/#run-configuration","title":"Run Configuration","text":"<p>Use <code>RunConfig</code> to specify which training runs to visualize:</p> <pre><code>from directmultistep.analysis.training import RunConfig\n\nrun = RunConfig(\n    run_name=\"flash_10M\",      # Folder name of the run\n    trace_name=\"Flash Model\",  # Display name for the traces\n    include_val=True          # Whether to include validation curve\n)\n</code></pre>"},{"location":"analysis/monitoring-training/#training-curves","title":"Training Curves","text":"<p>The <code>plot_training_curves</code> function creates a figure showing:</p> <ul> <li>Training loss curves (solid lines)</li> <li>Validation loss curves (dotted lines with markers)</li> <li>X-axis shows number of processed tokens</li> <li>Hovering over validation points shows epoch information</li> </ul>"},{"location":"analysis/monitoring-training/#learning-rate-curves","title":"Learning Rate Curves","text":"<p>The <code>plot_learning_rates</code> function visualizes the learning rate schedule:</p> <ul> <li>Shows learning rate vs. training step</li> <li>Useful for verifying learning rate schedules</li> <li>Multiple runs can be compared on the same plot</li> </ul>"},{"location":"analysis/monitoring-training/#advanced-usage","title":"Advanced Usage","text":"<p>For more control over visualization, you can load the training data directly:</p> <pre><code>from directmultistep.analysis.training import load_training_df\n\n# Load training data\ndf = load_training_df(train_path, \"flash_10M\")\n\n# Ignore specific training runs by ID\ndf = load_training_df(train_path, \"flash_10M\", ignore_ids=[0, 1])\n</code></pre> <p>The returned DataFrame contains columns:</p> <ul> <li><code>processed_tokens</code>: Number of tokens processed</li> <li><code>train_loss</code>: Training loss</li> <li><code>val_loss</code>: Validation loss (if available)</li> <li><code>train_lr</code>: Learning rate</li> <li><code>epoch</code>: Current epoch</li> <li>Additional metrics depending on the training configuration</li> </ul>"},{"location":"analysis/monitoring-training/#source-code","title":"Source Code","text":""},{"location":"analysis/monitoring-training/#directmultistep.analysis.training","title":"<code>directmultistep.analysis.training</code>","text":""},{"location":"analysis/monitoring-training/#directmultistep.analysis.training.RunConfig","title":"<code>RunConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a training run visualization.</p> Source code in <code>src/directmultistep/analysis/training.py</code> <pre><code>@dataclass\nclass RunConfig:\n    \"\"\"Configuration for a training run visualization.\"\"\"\n\n    run_name: str  # Folder name of the run\n    trace_name: str  # Display name for the traces\n    include_val: bool = True  # Whether to include validation curve\n    ignore_ids: list[int] | None = None  # Version IDs to ignore when loading data\n</code></pre>"},{"location":"analysis/monitoring-training/#directmultistep.analysis.training.plot_training_curves","title":"<code>plot_training_curves(train_path, runs, x_axis='processed_tokens')</code>","text":"<p>Create a figure showing training and validation curves for multiple runs.</p> <p>Parameters:</p> Name Type Description Default <code>train_path</code> <code>Path</code> <p>Path to training data directory</p> required <code>runs</code> <code>list[RunConfig]</code> <p>List of run configurations specifying what and how to plot</p> required <code>x_axis</code> <code>str</code> <p>Column to use for x-axis values (\"processed_tokens\", \"epoch\", or \"step\")</p> <code>'processed_tokens'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure with training and validation curves</p> Source code in <code>src/directmultistep/analysis/training.py</code> <pre><code>def plot_training_curves(\n    train_path: Path,\n    runs: list[RunConfig],\n    x_axis: str = \"processed_tokens\",\n) -&gt; go.Figure:\n    \"\"\"Create a figure showing training and validation curves for multiple runs.\n\n    Args:\n        train_path: Path to training data directory\n        runs: List of run configurations specifying what and how to plot\n        x_axis: Column to use for x-axis values (\"processed_tokens\", \"epoch\", or \"step\")\n\n    Returns:\n        Plotly figure with training and validation curves\n    \"\"\"\n    traces = []\n    for i, run in enumerate(runs):\n        df = load_training_df(train_path, run.run_name, run.ignore_ids)\n        color_idx = i % len(style.colors_light)\n        traces.append(\n            create_train_trace(df, run.trace_name, style.colors_light[color_idx % len(style.colors_light)], x_axis)\n        )\n        if run.include_val:\n            traces.append(\n                create_val_trace(df, run.trace_name, style.colors_dark[color_idx % len(style.colors_dark)], x_axis)\n            )\n\n    fig = go.Figure(data=traces)\n\n    fig.update_layout(\n        title=\"Training Loss\",\n        xaxis_title=x_axis,\n        yaxis_title=\"Loss\",\n        width=1000,\n    )\n    style.apply_development_style(fig)\n\n    return fig\n</code></pre>"},{"location":"analysis/monitoring-training/#directmultistep.analysis.training.plot_learning_rates","title":"<code>plot_learning_rates(train_path, runs)</code>","text":"<p>Create a figure showing learning rate curves for multiple runs.</p> <p>Parameters:</p> Name Type Description Default <code>train_path</code> <code>Path</code> <p>Path to training data directory</p> required <code>runs</code> <code>list[RunConfig]</code> <p>List of run configurations specifying what and how to plot</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure with learning rate curves</p> Source code in <code>src/directmultistep/analysis/training.py</code> <pre><code>def plot_learning_rates(\n    train_path: Path,\n    runs: list[RunConfig],\n) -&gt; go.Figure:\n    \"\"\"Create a figure showing learning rate curves for multiple runs.\n\n    Args:\n        train_path: Path to training data directory\n        runs: List of run configurations specifying what and how to plot\n\n    Returns:\n        Plotly figure with learning rate curves\n    \"\"\"\n    traces = []\n    for run in runs:\n        df = load_training_df(train_path, run.run_name, run.ignore_ids)\n        traces.append(get_lr_trace(df, run.trace_name))\n\n    fig = go.Figure(data=traces)\n\n    fig.update_layout(\n        title=\"Learning Rate\",\n        xaxis_title=\"Step\",\n        yaxis_title=\"Learning Rate\",\n        width=800,\n    )\n    style.apply_development_style(fig)\n\n    return fig\n</code></pre>"},{"location":"analysis/monitoring-training/#directmultistep.analysis.training.load_training_df","title":"<code>load_training_df(train_path, run_name, ignore_ids=None)</code>","text":"Source code in <code>src/directmultistep/analysis/training.py</code> <pre><code>def load_training_df(train_path: Path, run_name: str, ignore_ids: list[int] | None = None) -&gt; pd.DataFrame:\n    logger.debug(f\"Loading {run_name=}\")\n    log_path = train_path / run_name / \"lightning_logs\"\n    dfs = []\n    versions = [log.name for log in log_path.glob(\"version_*\")]\n    logger.debug(f\"Found versions: {versions} for {run_name}\")\n    if ignore_ids is not None:\n        ignored_folders = {f\"version_{i}\" for i in ignore_ids}\n    else:\n        ignored_folders = set()\n    for version in sorted(versions, key=lambda x: int(x.split(\"_\")[1])):\n        if version in ignored_folders:\n            continue\n        temp_df = pd.read_csv(log_path / version / \"metrics.csv\")\n        logger.debug(f\"Loaded df with shape {temp_df.shape}\")\n        dfs.append(temp_df)\n    df = pd.concat(dfs)\n    df = df.reset_index(drop=True)\n    return df\n</code></pre>"},{"location":"analysis/paper-figures/","title":"Paper Figures","text":"<p>This document describes the figures that can be generated using the <code>paper-figures.py</code> script.</p>"},{"location":"analysis/paper-figures/#available-figures","title":"Available Figures","text":""},{"location":"analysis/paper-figures/#1-route-length-distribution","title":"1. Route Length Distribution","text":"<ul> <li>File: <code>route_length_distribution.{pdf,html}</code></li> <li>Description: Visualizes the distribution of route lengths across different datasets (training, n1, and n5 datasets).</li> <li>Generated by: <code>plot_route_length_distribution()</code></li> </ul>"},{"location":"analysis/paper-figures/#2-leaf-distribution","title":"2. Leaf Distribution","text":"<ul> <li>File: <code>leaf_distribution.{pdf,html}</code></li> <li>Description: Shows the distribution of leaf nodes (end states) across different datasets.</li> <li>Generated by: <code>plot_leaf_distribution()</code></li> </ul>"},{"location":"analysis/paper-figures/#3-convergent-route-analysis","title":"3. Convergent Route Analysis","text":"<p>Two figures are generated for convergent route analysis:</p> <ul> <li>Files:<ul> <li><code>convergent_fraction_by_length.{pdf,html}</code></li> <li><code>convergent_fraction_overall.{pdf,html}</code></li> </ul> </li> <li>Description: Analyzes the fraction of convergent routes by length and overall convergent fraction across datasets.</li> <li>Generated by: <code>plot_convergent_fraction_by_length()</code> and <code>plot_convergent_fraction_overall()</code></li> </ul>"},{"location":"analysis/paper-figures/#4-top-k-accuracy-analysis","title":"4. Top-K Accuracy Analysis","text":"<ul> <li>File: <code>{dataset_name}_topk_accuracy_subplots.{pdf,html}</code></li> <li>Description: Comparative bar plots showing top-k accuracy metrics for different models and configurations.</li> <li>Features: Shows accuracy for k values [1, 2, 3, 4, 5, 10]</li> <li>Generated separately for n1 and n5 datasets</li> </ul>"},{"location":"analysis/paper-figures/#5-route-processing-stages","title":"5. Route Processing Stages","text":"<ul> <li>File: <code>{dataset_name}_route_processing_stages_{config}.{pdf,html}</code></li> <li>Description: Visualizes different stages of route processing, comparing:<ul> <li>Valid routes</li> <li>Processed routes without stock</li> <li>Processed routes with stock</li> <li>True routes</li> </ul> </li> </ul>"},{"location":"analysis/paper-figures/#6-accuracy-by-route-length","title":"6. Accuracy by Route Length","text":"<ul> <li>File: <code>accuracy_by_length_subplots_{config}.{pdf,html}</code></li> <li>Description: Shows top-k accuracy metrics broken down by route length</li> <li>Features: <ul> <li>Compares performance across different datasets (n1, n5)</li> <li>Shows accuracy for k=1 and k=10</li> </ul> </li> </ul>"},{"location":"analysis/paper-figures/#usage","title":"Usage","text":"<p>To generate these figures, modify the <code>rerun</code> dictionary in <code>paper-figures.py</code> to specify which figures you want to generate:</p> <pre><code>rerun = {\n    \"route-distribution\": False,\n    \"leaf-distribution\": False,\n    \"convergent-fraction\": False,\n    \"topk-accuracy\": False,\n    \"extraction-distribution\": True,\n    \"accuracy-by-length\": False,\n}\n</code></pre> <p>Set the corresponding flag to <code>True</code> for the figures you want to generate. All figures will be saved in both PDF and HTML formats in the <code>data/figures/paper</code> directory.</p>"},{"location":"analysis/paper-figures/#source-code","title":"Source Code","text":""},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.dataset_analysis","title":"<code>directmultistep.analysis.paper.dataset_analysis</code>","text":""},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.dataset_analysis.create_convergent_fraction_trace","title":"<code>create_convergent_fraction_trace(path_strings, route_lengths, label, color)</code>","text":"<p>Create a bar trace showing fraction of convergent routes by length.</p> <p>Parameters:</p> Name Type Description Default <code>path_strings</code> <code>list[str]</code> <p>List of path strings to analyze</p> required <code>route_lengths</code> <code>list[int]</code> <p>List of corresponding route lengths</p> required <code>label</code> <code>str</code> <p>Label for the trace</p> required <code>color</code> <code>str</code> <p>Color for the trace</p> required <p>Returns:</p> Type Description <code>Bar</code> <p>Bar trace showing convergent fraction by length</p> Source code in <code>src/directmultistep/analysis/paper/dataset_analysis.py</code> <pre><code>def create_convergent_fraction_trace(\n    path_strings: list[str], route_lengths: list[int], label: str, color: str\n) -&gt; go.Bar:\n    \"\"\"Create a bar trace showing fraction of convergent routes by length.\n\n    Args:\n        path_strings: List of path strings to analyze\n        route_lengths: List of corresponding route lengths\n        label: Label for the trace\n        color: Color for the trace\n\n    Returns:\n        Bar trace showing convergent fraction by length\n    \"\"\"\n    # Group paths by length and compute convergent fraction\n    max_length = 10\n    fractions = []\n    lengths = []\n\n    for length in tqdm(range(1, max_length + 1)):\n        # Get paths of this length\n        mask = np.array(route_lengths) == length\n        paths_at_length = np.array(path_strings)[mask]\n\n        if len(paths_at_length) == 0:\n            continue\n\n        # Compute fraction of convergent paths\n        n_convergent = sum(1 for path in paths_at_length if is_convergent(eval(path)))\n        fraction = n_convergent / len(paths_at_length)\n\n        fractions.append(fraction)\n        lengths.append(length)\n\n    return go.Bar(\n        x=lengths,\n        y=fractions,\n        name=label,\n        marker=dict(color=color),\n        hovertemplate=\"Route Length: %{x}&lt;br&gt;Convergent Fraction: %{y:.2%}&lt;extra&gt;&lt;/extra&gt;\",\n        # text=[f\"{v:.1%}\" for v in fractions],\n        # textposition=\"auto\",\n        # textfont=dict(size=12),\n    )\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.dataset_analysis.create_leaf_bar_trace","title":"<code>create_leaf_bar_trace(path_strings, label, color)</code>","text":"<p>Create a bar trace showing distribution of number of leaves at root node.</p> <p>Parameters:</p> Name Type Description Default <code>path_strings</code> <code>list[str]</code> <p>List of path strings to analyze</p> required <code>label</code> <code>str</code> <p>Label for the trace</p> required <code>color</code> <code>str</code> <p>Color for the trace</p> required <p>Returns:</p> Type Description <code>Bar</code> <p>Bar trace showing leaf distribution</p> Source code in <code>src/directmultistep/analysis/paper/dataset_analysis.py</code> <pre><code>def create_leaf_bar_trace(path_strings: list[str], label: str, color: str) -&gt; go.Bar:\n    \"\"\"Create a bar trace showing distribution of number of leaves at root node.\n\n    Args:\n        path_strings: List of path strings to analyze\n        label: Label for the trace\n        color: Color for the trace\n\n    Returns:\n        Bar trace showing leaf distribution\n    \"\"\"\n    n_leaves = []\n    for path in tqdm(path_strings):\n        path_dict = eval(path)\n        root_leaves = sum(\n            1 for child in path_dict[\"children\"] if \"children\" not in child or len(child[\"children\"]) == 0\n        )\n        n_leaves.append(root_leaves)\n\n    unique_lengths, counts = np.unique(n_leaves, return_counts=True)\n\n    unique_lengths = unique_lengths[:4]\n    counts = counts[:4]\n    relative_abundance = counts / len(path_strings)\n\n    return go.Bar(\n        x=unique_lengths,\n        y=relative_abundance,\n        name=label,\n        marker=dict(color=color),\n        hovertemplate=\"Number of Leaves: %{x}&lt;br&gt;Relative Frequency: %{y:.2%}&lt;extra&gt;&lt;/extra&gt;\",\n        text=[f\"{v:.1%}\" for v in relative_abundance],\n        textposition=\"auto\",\n        textfont=dict(size=12),\n    )\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.dataset_analysis.create_split_bar_trace","title":"<code>create_split_bar_trace(route_lengths, label, sep_threshold, color)</code>","text":"<p>Create two bar traces split by a threshold value.</p> <p>Parameters:</p> Name Type Description Default <code>route_lengths</code> <code>list[int]</code> <p>List of route lengths to plot</p> required <code>label</code> <code>str</code> <p>Label for the traces</p> required <code>sep_threshold</code> <code>int</code> <p>Threshold value to split traces</p> required <code>color</code> <code>str</code> <p>Color for both traces</p> required <p>Returns:</p> Type Description <code>tuple[Bar, Bar]</code> <p>Tuple of two bar traces - one for values &lt;= threshold, one for values &gt; threshold</p> Source code in <code>src/directmultistep/analysis/paper/dataset_analysis.py</code> <pre><code>def create_split_bar_trace(\n    route_lengths: list[int], label: str, sep_threshold: int, color: str\n) -&gt; tuple[go.Bar, go.Bar]:\n    \"\"\"Create two bar traces split by a threshold value.\n\n    Args:\n        route_lengths: List of route lengths to plot\n        label: Label for the traces\n        sep_threshold: Threshold value to split traces\n        color: Color for both traces\n\n    Returns:\n        Tuple of two bar traces - one for values &lt;= threshold, one for values &gt; threshold\n    \"\"\"\n    unique_lengths, counts = np.unique(route_lengths, return_counts=True)\n    relative_abundance = counts / len(route_lengths)\n\n    trace_settings = dict(\n        name=label,\n        marker=dict(color=color),\n        hovertemplate=\"Route Length: %{x}&lt;br&gt;Relative Abundance: %{y:.2%}&lt;extra&gt;&lt;/extra&gt;\",\n        textposition=\"auto\",\n    )\n\n    # Split data by threshold\n    mask_short = unique_lengths &lt;= sep_threshold\n    mask_long = unique_lengths &gt; sep_threshold\n\n    trace1 = go.Bar(x=unique_lengths[mask_short], y=relative_abundance[mask_short], **trace_settings)\n\n    trace2 = go.Bar(x=unique_lengths[mask_long], y=relative_abundance[mask_long], showlegend=False, **trace_settings)\n\n    return trace1, trace2\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.dataset_analysis.plot_convergent_fraction_by_length","title":"<code>plot_convergent_fraction_by_length(train_paths, train_lengths, n1_paths, n1_lengths, n5_paths, n5_lengths)</code>","text":"<p>Create a plot showing fraction of convergent routes by length for different datasets.</p> <p>Parameters:</p> Name Type Description Default <code>train_paths</code> <code>list[str]</code> <p>List of path strings from training set</p> required <code>train_lengths</code> <code>list[int]</code> <p>List of route lengths from training set</p> required <code>n1_paths</code> <code>list[str]</code> <p>List of path strings from n1 dataset</p> required <code>n1_lengths</code> <code>list[int]</code> <p>List of route lengths from n1 dataset</p> required <code>n5_paths</code> <code>list[str]</code> <p>List of path strings from n5 dataset</p> required <code>n5_lengths</code> <code>list[int]</code> <p>List of route lengths from n5 dataset</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object containing the visualization</p> Source code in <code>src/directmultistep/analysis/paper/dataset_analysis.py</code> <pre><code>def plot_convergent_fraction_by_length(\n    train_paths: list[str],\n    train_lengths: list[int],\n    n1_paths: list[str],\n    n1_lengths: list[int],\n    n5_paths: list[str],\n    n5_lengths: list[int],\n) -&gt; go.Figure:\n    \"\"\"Create a plot showing fraction of convergent routes by length for different datasets.\n\n    Args:\n        train_paths: List of path strings from training set\n        train_lengths: List of route lengths from training set\n        n1_paths: List of path strings from n1 dataset\n        n1_lengths: List of route lengths from n1 dataset\n        n5_paths: List of path strings from n5 dataset\n        n5_lengths: List of route lengths from n5 dataset\n\n    Returns:\n        Plotly figure object containing the visualization\n    \"\"\"\n    colors = [FONT_COLOR, publication_colors[\"dark_blue\"], publication_colors[\"dark_purple\"]]\n    datasets = [\n        (train_paths, train_lengths, \"Training Routes\", colors[0]),\n        (n1_paths, n1_lengths, \"n1\", colors[1]),\n        (n5_paths, n5_lengths, \"n5\", colors[2]),\n    ]\n\n    fig = go.Figure()\n\n    for paths, lengths, label, color in datasets:\n        fig.add_trace(create_convergent_fraction_trace(paths, lengths, label, color))\n\n    style.AXIS_STYLE[\"linecolor\"] = None\n    apply_publication_style(fig)\n    # fmt:off\n    fig.update_layout(width=1000, height=250, bargap=0.15,\n        margin=dict(l=100, r=50, t=20, b=50),\n        legend=dict( orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=0.99),\n        yaxis=dict(range=[0, 0.31], dtick=0.05),\n    )\n    # fmt:on\n    fig.update_xaxes(title_text=\"&lt;b&gt;Route Length&lt;/b&gt;\", dtick=1, showgrid=False)\n    fig.update_yaxes(title_text=\"&lt;b&gt;Fraction Convergent&lt;/b&gt;\", tickformat=\",.0%\")\n\n    return fig\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.dataset_analysis.plot_convergent_fraction_overall","title":"<code>plot_convergent_fraction_overall(train_paths, n1_paths, n5_paths)</code>","text":"<p>Create a plot showing overall fraction of convergent routes for different datasets.</p> <p>Parameters:</p> Name Type Description Default <code>train_paths</code> <code>list[str]</code> <p>List of path strings from training set</p> required <code>n1_paths</code> <code>list[str]</code> <p>List of path strings from n1 dataset</p> required <code>n5_paths</code> <code>list[str]</code> <p>List of path strings from n5 dataset</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object containing the visualization</p> Source code in <code>src/directmultistep/analysis/paper/dataset_analysis.py</code> <pre><code>def plot_convergent_fraction_overall(\n    train_paths: list[str],\n    n1_paths: list[str],\n    n5_paths: list[str],\n) -&gt; go.Figure:\n    \"\"\"Create a plot showing overall fraction of convergent routes for different datasets.\n\n    Args:\n        train_paths: List of path strings from training set\n        n1_paths: List of path strings from n1 dataset\n        n5_paths: List of path strings from n5 dataset\n\n    Returns:\n        Plotly figure object containing the visualization\n    \"\"\"\n    colors = [FONT_COLOR, publication_colors[\"dark_blue\"], publication_colors[\"dark_purple\"]]\n    datasets = [\n        (train_paths, \"Training Routes\", colors[0]),\n        (n1_paths, \"n1\", colors[1]),\n        (n5_paths, \"n5\", colors[2]),\n    ]\n\n    fractions = []\n    labels = []\n    colors_used = []\n\n    for paths, label, color in tqdm(datasets):\n        n_convergent = sum(1 for path in paths if is_convergent(eval(path)))\n        fraction = n_convergent / len(paths)\n        fractions.append(fraction)\n        labels.append(label)\n        colors_used.append(color)\n\n    fig = go.Figure()\n    # fmt:off\n    fig.add_trace(\n        go.Bar(x=labels, y=fractions,\n            marker=dict(color=colors_used), text=[f\"{v:.1%}\" for v in fractions], textposition=\"auto\"))\n\n    style.AXIS_STYLE[\"linecolor\"] = None\n    apply_publication_style(fig)\n    fig.update_layout(width=600, height=250, margin=dict(l=100, r=50, t=20, b=50), showlegend=False)\n    # fmt:on\n    fig.update_xaxes(title_text=\"&lt;b&gt;Dataset&lt;/b&gt;\", showgrid=False)\n    fig.update_yaxes(title_text=\"&lt;b&gt;Fraction Convergent&lt;/b&gt;\", tickformat=\",.0%\", dtick=0.05, range=[0, 0.31])\n\n    return fig\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.dataset_analysis.plot_leaf_distribution","title":"<code>plot_leaf_distribution(train_paths, n1_paths, n5_paths)</code>","text":"<p>Create a plot showing the distribution of number of leaves for different datasets.</p> <p>Parameters:</p> Name Type Description Default <code>train_paths</code> <code>list[str]</code> <p>List of path strings from training set</p> required <code>n1_paths</code> <code>list[str]</code> <p>List of path strings from n1 dataset</p> required <code>n5_paths</code> <code>list[str]</code> <p>List of path strings from n5 dataset</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object containing the visualization</p> Source code in <code>src/directmultistep/analysis/paper/dataset_analysis.py</code> <pre><code>def plot_leaf_distribution(\n    train_paths: list[str],\n    n1_paths: list[str],\n    n5_paths: list[str],\n) -&gt; go.Figure:\n    \"\"\"Create a plot showing the distribution of number of leaves for different datasets.\n\n    Args:\n        train_paths: List of path strings from training set\n        n1_paths: List of path strings from n1 dataset\n        n5_paths: List of path strings from n5 dataset\n\n    Returns:\n        Plotly figure object containing the visualization\n    \"\"\"\n    colors = [FONT_COLOR, publication_colors[\"dark_blue\"], publication_colors[\"dark_purple\"]]\n    datasets = [(train_paths, \"Training Routes\", colors[0]), (n1_paths, \"n1\", colors[1]), (n5_paths, \"n5\", colors[2])]\n\n    fig = go.Figure()\n\n    for paths, label, color in datasets:\n        fig.add_trace(create_leaf_bar_trace(paths, label, color))\n\n    style.AXIS_STYLE[\"linecolor\"] = None\n    apply_publication_style(fig)\n    # fmt:off\n    fig.update_layout(width=700, height=250, bargap=0.08, yaxis_range=[0, 0.81],\n        margin=dict(l=100, r=50, t=20, b=50),\n        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=0.99))\n\n    fig.update_xaxes(title_text=\"&lt;b&gt;Number of Leaves at Root Node&lt;/b&gt;\", dtick=1, showgrid=False)\n    fig.update_yaxes(title_text=\"&lt;b&gt;Relative Frequency&lt;/b&gt;\", tickformat=\",.0%\")\n\n    return fig\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.dataset_analysis.plot_route_length_distribution","title":"<code>plot_route_length_distribution(train_steps, n1_steps, n5_steps)</code>","text":"<p>Create a split plot showing the distribution of route lengths for different datasets.</p> <p>Parameters:</p> Name Type Description Default <code>train_steps</code> <code>list[int]</code> <p>List of route lengths from training set</p> required <code>n1_steps</code> <code>list[int]</code> <p>List of route lengths from n1 dataset</p> required <code>n5_steps</code> <code>list[int]</code> <p>List of route lengths from n5 dataset</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object containing the visualization</p> Source code in <code>src/directmultistep/analysis/paper/dataset_analysis.py</code> <pre><code>def plot_route_length_distribution(\n    train_steps: list[int],\n    n1_steps: list[int],\n    n5_steps: list[int],\n) -&gt; go.Figure:\n    \"\"\"Create a split plot showing the distribution of route lengths for different datasets.\n\n    Args:\n        train_steps: List of route lengths from training set\n        n1_steps: List of route lengths from n1 dataset\n        n5_steps: List of route lengths from n5 dataset\n\n    Returns:\n        Plotly figure object containing the visualization\n    \"\"\"\n    # Plot settings\n    colors = [FONT_COLOR, publication_colors[\"dark_blue\"], publication_colors[\"dark_purple\"]]\n    sep_threshold = 6\n\n    fig = make_subplots(rows=1, cols=2)\n\n    datasets = [(train_steps, \"Training Routes\", colors[0]), (n1_steps, \"n1\", colors[1]), (n5_steps, \"n5\", colors[2])]\n\n    for steps, label, color in datasets:\n        trace1, trace2 = create_split_bar_trace(steps, label, sep_threshold, color)\n        fig.add_trace(trace1, row=1, col=1)\n        fig.add_trace(trace2, row=1, col=2)\n\n    style.AXIS_STYLE[\"linecolor\"] = None\n    apply_publication_style(fig)\n\n    # fmt:off\n    fig.update_layout(width=1000, height=300, margin=dict(l=100, r=50, t=20, b=50))\n\n    for col in [1, 2]:\n        fig.update_xaxes(title_text=\"&lt;b&gt;Route Length&lt;/b&gt;\", showgrid=False, row=1, col=col, dtick=1)\n\n    fig.update_yaxes(title_text=\"&lt;b&gt;Relative Abundance&lt;/b&gt;\", tickformat=\",.0%\", dtick=0.1, row=1, col=1)\n    fig.update_yaxes(tickformat=\",.2%\", dtick=0.003, row=1, col=2)\n    fig.update_layout(legend=dict( orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=0.99))\n\n    return fig\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent","title":"<code>directmultistep.analysis.paper.linear_vs_convergent</code>","text":""},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.ModelPlotConfig","title":"<code>ModelPlotConfig</code>  <code>dataclass</code>","text":"<p>Configuration for model plotting.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>Name of the model (e.g. 'flex_20M', 'flash_10M').</p> <code>epoch</code> <code>str</code> <p>Epoch number as string (e.g. 'epoch=20').</p> <code>variant_base</code> <code>str</code> <p>Base variant string (e.g. 'b50_sm_st_ea=1_da=1').</p> <code>true_reacs</code> <code>bool</code> <p>Whether to use true reactions.</p> <code>stock</code> <code>bool</code> <p>Whether to use stock compounds.</p> <code>ds_name</code> <code>str</code> <p>Dataset name (e.g. 'n1', 'n5').</p> Source code in <code>src/directmultistep/analysis/paper/linear_vs_convergent.py</code> <pre><code>@dataclass\nclass ModelPlotConfig:\n    \"\"\"Configuration for model plotting.\n\n    Attributes:\n        model_name: Name of the model (e.g. 'flex_20M', 'flash_10M').\n        epoch: Epoch number as string (e.g. 'epoch=20').\n        variant_base: Base variant string (e.g. 'b50_sm_st_ea=1_da=1').\n        true_reacs: Whether to use true reactions.\n        stock: Whether to use stock compounds.\n        ds_name: Dataset name (e.g. 'n1', 'n5').\n    \"\"\"\n\n    model_name: str\n    epoch: str\n    variant_base: str\n    true_reacs: bool = True\n    stock: bool = True\n    ds_name: str = \"n1\"\n\n    def __post_init__(self) -&gt; None:\n        if \"nosm\" in self.variant_base:\n            self.true_reacs = False\n\n    @property\n    def display_name(self) -&gt; str:\n        \"\"\"Generate display name from model name.\n\n        Returns:\n            str: Display name of the model.\n        \"\"\"\n        base = self.model_name.replace(\"_\", \" \").title()\n\n        if \"nosm\" in self.variant_base:\n            base += \" (no SM)\"\n        elif \"sm\" in self.variant_base:\n            base += \" (SM)\"\n\n        if \"ea=1\" in self.variant_base and \"da=1\" in self.variant_base:\n            base = base.replace(\"(\", \"(Mono, \")\n            base += \")\"\n        elif \"ea=2\" in self.variant_base and \"da=2\" in self.variant_base:\n            base = base.replace(\"(\", \"(Duo, \")\n            base += \")\"\n\n        return base\n\n    @property\n    def variant(self) -&gt; str:\n        \"\"\"Get the full variant string.\n\n        Returns:\n            str: Full variant string.\n        \"\"\"\n        return f\"{self.ds_name}_{self.variant_base}\"\n\n    @property\n    def save_suffix(self) -&gt; str:\n        \"\"\"Get the name of the save file.\n\n        Returns:\n            str: Save file suffix.\n        \"\"\"\n        return f\"{self.model_name}_{self.variant}\"\n\n    @property\n    def processed_paths_name(self) -&gt; str:\n        \"\"\"Get the name of the processed paths file.\n\n        Returns:\n            str: Processed paths file name.\n        \"\"\"\n        return f\"processed_paths_NS2n_true_reacs={self.true_reacs}_stock={self.stock}.pkl\"\n\n    @property\n    def correct_paths_name(self) -&gt; str:\n        \"\"\"Get the name of the correct paths file.\n\n        Returns:\n            str: Correct paths file name.\n        \"\"\"\n        return \"correct_paths_NS2n.pkl\"\n\n    def with_dataset(self, ds_name: str) -&gt; \"ModelPlotConfig\":\n        \"\"\"Create a new config with dataset information.\n\n        Args:\n            ds_name: Dataset name.\n\n        Returns:\n            ModelPlotConfig: New config with dataset information.\n        \"\"\"\n        return replace(self, ds_name=ds_name)\n\n    def get_result_path(self, eval_path: Path) -&gt; Path:\n        \"\"\"Get the path to the results directory for this config.\n\n        Args:\n            eval_path: Path to the evaluation directory.\n\n        Returns:\n            Path: Path to the results directory.\n        \"\"\"\n        return eval_path / self.model_name / self.epoch / self.variant\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.ModelPlotConfig.correct_paths_name","title":"<code>correct_paths_name: str</code>  <code>property</code>","text":"<p>Get the name of the correct paths file.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Correct paths file name.</p>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.ModelPlotConfig.display_name","title":"<code>display_name: str</code>  <code>property</code>","text":"<p>Generate display name from model name.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Display name of the model.</p>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.ModelPlotConfig.processed_paths_name","title":"<code>processed_paths_name: str</code>  <code>property</code>","text":"<p>Get the name of the processed paths file.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Processed paths file name.</p>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.ModelPlotConfig.save_suffix","title":"<code>save_suffix: str</code>  <code>property</code>","text":"<p>Get the name of the save file.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Save file suffix.</p>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.ModelPlotConfig.variant","title":"<code>variant: str</code>  <code>property</code>","text":"<p>Get the full variant string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Full variant string.</p>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.ModelPlotConfig.get_result_path","title":"<code>get_result_path(eval_path)</code>","text":"<p>Get the path to the results directory for this config.</p> <p>Parameters:</p> Name Type Description Default <code>eval_path</code> <code>Path</code> <p>Path to the evaluation directory.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Path to the results directory.</p> Source code in <code>src/directmultistep/analysis/paper/linear_vs_convergent.py</code> <pre><code>def get_result_path(self, eval_path: Path) -&gt; Path:\n    \"\"\"Get the path to the results directory for this config.\n\n    Args:\n        eval_path: Path to the evaluation directory.\n\n    Returns:\n        Path: Path to the results directory.\n    \"\"\"\n    return eval_path / self.model_name / self.epoch / self.variant\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.ModelPlotConfig.with_dataset","title":"<code>with_dataset(ds_name)</code>","text":"<p>Create a new config with dataset information.</p> <p>Parameters:</p> Name Type Description Default <code>ds_name</code> <code>str</code> <p>Dataset name.</p> required <p>Returns:</p> Name Type Description <code>ModelPlotConfig</code> <code>ModelPlotConfig</code> <p>New config with dataset information.</p> Source code in <code>src/directmultistep/analysis/paper/linear_vs_convergent.py</code> <pre><code>def with_dataset(self, ds_name: str) -&gt; \"ModelPlotConfig\":\n    \"\"\"Create a new config with dataset information.\n\n    Args:\n        ds_name: Dataset name.\n\n    Returns:\n        ModelPlotConfig: New config with dataset information.\n    \"\"\"\n    return replace(self, ds_name=ds_name)\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.RouteAnalyzer","title":"<code>RouteAnalyzer</code>","text":"<p>Analyzes predicted routes and calculates various statistics.</p> Source code in <code>src/directmultistep/analysis/paper/linear_vs_convergent.py</code> <pre><code>class RouteAnalyzer:\n    \"\"\"Analyzes predicted routes and calculates various statistics.\"\"\"\n\n    def __init__(self, predicted_routes: PathsProcessedType, true_routes: list[str], k_vals: list[int] | None = None):\n        \"\"\"Initializes the RouteAnalyzer.\n\n        Args:\n            predicted_routes: Predicted routes.\n            true_routes: True routes.\n            k_vals: List of k values for top-k accuracy calculation.\n        \"\"\"\n        self.predicted_routes = predicted_routes\n        self.true_routes = true_routes\n        self.k_vals = k_vals if k_vals is not None else [1, 2, 3, 4, 5, 10, 20, 50]\n        self.convergent_idxs = get_convergent_indices(true_routes)\n        self.non_convergent_idxs = set(range(len(true_routes))) - self.convergent_idxs\n\n    def analyze_convergence_stats(self) -&gt; None:\n        \"\"\"Analyze and log basic convergence statistics.\"\"\"\n        n_convergent = len(self.convergent_idxs)\n        total = len(self.true_routes)\n        logger.info(f\"Found {n_convergent} convergent routes out of {total} total routes\")\n        logger.info(f\"Percentage convergent: {100 * n_convergent / total:.1f}%\")\n\n    def calculate_top_k_accuracies(self, save_path: Path | None = None) -&gt; dict[str, dict[str, str]]:\n        \"\"\"Calculate top-k accuracies for different route subsets and optionally save results.\n\n        Args:\n            save_path: Optional path to save detailed accuracies to YAML file.\n\n        Returns:\n            dict[str, dict[str, str]]: Dictionary of top-k accuracies.\n        \"\"\"\n        results = {}\n        route_types = {\"all\": None, \"convergent\": self.non_convergent_idxs, \"non_convergent\": self.convergent_idxs}\n\n        with tqdm(total=len(route_types), desc=\"Analyzing top-k accuracy\") as pbar:\n            for route_type, ignore_ids in route_types.items():\n                pbar.set_description(f\"{route_type} routes\")\n                _, perm_matches = find_matching_paths(self.predicted_routes, self.true_routes, ignore_ids=ignore_ids)\n                results[route_type] = find_top_n_accuracy(perm_matches, self.k_vals)\n                pbar.update(1)\n\n        if save_path is not None:\n            save_path = save_path / \"top_k_accuracy_detailed.yaml\"\n            with open(save_path, \"w\") as f:\n                yaml.dump(results, f, default_flow_style=False)\n            logger.info(f\"Saved detailed accuracies to {save_path}\")\n\n        return results\n\n    def analyze_and_log_results(self) -&gt; dict[str, dict[str, str]]:\n        \"\"\"Run full analysis and log results.\n\n        Returns:\n            dict[str, dict[str, str]]: Dictionary of top-k accuracies.\n        \"\"\"\n        self.analyze_convergence_stats()\n        results = self.calculate_top_k_accuracies()\n\n        for route_type, accuracies in results.items():\n            logger.info(f\"\\nTop-k accuracy for {route_type} routes:\")\n            logger.info(accuracies)\n\n        return results\n\n    def visualize_route_distributions(self, dataset_name: str = \"\") -&gt; go.Figure:\n        \"\"\"Create a publication-quality figure showing the distribution of predicted routes.\n\n        Args:\n            dataset_name: Name of the dataset being analyzed, used in plot title.\n\n        Returns:\n            go.Figure: Plotly figure object.\n        \"\"\"\n        n_predictions = [len(routes) for routes in self.predicted_routes]\n\n        conv_predictions = [n_predictions[i] for i in self.convergent_idxs]\n        nonconv_predictions = [n_predictions[i] for i in self.non_convergent_idxs]\n\n        mean_all, median_all, mean_all_filtered, median_all_filtered = calculate_prediction_stats(n_predictions)\n        mean_conv, median_conv, mean_conv_filtered, median_conv_filtered = calculate_prediction_stats(conv_predictions)\n        mean_nonconv, median_nonconv, mean_nonconv_filtered, median_nonconv_filtered = calculate_prediction_stats(\n            nonconv_predictions\n        )\n\n        # fmt: off\n        fig = make_subplots(rows=1, cols=3,\n            subplot_titles=(\n                f'All Routes&lt;br&gt;&lt;span style=\"font-size:{FONT_SIZES[\"subplot_title\"]}px\"&gt;mean: {mean_all:.1f}, median: {median_all:.1f} (mean*: {mean_all_filtered:.1f}, median*: {median_all_filtered:.1f})&lt;/span&gt;',\n                f'Convergent Routes&lt;br&gt;&lt;span style=\"font-size:{FONT_SIZES[\"subplot_title\"]}px\"&gt;mean: {mean_conv:.1f}, median: {median_conv:.1f} (mean*: {mean_conv_filtered:.1f}, median*: {median_conv_filtered:.1f})&lt;/span&gt;',\n                f'Non-convergent Routes&lt;br&gt;&lt;span style=\"font-size:{FONT_SIZES[\"subplot_title\"]}px\"&gt;mean: {mean_nonconv:.1f}, median: {median_nonconv:.1f} (mean*: {mean_nonconv_filtered:.1f}, median*: {median_nonconv_filtered:.1f})&lt;/span&gt;'\n            ), horizontal_spacing=0.1)\n\n        histogram_style = dict(opacity =0.75, nbinsx=30,histnorm='percent',marker_color=style.publication_colors[\"dark_blue\"])\n\n        data = [(n_predictions, \"All\"), (conv_predictions, \"Convergent\"), (nonconv_predictions, \"Non-convergent\")]\n        for i, (predictions, name) in enumerate(data, start=1):\n            fig.add_trace(go.Histogram(x=predictions, name=name, **histogram_style), row=1, col=i)\n\n        title = \"Distribution of Predicted Routes per Target\"\n        if dataset_name:\n            title = f\"{title} - {dataset_name}\"\n\n        fig.update_layout(title=dict(text=title, x=0.5, xanchor='center'), showlegend=False, height=400, width=1200,)\n\n        apply_publication_style(fig)\n\n        for i in range(1, 4):\n            fig.update_xaxes(title=dict(text=\"Number of Predicted Routes\", font=get_font_dict(FONT_SIZES[\"axis_title\"]), standoff=15), row=1, col=i)\n            fig.update_yaxes(title=dict(text=\"Percentage (%)\", font=get_font_dict(FONT_SIZES[\"axis_title\"]), standoff=15), row=1, col=i)\n        # fmt: on\n        return fig\n\n    @staticmethod\n    def create_comparative_bar_plots(\n        result_paths: list[Path], trace_names: list[str], k_vals: list[int] | None = None, title: str = \"\"\n    ) -&gt; go.Figure:\n        \"\"\"Create comparative bar plots showing top-k accuracy for different configurations.\n\n        Args:\n            result_paths: List of paths to top_k_accuracy_detailed.yaml files.\n            trace_names: List of names for each trace (must match length of result_paths).\n            k_vals: Optional list of k values to show. If None, shows all k values.\n            title: Title for the plot.\n\n        Returns:\n            go.Figure: Plotly figure object.\n        \"\"\"\n        if len(result_paths) != len(trace_names):\n            raise ValueError(\"Number of result paths must match number of trace names\")\n\n        results = []\n        for path in result_paths:\n            with open(path / \"top_k_accuracy_detailed.yaml\") as f:\n                results.append(yaml.safe_load(f))\n\n        # fmt: off\n        fig = make_subplots(rows=3, cols=1, horizontal_spacing=0.07, vertical_spacing=0.12,\n            subplot_titles=[f\"&lt;b&gt;{t}&lt;/b&gt;\" for t in ('(a) all routes', '(b) convergent routes', '(c) non-convergent routes')])\n\n        categories = ['all', 'convergent', 'non_convergent']\n        positions = [1, 2, 3]\n\n        colors = style.colors_blue + style.colors_purple + style.colors_red\n\n        for cat, pos in zip(categories, positions):\n            x = list(results[0][cat].keys())\n            x.sort(key=lambda k: int(k.split()[-1]))\n\n            if k_vals is not None:\n                k_vals_str = [f\"Top {k}\" for k in k_vals]\n                x = [k for k in x if k in k_vals_str]\n\n            for i, (result, name) in enumerate(zip(results, trace_names)):\n                y = [float(result[cat][k].strip('%')) for k in x]\n\n                fig.add_trace(\n                    go.Bar(name=name, x=x, y=y, showlegend=pos == 1, marker_color=colors[i % len(colors)],\n                        legendgroup=name,), row=pos, col=1)\n\n        fig.update_layout(\n            title=dict(text=title, x=0.5, xanchor='center'),\n            barmode='group', height=600, width=1000,\n            legend=dict(font=get_font_dict(FONT_SIZES[\"legend\"]), orientation=\"h\", yanchor=\"bottom\",\n                y=-0.20, xanchor=\"center\", x=0.5, entrywidth=140, tracegroupgap=0))\n\n        style.AXIS_STYLE[\"linecolor\"] = None\n        apply_publication_style(fig)\n\n        for i in range(1, 4):\n            fig.update_yaxes(dtick=10, title=dict(text=\"Accuracy (%)\", font=get_font_dict(FONT_SIZES[\"axis_title\"])), row=i, col=1)\n            fig.update_xaxes(showgrid=False, row=i, col=1)\n        # fmt: on\n        return fig\n\n    @staticmethod\n    def _calculate_accuracy_by_length_data(\n        predicted_routes: PathsProcessedType,\n        dataset: DatasetDict,\n        k_vals: list[int],\n        ignore_ids: set[int] | None = None,\n    ) -&gt; tuple[list[int], dict[int, dict[str, int]]]:\n        \"\"\"Helper function to calculate accuracy by length data.\n\n        Args:\n            predicted_routes: List of predicted routes.\n            dataset: Dataset dictionary.\n            k_vals: List of k values to calculate accuracy for.\n            ignore_ids: Optional set of indices to ignore.\n\n        Returns:\n            Tuple of (lengths, step_stats) where step_stats maps length to accuracy stats.\n        \"\"\"\n        _, perm_matches = find_matching_paths(predicted_routes, dataset[\"path_strings\"], ignore_ids=ignore_ids)\n        step_stats = calculate_top_k_counts_by_step_length(perm_matches, dataset[\"n_steps_list\"], k_vals)\n        lengths = list(step_stats.keys())\n        return lengths, step_stats\n\n    @staticmethod\n    def create_accuracy_by_length_plot(\n        result_paths: list[Path],\n        datasets: list[DatasetDict],\n        configs: list[ModelPlotConfig],\n        k_vals: list[int],\n        title: str = \"\",\n    ) -&gt; go.Figure:\n        \"\"\"Create plot showing accuracy by route length.\n\n        Args:\n            result_paths: List of paths to result directories.\n            datasets: List of datasets to analyze.\n            configs: List of model configurations.\n            k_vals: List of k values to calculate accuracy for.\n            title: Title for the plot.\n\n        Returns:\n            go.Figure: Plotly figure object.\n        \"\"\"\n        fig = go.Figure()\n\n        cset = style.publication_colors\n        colors = [cset[\"primary_blue\"], cset[\"dark_blue\"], cset[\"purple\"], cset[\"dark_purple\"]]\n\n        for i, (path, dataset, config) in enumerate(zip(result_paths, datasets, configs)):\n            paths_name = config.processed_paths_name\n\n            with open(path / paths_name, \"rb\") as f:\n                predicted_routes = pickle.load(f)\n\n            lengths, step_stats = RouteAnalyzer._calculate_accuracy_by_length_data(predicted_routes, dataset, k_vals)\n\n            for k_idx, k in enumerate(k_vals):\n                accuracies = [\n                    step_stats[length].get(f\"Top {k}\", 0) / step_stats[length][\"Total\"] * 100 for length in lengths\n                ]\n                # fmt:off\n                fig.add_trace(go.Bar(name=f\"{dataset['ds_name']} (Top-{k})\", x=lengths, y=accuracies, marker_color=colors[i * len(k_vals) + k_idx]))\n\n        fig.update_layout(\n            barmode=\"group\",\n            height=300,\n            width=1000,\n            xaxis=dict(title=\"&lt;b&gt;Route Length&lt;/b&gt;\", dtick=1),\n            yaxis=dict(title=\"&lt;b&gt;Accuracy (%)&lt;/b&gt;\", dtick=10, range=[0, 82]),\n            legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1.0),\n        )\n        # fmt: on\n        style.AXIS_STYLE[\"linecolor\"] = None\n        apply_publication_style(fig)\n        fig.update_xaxes(showgrid=False)\n        return fig\n\n    @staticmethod\n    def create_accuracy_by_length_subplots(\n        result_paths: list[Path],\n        datasets: list[DatasetDict],\n        configs: list[ModelPlotConfig],\n        k_vals: list[int],\n        title: str = \"\",\n    ) -&gt; go.Figure:\n        \"\"\"Create plot showing accuracy by route length with subplots for all/convergent/non-convergent routes.\n\n        Args:\n            result_paths: List of paths to result directories.\n            datasets: List of datasets to analyze.\n            configs: List of model configurations.\n            k_vals: List of k values to calculate accuracy for.\n            title: Title for the plot.\n\n        Returns:\n            go.Figure: Plotly figure object.\n        \"\"\"\n        fig = make_subplots(\n            rows=3,\n            cols=1,\n            subplot_titles=[\n                f\"&lt;b&gt;{t}&lt;/b&gt;\" for t in (\"(a) all routes\", \"(b) convergent routes\", \"(c) non-convergent routes\")\n            ],\n            vertical_spacing=0.12,\n        )\n\n        cset = style.publication_colors\n        colors = [cset[\"primary_blue\"], cset[\"dark_blue\"], cset[\"purple\"], cset[\"dark_purple\"]]\n\n        for i, (path, dataset, config) in enumerate(zip(result_paths, datasets, configs)):\n            paths_name = config.processed_paths_name\n\n            with open(path / paths_name, \"rb\") as f:\n                predicted_routes = pickle.load(f)\n\n            analyzer = RouteAnalyzer(predicted_routes, dataset[\"path_strings\"])\n            route_types = {\n                \"all\": (None, 1),\n                \"convergent\": (analyzer.non_convergent_idxs, 2),\n                \"non_convergent\": (analyzer.convergent_idxs, 3),\n            }\n\n            for route_type, (ignore_ids, row) in route_types.items():\n                lengths, step_stats = RouteAnalyzer._calculate_accuracy_by_length_data(\n                    predicted_routes, dataset, k_vals, ignore_ids=ignore_ids\n                )\n\n                for k_idx, k in enumerate(k_vals):\n                    accuracies = [\n                        step_stats[length].get(f\"Top {k}\", 0) / step_stats[length][\"Total\"] * 100 for length in lengths\n                    ]\n                    # fmt:off\n                    fig.add_trace(go.Bar(name=f\"{dataset['ds_name']} (Top-{k})\",\n                            x=lengths,y=accuracies, marker_color=colors[i * len(k_vals) + k_idx],\n                            showlegend=row == 1, legendgroup=f\"{dataset['ds_name']} (Top-{k})\"\n                        ), row=row, col=1)\n\n        fig.update_layout(\n            barmode=\"group\",\n            height=900,\n            width=1000,\n            legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.15, xanchor=\"center\", x=0.5),\n        )\n\n        for i in range(1, 4):\n            fig.update_xaxes(title=\"&lt;b&gt;Route Length&lt;/b&gt;\", dtick=1, row=i, col=1, showgrid=False)\n            fig.update_yaxes(title=\"&lt;b&gt;Accuracy (%)&lt;/b&gt;\", dtick=10, range=[0, 82], row=i, col=1)\n        # fmt: on\n        style.AXIS_STYLE[\"linecolor\"] = None\n        apply_publication_style(fig)\n        fig.update_xaxes(showgrid=False)\n        return fig\n\n    @staticmethod\n    def visualize_route_processing_stages(\n        valid_routes: PathsProcessedType,\n        processed_routes_no_stock: PathsProcessedType,\n        processed_routes_with_stock: PathsProcessedType,\n        true_routes: list[str],\n        dataset_name: str = \"\",\n        show_filtered_stats: bool = False,\n    ) -&gt; go.Figure:\n        \"\"\"Create a publication-quality figure showing the distribution of routes at different processing stages.\n\n        Args:\n            valid_routes: Valid routes from beam search.\n            processed_routes_no_stock: Routes after canonicalization/removing repetitions.\n            processed_routes_with_stock: Routes after applying stock filter.\n            true_routes: True routes for convergence analysis.\n            dataset_name: Name of the dataset being analyzed.\n            show_filtered_stats: Whether to show filtered statistics (mean* and median*).\n\n        Returns:\n            go.Figure: Plotly figure object.\n        \"\"\"\n        # Get convergent indices\n        convergent_idxs = get_convergent_indices(true_routes)\n        non_convergent_idxs = set(range(len(true_routes))) - convergent_idxs\n\n        def get_predictions_by_type(routes: PathsProcessedType) -&gt; tuple[list[int], list[int], list[int]]:\n            all_predictions = [len(routes) for routes in routes]\n            conv_predictions = [all_predictions[i] for i in convergent_idxs]\n            nonconv_predictions = [all_predictions[i] for i in non_convergent_idxs]\n            return all_predictions, conv_predictions, nonconv_predictions\n\n        valid_all, valid_conv, valid_nonconv = get_predictions_by_type(valid_routes)\n        no_stock_all, no_stock_conv, no_stock_nonconv = get_predictions_by_type(processed_routes_no_stock)\n        with_stock_all, with_stock_conv, with_stock_nonconv = get_predictions_by_type(processed_routes_with_stock)\n\n        # Create subplot titles\n        def create_subtitle(stage: str, predictions: list[int]) -&gt; str:\n            mean, median, mean_f, median_f = calculate_prediction_stats(predictions)\n            base = f\"{stage}&lt;br&gt;&lt;span style=\\\"font-size:{FONT_SIZES['subplot_title']-4}px\\\"&gt;\"\n            stats = f\"mean={mean:.1f}, median={median:.1f}\"\n            if show_filtered_stats:\n                stats += f\" (\u03bc*={mean_f:.1f}, m*={median_f:.1f})\"\n            return base + stats + \"&lt;/span&gt;\"\n\n        # fmt:off\n        fig = make_subplots(rows=3, cols=3,\n            subplot_titles=[\n                create_subtitle(\"&lt;b&gt;(a) valid routes (all)&lt;/b&gt;\", valid_all),\n                create_subtitle(\"&lt;b&gt;(b) valid routes (convergent)&lt;/b&gt;\", valid_conv),\n                create_subtitle(\"&lt;b&gt;(c) valid routes (non-convergent)&lt;/b&gt;\", valid_nonconv),\n                create_subtitle(\"&lt;b&gt;(d) after canonicalization (all)&lt;/b&gt;\", no_stock_all),\n                create_subtitle(\"&lt;b&gt;(e) after canonicalization (convergent)&lt;/b&gt;\", no_stock_conv),\n                create_subtitle(\"&lt;b&gt;(f) after canonicalization (non-convergent)&lt;/b&gt;\", no_stock_nonconv),\n                create_subtitle(\"&lt;b&gt;(g) after stock filter (all)&lt;/b&gt;\", with_stock_all),\n                create_subtitle(\"&lt;b&gt;(h) after stock filter (convergent)&lt;/b&gt;\", with_stock_conv),\n                create_subtitle(\"&lt;b&gt;(i) after stock filter (non-convergent)&lt;/b&gt;\", with_stock_nonconv),\n            ], vertical_spacing=0.10, horizontal_spacing=0.05)\n\n        histogram_style = dict(histnorm='percent', marker_color=style.publication_colors[\"dark_blue\"], marker_line_width=0)\n\n        data = [\n            (valid_all, valid_conv, valid_nonconv),\n            (no_stock_all, no_stock_conv, no_stock_nonconv),\n            (with_stock_all, with_stock_conv, with_stock_nonconv)\n        ]\n\n        for row, (all_pred, conv_pred, nonconv_pred) in enumerate(data, start=1):\n            for col, predictions in enumerate([all_pred, conv_pred, nonconv_pred], start=1):\n                fig.add_trace(go.Histogram(x=predictions, xbins=dict(start=0, end=50, size=2), **histogram_style), row=row, col=col)\n\n        apply_publication_style(fig)\n        fig.update_layout(showlegend=False, height=900, width=1200, margin_t=60, bargap=0.03)\n\n        for row in range(1, 4):\n            for col in range(1, 4):\n                fig.update_xaxes(title=None, dtick=5, range=[0, 50], row=row, col=col)\n                if row == 3:\n                    fig.update_xaxes(title=dict(text=\"&lt;b&gt;Number of Routes&lt;/b&gt;\", font=get_font_dict(FONT_SIZES[\"axis_title\"]), standoff=15), row=row, col=col)\n\n                if col == 1:\n                    fig.update_yaxes(title=dict(text=\"&lt;b&gt;Percentage (%)&lt;/b&gt;\", font=get_font_dict(FONT_SIZES[\"axis_title\"]), standoff=15), row=row, col=col)\n                else:\n                    fig.update_yaxes(title=None, row=row, col=col)\n\n        return fig\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.RouteAnalyzer.__init__","title":"<code>__init__(predicted_routes, true_routes, k_vals=None)</code>","text":"<p>Initializes the RouteAnalyzer.</p> <p>Parameters:</p> Name Type Description Default <code>predicted_routes</code> <code>PathsProcessedType</code> <p>Predicted routes.</p> required <code>true_routes</code> <code>list[str]</code> <p>True routes.</p> required <code>k_vals</code> <code>list[int] | None</code> <p>List of k values for top-k accuracy calculation.</p> <code>None</code> Source code in <code>src/directmultistep/analysis/paper/linear_vs_convergent.py</code> <pre><code>def __init__(self, predicted_routes: PathsProcessedType, true_routes: list[str], k_vals: list[int] | None = None):\n    \"\"\"Initializes the RouteAnalyzer.\n\n    Args:\n        predicted_routes: Predicted routes.\n        true_routes: True routes.\n        k_vals: List of k values for top-k accuracy calculation.\n    \"\"\"\n    self.predicted_routes = predicted_routes\n    self.true_routes = true_routes\n    self.k_vals = k_vals if k_vals is not None else [1, 2, 3, 4, 5, 10, 20, 50]\n    self.convergent_idxs = get_convergent_indices(true_routes)\n    self.non_convergent_idxs = set(range(len(true_routes))) - self.convergent_idxs\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.RouteAnalyzer.analyze_and_log_results","title":"<code>analyze_and_log_results()</code>","text":"<p>Run full analysis and log results.</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, str]]</code> <p>dict[str, dict[str, str]]: Dictionary of top-k accuracies.</p> Source code in <code>src/directmultistep/analysis/paper/linear_vs_convergent.py</code> <pre><code>def analyze_and_log_results(self) -&gt; dict[str, dict[str, str]]:\n    \"\"\"Run full analysis and log results.\n\n    Returns:\n        dict[str, dict[str, str]]: Dictionary of top-k accuracies.\n    \"\"\"\n    self.analyze_convergence_stats()\n    results = self.calculate_top_k_accuracies()\n\n    for route_type, accuracies in results.items():\n        logger.info(f\"\\nTop-k accuracy for {route_type} routes:\")\n        logger.info(accuracies)\n\n    return results\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.RouteAnalyzer.analyze_convergence_stats","title":"<code>analyze_convergence_stats()</code>","text":"<p>Analyze and log basic convergence statistics.</p> Source code in <code>src/directmultistep/analysis/paper/linear_vs_convergent.py</code> <pre><code>def analyze_convergence_stats(self) -&gt; None:\n    \"\"\"Analyze and log basic convergence statistics.\"\"\"\n    n_convergent = len(self.convergent_idxs)\n    total = len(self.true_routes)\n    logger.info(f\"Found {n_convergent} convergent routes out of {total} total routes\")\n    logger.info(f\"Percentage convergent: {100 * n_convergent / total:.1f}%\")\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.RouteAnalyzer.calculate_top_k_accuracies","title":"<code>calculate_top_k_accuracies(save_path=None)</code>","text":"<p>Calculate top-k accuracies for different route subsets and optionally save results.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>Path | None</code> <p>Optional path to save detailed accuracies to YAML file.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, dict[str, str]]</code> <p>dict[str, dict[str, str]]: Dictionary of top-k accuracies.</p> Source code in <code>src/directmultistep/analysis/paper/linear_vs_convergent.py</code> <pre><code>def calculate_top_k_accuracies(self, save_path: Path | None = None) -&gt; dict[str, dict[str, str]]:\n    \"\"\"Calculate top-k accuracies for different route subsets and optionally save results.\n\n    Args:\n        save_path: Optional path to save detailed accuracies to YAML file.\n\n    Returns:\n        dict[str, dict[str, str]]: Dictionary of top-k accuracies.\n    \"\"\"\n    results = {}\n    route_types = {\"all\": None, \"convergent\": self.non_convergent_idxs, \"non_convergent\": self.convergent_idxs}\n\n    with tqdm(total=len(route_types), desc=\"Analyzing top-k accuracy\") as pbar:\n        for route_type, ignore_ids in route_types.items():\n            pbar.set_description(f\"{route_type} routes\")\n            _, perm_matches = find_matching_paths(self.predicted_routes, self.true_routes, ignore_ids=ignore_ids)\n            results[route_type] = find_top_n_accuracy(perm_matches, self.k_vals)\n            pbar.update(1)\n\n    if save_path is not None:\n        save_path = save_path / \"top_k_accuracy_detailed.yaml\"\n        with open(save_path, \"w\") as f:\n            yaml.dump(results, f, default_flow_style=False)\n        logger.info(f\"Saved detailed accuracies to {save_path}\")\n\n    return results\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.RouteAnalyzer.create_accuracy_by_length_plot","title":"<code>create_accuracy_by_length_plot(result_paths, datasets, configs, k_vals, title='')</code>  <code>staticmethod</code>","text":"<p>Create plot showing accuracy by route length.</p> <p>Parameters:</p> Name Type Description Default <code>result_paths</code> <code>list[Path]</code> <p>List of paths to result directories.</p> required <code>datasets</code> <code>list[DatasetDict]</code> <p>List of datasets to analyze.</p> required <code>configs</code> <code>list[ModelPlotConfig]</code> <p>List of model configurations.</p> required <code>k_vals</code> <code>list[int]</code> <p>List of k values to calculate accuracy for.</p> required <code>title</code> <code>str</code> <p>Title for the plot.</p> <code>''</code> <p>Returns:</p> Type Description <code>Figure</code> <p>go.Figure: Plotly figure object.</p> Source code in <code>src/directmultistep/analysis/paper/linear_vs_convergent.py</code> <pre><code>@staticmethod\ndef create_accuracy_by_length_plot(\n    result_paths: list[Path],\n    datasets: list[DatasetDict],\n    configs: list[ModelPlotConfig],\n    k_vals: list[int],\n    title: str = \"\",\n) -&gt; go.Figure:\n    \"\"\"Create plot showing accuracy by route length.\n\n    Args:\n        result_paths: List of paths to result directories.\n        datasets: List of datasets to analyze.\n        configs: List of model configurations.\n        k_vals: List of k values to calculate accuracy for.\n        title: Title for the plot.\n\n    Returns:\n        go.Figure: Plotly figure object.\n    \"\"\"\n    fig = go.Figure()\n\n    cset = style.publication_colors\n    colors = [cset[\"primary_blue\"], cset[\"dark_blue\"], cset[\"purple\"], cset[\"dark_purple\"]]\n\n    for i, (path, dataset, config) in enumerate(zip(result_paths, datasets, configs)):\n        paths_name = config.processed_paths_name\n\n        with open(path / paths_name, \"rb\") as f:\n            predicted_routes = pickle.load(f)\n\n        lengths, step_stats = RouteAnalyzer._calculate_accuracy_by_length_data(predicted_routes, dataset, k_vals)\n\n        for k_idx, k in enumerate(k_vals):\n            accuracies = [\n                step_stats[length].get(f\"Top {k}\", 0) / step_stats[length][\"Total\"] * 100 for length in lengths\n            ]\n            # fmt:off\n            fig.add_trace(go.Bar(name=f\"{dataset['ds_name']} (Top-{k})\", x=lengths, y=accuracies, marker_color=colors[i * len(k_vals) + k_idx]))\n\n    fig.update_layout(\n        barmode=\"group\",\n        height=300,\n        width=1000,\n        xaxis=dict(title=\"&lt;b&gt;Route Length&lt;/b&gt;\", dtick=1),\n        yaxis=dict(title=\"&lt;b&gt;Accuracy (%)&lt;/b&gt;\", dtick=10, range=[0, 82]),\n        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1.0),\n    )\n    # fmt: on\n    style.AXIS_STYLE[\"linecolor\"] = None\n    apply_publication_style(fig)\n    fig.update_xaxes(showgrid=False)\n    return fig\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.RouteAnalyzer.create_accuracy_by_length_subplots","title":"<code>create_accuracy_by_length_subplots(result_paths, datasets, configs, k_vals, title='')</code>  <code>staticmethod</code>","text":"<p>Create plot showing accuracy by route length with subplots for all/convergent/non-convergent routes.</p> <p>Parameters:</p> Name Type Description Default <code>result_paths</code> <code>list[Path]</code> <p>List of paths to result directories.</p> required <code>datasets</code> <code>list[DatasetDict]</code> <p>List of datasets to analyze.</p> required <code>configs</code> <code>list[ModelPlotConfig]</code> <p>List of model configurations.</p> required <code>k_vals</code> <code>list[int]</code> <p>List of k values to calculate accuracy for.</p> required <code>title</code> <code>str</code> <p>Title for the plot.</p> <code>''</code> <p>Returns:</p> Type Description <code>Figure</code> <p>go.Figure: Plotly figure object.</p> Source code in <code>src/directmultistep/analysis/paper/linear_vs_convergent.py</code> <pre><code>@staticmethod\ndef create_accuracy_by_length_subplots(\n    result_paths: list[Path],\n    datasets: list[DatasetDict],\n    configs: list[ModelPlotConfig],\n    k_vals: list[int],\n    title: str = \"\",\n) -&gt; go.Figure:\n    \"\"\"Create plot showing accuracy by route length with subplots for all/convergent/non-convergent routes.\n\n    Args:\n        result_paths: List of paths to result directories.\n        datasets: List of datasets to analyze.\n        configs: List of model configurations.\n        k_vals: List of k values to calculate accuracy for.\n        title: Title for the plot.\n\n    Returns:\n        go.Figure: Plotly figure object.\n    \"\"\"\n    fig = make_subplots(\n        rows=3,\n        cols=1,\n        subplot_titles=[\n            f\"&lt;b&gt;{t}&lt;/b&gt;\" for t in (\"(a) all routes\", \"(b) convergent routes\", \"(c) non-convergent routes\")\n        ],\n        vertical_spacing=0.12,\n    )\n\n    cset = style.publication_colors\n    colors = [cset[\"primary_blue\"], cset[\"dark_blue\"], cset[\"purple\"], cset[\"dark_purple\"]]\n\n    for i, (path, dataset, config) in enumerate(zip(result_paths, datasets, configs)):\n        paths_name = config.processed_paths_name\n\n        with open(path / paths_name, \"rb\") as f:\n            predicted_routes = pickle.load(f)\n\n        analyzer = RouteAnalyzer(predicted_routes, dataset[\"path_strings\"])\n        route_types = {\n            \"all\": (None, 1),\n            \"convergent\": (analyzer.non_convergent_idxs, 2),\n            \"non_convergent\": (analyzer.convergent_idxs, 3),\n        }\n\n        for route_type, (ignore_ids, row) in route_types.items():\n            lengths, step_stats = RouteAnalyzer._calculate_accuracy_by_length_data(\n                predicted_routes, dataset, k_vals, ignore_ids=ignore_ids\n            )\n\n            for k_idx, k in enumerate(k_vals):\n                accuracies = [\n                    step_stats[length].get(f\"Top {k}\", 0) / step_stats[length][\"Total\"] * 100 for length in lengths\n                ]\n                # fmt:off\n                fig.add_trace(go.Bar(name=f\"{dataset['ds_name']} (Top-{k})\",\n                        x=lengths,y=accuracies, marker_color=colors[i * len(k_vals) + k_idx],\n                        showlegend=row == 1, legendgroup=f\"{dataset['ds_name']} (Top-{k})\"\n                    ), row=row, col=1)\n\n    fig.update_layout(\n        barmode=\"group\",\n        height=900,\n        width=1000,\n        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.15, xanchor=\"center\", x=0.5),\n    )\n\n    for i in range(1, 4):\n        fig.update_xaxes(title=\"&lt;b&gt;Route Length&lt;/b&gt;\", dtick=1, row=i, col=1, showgrid=False)\n        fig.update_yaxes(title=\"&lt;b&gt;Accuracy (%)&lt;/b&gt;\", dtick=10, range=[0, 82], row=i, col=1)\n    # fmt: on\n    style.AXIS_STYLE[\"linecolor\"] = None\n    apply_publication_style(fig)\n    fig.update_xaxes(showgrid=False)\n    return fig\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.RouteAnalyzer.create_comparative_bar_plots","title":"<code>create_comparative_bar_plots(result_paths, trace_names, k_vals=None, title='')</code>  <code>staticmethod</code>","text":"<p>Create comparative bar plots showing top-k accuracy for different configurations.</p> <p>Parameters:</p> Name Type Description Default <code>result_paths</code> <code>list[Path]</code> <p>List of paths to top_k_accuracy_detailed.yaml files.</p> required <code>trace_names</code> <code>list[str]</code> <p>List of names for each trace (must match length of result_paths).</p> required <code>k_vals</code> <code>list[int] | None</code> <p>Optional list of k values to show. If None, shows all k values.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for the plot.</p> <code>''</code> <p>Returns:</p> Type Description <code>Figure</code> <p>go.Figure: Plotly figure object.</p> Source code in <code>src/directmultistep/analysis/paper/linear_vs_convergent.py</code> <pre><code>@staticmethod\ndef create_comparative_bar_plots(\n    result_paths: list[Path], trace_names: list[str], k_vals: list[int] | None = None, title: str = \"\"\n) -&gt; go.Figure:\n    \"\"\"Create comparative bar plots showing top-k accuracy for different configurations.\n\n    Args:\n        result_paths: List of paths to top_k_accuracy_detailed.yaml files.\n        trace_names: List of names for each trace (must match length of result_paths).\n        k_vals: Optional list of k values to show. If None, shows all k values.\n        title: Title for the plot.\n\n    Returns:\n        go.Figure: Plotly figure object.\n    \"\"\"\n    if len(result_paths) != len(trace_names):\n        raise ValueError(\"Number of result paths must match number of trace names\")\n\n    results = []\n    for path in result_paths:\n        with open(path / \"top_k_accuracy_detailed.yaml\") as f:\n            results.append(yaml.safe_load(f))\n\n    # fmt: off\n    fig = make_subplots(rows=3, cols=1, horizontal_spacing=0.07, vertical_spacing=0.12,\n        subplot_titles=[f\"&lt;b&gt;{t}&lt;/b&gt;\" for t in ('(a) all routes', '(b) convergent routes', '(c) non-convergent routes')])\n\n    categories = ['all', 'convergent', 'non_convergent']\n    positions = [1, 2, 3]\n\n    colors = style.colors_blue + style.colors_purple + style.colors_red\n\n    for cat, pos in zip(categories, positions):\n        x = list(results[0][cat].keys())\n        x.sort(key=lambda k: int(k.split()[-1]))\n\n        if k_vals is not None:\n            k_vals_str = [f\"Top {k}\" for k in k_vals]\n            x = [k for k in x if k in k_vals_str]\n\n        for i, (result, name) in enumerate(zip(results, trace_names)):\n            y = [float(result[cat][k].strip('%')) for k in x]\n\n            fig.add_trace(\n                go.Bar(name=name, x=x, y=y, showlegend=pos == 1, marker_color=colors[i % len(colors)],\n                    legendgroup=name,), row=pos, col=1)\n\n    fig.update_layout(\n        title=dict(text=title, x=0.5, xanchor='center'),\n        barmode='group', height=600, width=1000,\n        legend=dict(font=get_font_dict(FONT_SIZES[\"legend\"]), orientation=\"h\", yanchor=\"bottom\",\n            y=-0.20, xanchor=\"center\", x=0.5, entrywidth=140, tracegroupgap=0))\n\n    style.AXIS_STYLE[\"linecolor\"] = None\n    apply_publication_style(fig)\n\n    for i in range(1, 4):\n        fig.update_yaxes(dtick=10, title=dict(text=\"Accuracy (%)\", font=get_font_dict(FONT_SIZES[\"axis_title\"])), row=i, col=1)\n        fig.update_xaxes(showgrid=False, row=i, col=1)\n    # fmt: on\n    return fig\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.RouteAnalyzer.visualize_route_distributions","title":"<code>visualize_route_distributions(dataset_name='')</code>","text":"<p>Create a publication-quality figure showing the distribution of predicted routes.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset being analyzed, used in plot title.</p> <code>''</code> <p>Returns:</p> Type Description <code>Figure</code> <p>go.Figure: Plotly figure object.</p> Source code in <code>src/directmultistep/analysis/paper/linear_vs_convergent.py</code> <pre><code>def visualize_route_distributions(self, dataset_name: str = \"\") -&gt; go.Figure:\n    \"\"\"Create a publication-quality figure showing the distribution of predicted routes.\n\n    Args:\n        dataset_name: Name of the dataset being analyzed, used in plot title.\n\n    Returns:\n        go.Figure: Plotly figure object.\n    \"\"\"\n    n_predictions = [len(routes) for routes in self.predicted_routes]\n\n    conv_predictions = [n_predictions[i] for i in self.convergent_idxs]\n    nonconv_predictions = [n_predictions[i] for i in self.non_convergent_idxs]\n\n    mean_all, median_all, mean_all_filtered, median_all_filtered = calculate_prediction_stats(n_predictions)\n    mean_conv, median_conv, mean_conv_filtered, median_conv_filtered = calculate_prediction_stats(conv_predictions)\n    mean_nonconv, median_nonconv, mean_nonconv_filtered, median_nonconv_filtered = calculate_prediction_stats(\n        nonconv_predictions\n    )\n\n    # fmt: off\n    fig = make_subplots(rows=1, cols=3,\n        subplot_titles=(\n            f'All Routes&lt;br&gt;&lt;span style=\"font-size:{FONT_SIZES[\"subplot_title\"]}px\"&gt;mean: {mean_all:.1f}, median: {median_all:.1f} (mean*: {mean_all_filtered:.1f}, median*: {median_all_filtered:.1f})&lt;/span&gt;',\n            f'Convergent Routes&lt;br&gt;&lt;span style=\"font-size:{FONT_SIZES[\"subplot_title\"]}px\"&gt;mean: {mean_conv:.1f}, median: {median_conv:.1f} (mean*: {mean_conv_filtered:.1f}, median*: {median_conv_filtered:.1f})&lt;/span&gt;',\n            f'Non-convergent Routes&lt;br&gt;&lt;span style=\"font-size:{FONT_SIZES[\"subplot_title\"]}px\"&gt;mean: {mean_nonconv:.1f}, median: {median_nonconv:.1f} (mean*: {mean_nonconv_filtered:.1f}, median*: {median_nonconv_filtered:.1f})&lt;/span&gt;'\n        ), horizontal_spacing=0.1)\n\n    histogram_style = dict(opacity =0.75, nbinsx=30,histnorm='percent',marker_color=style.publication_colors[\"dark_blue\"])\n\n    data = [(n_predictions, \"All\"), (conv_predictions, \"Convergent\"), (nonconv_predictions, \"Non-convergent\")]\n    for i, (predictions, name) in enumerate(data, start=1):\n        fig.add_trace(go.Histogram(x=predictions, name=name, **histogram_style), row=1, col=i)\n\n    title = \"Distribution of Predicted Routes per Target\"\n    if dataset_name:\n        title = f\"{title} - {dataset_name}\"\n\n    fig.update_layout(title=dict(text=title, x=0.5, xanchor='center'), showlegend=False, height=400, width=1200,)\n\n    apply_publication_style(fig)\n\n    for i in range(1, 4):\n        fig.update_xaxes(title=dict(text=\"Number of Predicted Routes\", font=get_font_dict(FONT_SIZES[\"axis_title\"]), standoff=15), row=1, col=i)\n        fig.update_yaxes(title=dict(text=\"Percentage (%)\", font=get_font_dict(FONT_SIZES[\"axis_title\"]), standoff=15), row=1, col=i)\n    # fmt: on\n    return fig\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.RouteAnalyzer.visualize_route_processing_stages","title":"<code>visualize_route_processing_stages(valid_routes, processed_routes_no_stock, processed_routes_with_stock, true_routes, dataset_name='', show_filtered_stats=False)</code>  <code>staticmethod</code>","text":"<p>Create a publication-quality figure showing the distribution of routes at different processing stages.</p> <p>Parameters:</p> Name Type Description Default <code>valid_routes</code> <code>PathsProcessedType</code> <p>Valid routes from beam search.</p> required <code>processed_routes_no_stock</code> <code>PathsProcessedType</code> <p>Routes after canonicalization/removing repetitions.</p> required <code>processed_routes_with_stock</code> <code>PathsProcessedType</code> <p>Routes after applying stock filter.</p> required <code>true_routes</code> <code>list[str]</code> <p>True routes for convergence analysis.</p> required <code>dataset_name</code> <code>str</code> <p>Name of the dataset being analyzed.</p> <code>''</code> <code>show_filtered_stats</code> <code>bool</code> <p>Whether to show filtered statistics (mean and median).</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>go.Figure: Plotly figure object.</p> Source code in <code>src/directmultistep/analysis/paper/linear_vs_convergent.py</code> <pre><code>@staticmethod\ndef visualize_route_processing_stages(\n    valid_routes: PathsProcessedType,\n    processed_routes_no_stock: PathsProcessedType,\n    processed_routes_with_stock: PathsProcessedType,\n    true_routes: list[str],\n    dataset_name: str = \"\",\n    show_filtered_stats: bool = False,\n) -&gt; go.Figure:\n    \"\"\"Create a publication-quality figure showing the distribution of routes at different processing stages.\n\n    Args:\n        valid_routes: Valid routes from beam search.\n        processed_routes_no_stock: Routes after canonicalization/removing repetitions.\n        processed_routes_with_stock: Routes after applying stock filter.\n        true_routes: True routes for convergence analysis.\n        dataset_name: Name of the dataset being analyzed.\n        show_filtered_stats: Whether to show filtered statistics (mean* and median*).\n\n    Returns:\n        go.Figure: Plotly figure object.\n    \"\"\"\n    # Get convergent indices\n    convergent_idxs = get_convergent_indices(true_routes)\n    non_convergent_idxs = set(range(len(true_routes))) - convergent_idxs\n\n    def get_predictions_by_type(routes: PathsProcessedType) -&gt; tuple[list[int], list[int], list[int]]:\n        all_predictions = [len(routes) for routes in routes]\n        conv_predictions = [all_predictions[i] for i in convergent_idxs]\n        nonconv_predictions = [all_predictions[i] for i in non_convergent_idxs]\n        return all_predictions, conv_predictions, nonconv_predictions\n\n    valid_all, valid_conv, valid_nonconv = get_predictions_by_type(valid_routes)\n    no_stock_all, no_stock_conv, no_stock_nonconv = get_predictions_by_type(processed_routes_no_stock)\n    with_stock_all, with_stock_conv, with_stock_nonconv = get_predictions_by_type(processed_routes_with_stock)\n\n    # Create subplot titles\n    def create_subtitle(stage: str, predictions: list[int]) -&gt; str:\n        mean, median, mean_f, median_f = calculate_prediction_stats(predictions)\n        base = f\"{stage}&lt;br&gt;&lt;span style=\\\"font-size:{FONT_SIZES['subplot_title']-4}px\\\"&gt;\"\n        stats = f\"mean={mean:.1f}, median={median:.1f}\"\n        if show_filtered_stats:\n            stats += f\" (\u03bc*={mean_f:.1f}, m*={median_f:.1f})\"\n        return base + stats + \"&lt;/span&gt;\"\n\n    # fmt:off\n    fig = make_subplots(rows=3, cols=3,\n        subplot_titles=[\n            create_subtitle(\"&lt;b&gt;(a) valid routes (all)&lt;/b&gt;\", valid_all),\n            create_subtitle(\"&lt;b&gt;(b) valid routes (convergent)&lt;/b&gt;\", valid_conv),\n            create_subtitle(\"&lt;b&gt;(c) valid routes (non-convergent)&lt;/b&gt;\", valid_nonconv),\n            create_subtitle(\"&lt;b&gt;(d) after canonicalization (all)&lt;/b&gt;\", no_stock_all),\n            create_subtitle(\"&lt;b&gt;(e) after canonicalization (convergent)&lt;/b&gt;\", no_stock_conv),\n            create_subtitle(\"&lt;b&gt;(f) after canonicalization (non-convergent)&lt;/b&gt;\", no_stock_nonconv),\n            create_subtitle(\"&lt;b&gt;(g) after stock filter (all)&lt;/b&gt;\", with_stock_all),\n            create_subtitle(\"&lt;b&gt;(h) after stock filter (convergent)&lt;/b&gt;\", with_stock_conv),\n            create_subtitle(\"&lt;b&gt;(i) after stock filter (non-convergent)&lt;/b&gt;\", with_stock_nonconv),\n        ], vertical_spacing=0.10, horizontal_spacing=0.05)\n\n    histogram_style = dict(histnorm='percent', marker_color=style.publication_colors[\"dark_blue\"], marker_line_width=0)\n\n    data = [\n        (valid_all, valid_conv, valid_nonconv),\n        (no_stock_all, no_stock_conv, no_stock_nonconv),\n        (with_stock_all, with_stock_conv, with_stock_nonconv)\n    ]\n\n    for row, (all_pred, conv_pred, nonconv_pred) in enumerate(data, start=1):\n        for col, predictions in enumerate([all_pred, conv_pred, nonconv_pred], start=1):\n            fig.add_trace(go.Histogram(x=predictions, xbins=dict(start=0, end=50, size=2), **histogram_style), row=row, col=col)\n\n    apply_publication_style(fig)\n    fig.update_layout(showlegend=False, height=900, width=1200, margin_t=60, bargap=0.03)\n\n    for row in range(1, 4):\n        for col in range(1, 4):\n            fig.update_xaxes(title=None, dtick=5, range=[0, 50], row=row, col=col)\n            if row == 3:\n                fig.update_xaxes(title=dict(text=\"&lt;b&gt;Number of Routes&lt;/b&gt;\", font=get_font_dict(FONT_SIZES[\"axis_title\"]), standoff=15), row=row, col=col)\n\n            if col == 1:\n                fig.update_yaxes(title=dict(text=\"&lt;b&gt;Percentage (%)&lt;/b&gt;\", font=get_font_dict(FONT_SIZES[\"axis_title\"]), standoff=15), row=row, col=col)\n            else:\n                fig.update_yaxes(title=None, row=row, col=col)\n\n    return fig\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.calculate_prediction_stats","title":"<code>calculate_prediction_stats(predictions)</code>","text":"<p>Calculate mean and median statistics for a list of predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>list[int]</code> <p>List of prediction counts.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Tuple of (mean, median, filtered_mean, filtered_median) where filtered</p> <code>float</code> <p>versions only consider predictions with count &gt; 0.</p> Source code in <code>src/directmultistep/analysis/paper/linear_vs_convergent.py</code> <pre><code>def calculate_prediction_stats(predictions: list[int]) -&gt; tuple[float, float, float, float]:\n    \"\"\"Calculate mean and median statistics for a list of predictions.\n\n    Args:\n        predictions: List of prediction counts.\n\n    Returns:\n        Tuple of (mean, median, filtered_mean, filtered_median) where filtered\n        versions only consider predictions with count &gt; 0.\n    \"\"\"\n    mean = np.float64(np.mean(predictions)).item()\n    median = np.float64(np.median(predictions)).item()\n\n    filtered = [x for x in predictions if x &gt; 0]\n    filtered_mean = np.float64(np.mean(filtered)).item() if filtered else 0.0\n    filtered_median = np.float64(np.median(filtered)).item() if filtered else 0.0\n\n    return mean, median, filtered_mean, filtered_median\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.get_convergent_indices","title":"<code>get_convergent_indices(path_strings)</code>","text":"<p>Identify indices of convergent routes in dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path_strings</code> <code>list[str]</code> <p>List of path strings.</p> required <p>Returns:</p> Type Description <code>set[int]</code> <p>set[int]: Set of indices of convergent routes.</p> Source code in <code>src/directmultistep/analysis/paper/linear_vs_convergent.py</code> <pre><code>def get_convergent_indices(path_strings: list[str]) -&gt; set[int]:\n    \"\"\"Identify indices of convergent routes in dataset.\n\n    Args:\n        path_strings: List of path strings.\n\n    Returns:\n        set[int]: Set of indices of convergent routes.\n    \"\"\"\n    convergent_idxs = set()\n    logger.info(\"Finding convergent routes\")\n    for i, path_str in enumerate(tqdm(path_strings)):\n        path_dict = eval(path_str)\n        if is_convergent(path_dict):\n            convergent_idxs.add(i)\n    return convergent_idxs\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.load_predicted_routes","title":"<code>load_predicted_routes(path)</code>","text":"<p>Load predicted routes from a pickle file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the pickle file.</p> required <p>Returns:</p> Name Type Description <code>PathsProcessedType</code> <code>PathsProcessedType</code> <p>Loaded predicted routes.</p> Source code in <code>src/directmultistep/analysis/paper/linear_vs_convergent.py</code> <pre><code>def load_predicted_routes(path: Path) -&gt; PathsProcessedType:\n    \"\"\"Load predicted routes from a pickle file.\n\n    Args:\n        path: Path to the pickle file.\n\n    Returns:\n        PathsProcessedType: Loaded predicted routes.\n    \"\"\"\n    with open(path, \"rb\") as f:\n        routes: PathsProcessedType = pickle.load(f)\n    logger.info(f\"Loaded {len(routes)} predicted routes\")\n    return routes\n</code></pre>"},{"location":"analysis/paper-figures/#directmultistep.analysis.paper.linear_vs_convergent.process_model_configs","title":"<code>process_model_configs(eval_path, configs, dataset)</code>","text":"<p>Process model configurations and ensure top-k accuracies are calculated.</p> <p>Parameters:</p> Name Type Description Default <code>eval_path</code> <code>Path</code> <p>Path to evaluation directory.</p> required <code>configs</code> <code>list[ModelPlotConfig]</code> <p>List of model configurations.</p> required <code>dataset</code> <code>DatasetDict</code> <p>Dataset to process.</p> required <p>Returns:</p> Type Description <code>tuple[list[Path], list[str]]</code> <p>Tuple of (result_paths, trace_names) for plotting.</p> Source code in <code>src/directmultistep/analysis/paper/linear_vs_convergent.py</code> <pre><code>def process_model_configs(\n    eval_path: Path, configs: list[ModelPlotConfig], dataset: DatasetDict\n) -&gt; tuple[list[Path], list[str]]:\n    \"\"\"Process model configurations and ensure top-k accuracies are calculated.\n\n    Args:\n        eval_path: Path to evaluation directory.\n        configs: List of model configurations.\n        dataset: Dataset to process.\n\n    Returns:\n        Tuple of (result_paths, trace_names) for plotting.\n    \"\"\"\n    result_paths = []\n    trace_names = []\n\n    for config in configs:\n        res_path = config.get_result_path(eval_path)\n        accuracy_file = res_path / \"top_k_accuracy_detailed.yaml\"\n\n        if not accuracy_file.exists():\n            logger.info(f\"Calculating accuracies for {config.display_name}...\")\n            predicted_routes = load_predicted_routes(res_path / config.processed_paths_name)\n            analyzer = RouteAnalyzer(predicted_routes, dataset[\"path_strings\"])\n            analyzer.calculate_top_k_accuracies(save_path=res_path)\n\n        result_paths.append(res_path)\n        trace_names.append(config.display_name)\n\n    return result_paths, trace_names\n</code></pre>"},{"location":"analysis/style-settings/","title":"Visualization Style Settings","text":"<p>This guide explains the available style settings for visualizations in the analysis tools.</p>"},{"location":"analysis/style-settings/#color-palettes","title":"Color Palettes","text":"<p>The analysis tools provide several predefined color palettes for consistent visualization:</p> <ul> <li><code>style.colors_names</code></li> <li><code>style.colors_light</code></li> <li><code>style.colors_dark</code></li> </ul>"},{"location":"analysis/style-settings/#plot-settings","title":"Plot Settings","text":"<p>The default plot settings use a dark theme:</p> <pre><code>template = \"plotly_dark\"      # Plotly dark theme\nplot_bgcolor = \"#000000\"      # Black plot background\npaper_bgcolor = \"#000000\"     # Black paper background\n</code></pre>"},{"location":"analysis/style-settings/#usage-in-visualizations","title":"Usage in Visualizations","text":"<p>The style settings are automatically applied in visualization functions like <code>plot_training_curves</code> and <code>plot_learning_rates</code>. The color palettes are used cyclically when plotting multiple runs:</p> <ul> <li>Training curves use <code>colors_light</code></li> <li>Validation curves use <code>colors_dark</code></li> <li>Special visualizations can use specific colors from <code>colors_names</code></li> </ul>"},{"location":"dev/logging/","title":"Logging Best Practices","text":"<p>This guide explains how to effectively use Python's logging module in our codebase, whether you're writing modules, running scripts from CLI, or working in Jupyter notebooks.</p>"},{"location":"dev/logging/#environment-variables","title":"Environment Variables","text":"<p>The application's log level can be controlled using the <code>DIRECTMULTISTEP_LOG_LEVEL</code> environment variable:</p> <pre><code># Set log level for the current session\nexport DIRECTMULTISTEP_LOG_LEVEL=DEBUG\npython your_script.py\n\n# Or set it for a single command\nDIRECTMULTISTEP_LOG_LEVEL=DEBUG python your_script.py\n</code></pre> <p>Valid log levels are:</p> <ul> <li><code>DEBUG</code>: Most verbose, detailed debugging information</li> <li><code>INFO</code>: General operational information (default)</li> <li><code>WARNING</code>: Unexpected situations that aren't errors</li> <li><code>ERROR</code>: Serious problems that need attention</li> <li><code>CRITICAL</code>: Critical issues that may cause program failure</li> </ul>"},{"location":"dev/logging/#module-development","title":"Module Development","text":"<p>When writing a module, follow these guidelines:</p> <pre><code>from directmultistep.utils.logging_config import logger\n\ndef my_function():\n    # Use appropriate log levels\n    logger.debug(\"Detailed information for debugging\")\n    logger.info(\"General information about progress\")\n    logger.warning(\"Something unexpected but not error\")\n    logger.error(\"A more serious problem\")\n    logger.critical(\"Program may not be able to continue\")\n</code></pre> <p>Key points:</p> <ul> <li>Don't configure the logger in your modules</li> <li>Always use <code>from directmultistep.utils.logging_config import logger</code></li> <li>Choose appropriate log levels</li> <li>Don't use print statements for debugging</li> <li>Don't add parameters like <code>verbose</code> to your functions</li> </ul>"},{"location":"dev/logging/#jupyter-notebook-usage","title":"Jupyter Notebook Usage","text":"<p>For Jupyter notebooks, put this in your first cell:</p> <pre><code>from directmultistep.utils.logging_config import logger\n\nlogger.setLevel(logging.DEBUG)  # To see debug messages\nlogger.setLevel(logging.INFO)   # Back to info only\n</code></pre>"},{"location":"dev/logging/#log-levels-guide","title":"Log Levels Guide","text":"<p>Choose the appropriate level based on the message importance:</p> <ul> <li>DEBUG: Detailed information for diagnosing problems</li> </ul> <pre><code>logger.debug(f\"Processing data frame with shape {df.shape}\")\n</code></pre> <ul> <li>INFO: Confirmation that things are working as expected</li> </ul> <pre><code>logger.info(\"Model training started\")\n</code></pre> <ul> <li>WARNING: Indication that something unexpected happened</li> </ul> <pre><code>logger.warning(\"Using fallback parameter value\")\n</code></pre> <ul> <li>ERROR: More serious problem that prevented function from working</li> </ul> <pre><code>logger.error(\"Failed to load model weights\")\n</code></pre> <ul> <li>CRITICAL: Program may not be able to continue</li> </ul> <pre><code>logger.critical(\"Out of memory - cannot continue processing\")\n</code></pre>"},{"location":"dev/logging/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li> <p>Configuring Loggers in Modules: Only configure logging in your entry points (main scripts, notebooks)</p> </li> <li> <p>Using Print Statements: Avoid print statements for debugging; use logger.debug instead</p> </li> <li> <p>Hard-coding Log Levels: Don't set log levels in your modules; let the application control them</p> </li> <li> <p>Creating Multiple Handlers: Clear existing handlers in notebooks to avoid duplicate logs</p> </li> <li> <p>Using f-strings for Debug Messages: For expensive operations, check level first:</p> </li> </ol> <pre><code># Bad (string formatting happens regardless of level)\nlogger.debug(f\"Expensive operation result: {expensive_operation()}\")\n\n# Good (string formatting only happens if needed)\nif logger.isEnabledFor(logging.DEBUG):\n    logger.debug(f\"Expensive operation result: {expensive_operation()}\")\n</code></pre>"}]}